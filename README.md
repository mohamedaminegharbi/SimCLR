# SimCLR: Self-Supervised Contrastive Learning on CIFAR-10

A PyTorch implementation of **SimCLR** (Simple Framework for Contrastive Learning of Visual Representations) for self-supervised learning on CIFAR-10 dataset.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Methodology](#methodology)
- [Evaluation Metrics](#evaluation-metrics)
- [Contributing](#contributing)
- [References](#references)

---

## ğŸ¯ Overview

This project implements **SimCLR**, a self-supervised learning framework that learns visual representations without labels through contrastive learning. The model is trained on CIFAR-10 and evaluated using linear probing and fine-tuning approaches.

### Key Highlights

- âœ… **Self-supervised pre-training** using contrastive learning (NT-Xent loss)
- âœ… **ResNet-18** encoder with projection head
- âœ… **Mixed precision training** (FP16) for faster training
- âœ… **LARS optimizer** for large batch training
- âœ… **Comprehensive evaluation** with multiple metrics
- âœ… **Comparison with supervised baseline**

---

## ğŸš€ Features

### Training Features
- **Contrastive Augmentation Pipeline**: Random crop, color jitter, horizontal flip, grayscale
- **NT-Xent Loss**: Normalized Temperature-scaled Cross Entropy Loss
- **LARS Optimizer**: Layer-wise Adaptive Rate Scaling for stable large-batch training
- **Cosine Annealing Scheduler**: Learning rate decay strategy
- **Mixed Precision Training**: Automatic Mixed Precision (AMP) with GradScaler
- **Checkpoint Saving**: Periodic model checkpointing every 100 epochs

### Evaluation Features
- **Two Evaluation Methods**:
  - **Linear Evaluation**: sklearn Logistic Regression on frozen features
  - **Fine-tune Evaluation**: PyTorch linear classifier with gradient descent
- **Comprehensive Metrics**: Accuracy, F1-score, IoU, confusion matrix
- **Per-class Analysis**: Detailed performance breakdown for each class
- **Visualization**: Confusion matrices and per-class metric plots
- **Supervised Baseline Comparison**: Fair comparison with end-to-end supervised learning

---

## ğŸ“Š Results

### Performance Summary

| Method | Accuracy | F1-Score (Macro) | Mean IoU |
|--------|----------|------------------|----------|
| **SimCLR (Linear Eval)** | **79.65%** | **0.7960** | **0.6726** |
| **SimCLR (Fine-tune)** | **79.53%** | **0.7950** | **0.6705** |
| **Supervised Baseline** | 77.68% | 0.7768 | 0.6418 |

### Key Findings

âœ¨ **SimCLR outperforms supervised baseline by ~2%** despite not using labels during pre-training!

This demonstrates the power of self-supervised learning:
- Learns rich representations from unlabeled data
- Requires minimal labeled data for downstream tasks
- More robust and generalizable features

### Per-Class Performance (Linear Evaluation)

| Class | Accuracy | F1-Score | IoU |
|-------|----------|----------|-----|
| Automobile | 91.80% | 0.9189 | 0.8500 |
| Frog | 85.70% | 0.8398 | 0.7238 |
| Airplane | 83.10% | 0.8343 | 0.7158 |
| Ship | 88.10% | 0.8876 | 0.7977 |
| Horse | 81.50% | 0.8232 | 0.6996 |
| Truck | 89.00% | 0.8857 | 0.7950 |
| Deer | 79.80% | 0.7658 | 0.6205 |
| Bird | 66.50% | 0.6845 | 0.5203 |
| Dog | 64.50% | 0.6815 | 0.5168 |
| Cat | 62.40% | 0.6094 | 0.4382 |

**Insight**: SimCLR excels at recognizing vehicles and large objects, but struggles with fine-grained animal classification (cats, dogs, birds).

---

## ğŸ”§ Installation

### Prerequisites

```bash
Python 3.8+
CUDA 11.0+ (for GPU training)
```

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/simclr-cifar10.git
cd simclr-cifar10
```

2. **Install dependencies**
```bash
pip install torch torchvision numpy scikit-learn matplotlib seaborn tqdm
```

3. **Download CIFAR-10 dataset**
```bash
# Dataset will be automatically downloaded when running training script
```

---

## ğŸ’» Usage

### 1. Training SimCLR

Train the SimCLR model with contrastive learning:

```bash
python train_simclr.py
```

**Training Configuration:**
- **Epochs**: 800
- **Batch Size**: 512 (adjust based on GPU memory)
- **Learning Rate**: 0.3 Ã— (batch_size / 256)
- **Temperature**: 0.5
- **Optimizer**: LARS
- **Mixed Precision**: Enabled (FP16)

**Expected Training Time:**
- ~12-15 hours on NVIDIA T4/V100
- ~6-8 hours on NVIDIA A100

**Output:**
```
simclr_model_final.pth          # Full model (encoder + projection head)
simclr_encoder_final.pth        # Encoder only
simclr_checkpoint_epoch_*.pth   # Periodic checkpoints
```

### 2. Evaluating SimCLR

Evaluate the trained model:

```bash
python evaluate_simclr.py
```

**Evaluation Options:**

```
ğŸ” Evaluation Method:
1. Linear Evaluation (sklearn Logistic Regression)
2. Fine-tune Evaluation (PyTorch Linear Classifier)
3. Both methods
4. Compare with supervised baseline

Select method (1/2/3/4):
```

**Option 1: Linear Evaluation**
- Fastest evaluation (~30 seconds)
- Freezes encoder, trains sklearn LogisticRegression
- Tests quality of learned representations

**Option 2: Fine-tune Evaluation**
- Medium speed (~2 minutes)
- Freezes encoder, trains PyTorch linear layer with backprop
- 10 epochs of training

**Option 3: Both Methods**
- Compares both evaluation approaches
- Shows if gradient-based training helps

**Option 4: Compare with Supervised**
- Trains supervised ResNet-18 from scratch
- Fair comparison (same data, architecture, training time)
- Demonstrates advantage of self-supervised learning

**Output:**
```
confusion_matrix_linear.png      # Confusion matrix visualization
confusion_matrix_finetune.png
per_class_metrics_linear.png     # Per-class performance charts
per_class_metrics_finetune.png
```

---

## ğŸ“ Project Structure

```
simclr-cifar10/
â”œâ”€â”€ train_simclr.py              # SimCLR training script
â”œâ”€â”€ evaluate_simclr.py           # Evaluation and comparison script
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ data/                        # CIFAR-10 dataset (auto-downloaded)
â”œâ”€â”€ checkpoints/                 # Model checkpoints
â”‚   â”œâ”€â”€ simclr_model_final.pth
â”‚   â”œâ”€â”€ simclr_encoder_final.pth
â”‚   â””â”€â”€ simclr_checkpoint_epoch_*.pth
â””â”€â”€ results/                     # Evaluation results
    â”œâ”€â”€ confusion_matrix_*.png
    â””â”€â”€ per_class_metrics_*.png
```

---

## ğŸ§  Methodology

### SimCLR Framework

SimCLR learns representations by maximizing agreement between augmented views of the same image.

#### 1. Data Augmentation

For each image, create two augmented views:

```python
Augmentation Pipeline:
- RandomResizedCrop(32, scale=(0.2, 1.0))
- RandomHorizontalFlip(p=0.5)
- ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1) [p=0.8]
- RandomGrayscale(p=0.2)
- Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
```

#### 2. Encoder Architecture

**ResNet-18** (modified):
```
Input (3Ã—32Ã—32)
    â†“
[Conv1 + BN + ReLU]
    â†“
[ResBlock1] Ã—2
    â†“
[ResBlock2] Ã—2
    â†“
[ResBlock3] Ã—2
    â†“
[ResBlock4] Ã—2
    â†“
[AvgPool]
    â†“
Feature Vector h (512-dim)
```

#### 3. Projection Head

**MLP** for contrastive learning:
```
h (512-dim)
    â†“
[Linear(512 â†’ 2048) + BN + ReLU]
    â†“
[Linear(2048 â†’ 128) + BN]
    â†“
z (128-dim) â†’ Used for contrastive loss
```

**Note**: Projection head is discarded after training; only encoder is used for downstream tasks.

#### 4. Contrastive Loss (NT-Xent)

For a batch of N images, create 2N augmented views. For each positive pair (i, j):

```
Loss = -log[ exp(sim(z_i, z_j) / Ï„) / Î£_{kâ‰ i} exp(sim(z_i, z_k) / Ï„) ]
```

Where:
- `sim(u, v) = uÂ·v / (||u|| ||v||)` (cosine similarity)
- `Ï„ = 0.5` (temperature parameter)
- Sum over all 2N-1 negative pairs

**Intuition**: Pull positive pairs together, push negative pairs apart in embedding space.

#### 5. Optimization

**LARS Optimizer** (Layer-wise Adaptive Rate Scaling):
- Adapts learning rate per layer based on gradient/weight norms
- Essential for stable large-batch training
- Formula: `lr_layer = trust_coef Ã— ||W|| / (||âˆ‡W|| + weight_decay Ã— ||W||)`

**Cosine Annealing Scheduler**:
```
lr(t) = lr_min + 0.5 Ã— (lr_max - lr_min) Ã— (1 + cos(Ï€t / T))
```
Where T = 800 epochs

### Evaluation Protocol

#### Linear Evaluation (Protocol from SimCLR paper)

1. **Freeze encoder** (no gradient updates)
2. **Extract features** for all training images
3. **Train linear classifier** (sklearn LogisticRegression)
4. **Evaluate** on test set

#### Fine-tune Evaluation

1. **Freeze encoder** (no gradient updates)
2. **Add trainable linear layer** (512 â†’ 10)
3. **Train with SGD/Adam** for 10 epochs
4. **Evaluate** on test set

---

## ğŸ“ˆ Evaluation Metrics

### Overall Metrics

| Metric | Description | Formula |
|--------|-------------|---------|
| **Accuracy** | Overall classification accuracy | `Correct / Total` |
| **F1-Score (Micro)** | Global F1 across all samples | `2 Ã— P Ã— R / (P + R)` |
| **F1-Score (Macro)** | Average F1 per class | `mean(F1_class)` |
| **F1-Score (Weighted)** | Weighted by class frequency | `Î£(n_class Ã— F1_class) / N` |
| **Mean IoU** | Average Intersection over Union | `mean(TP / (TP + FP + FN))` |

### Per-Class Metrics

For each class i:

| Metric | Formula |
|--------|---------|
| **Accuracy** | `TP_i / (TP_i + FN_i)` |
| **F1-Score** | `2 Ã— P_i Ã— R_i / (P_i + R_i)` |
| **IoU** | `TP_i / (TP_i + FP_i + FN_i)` |

Where:
- `TP` = True Positives
- `FP` = False Positives  
- `FN` = False Negatives
- `P` = Precision = `TP / (TP + FP)`
- `R` = Recall = `TP / (TP + FN)`

---

## ğŸ“ Key Learnings

### Why SimCLR Works

1. **Strong Augmentations**: Composition of multiple augmentations is crucial
2. **Large Batch Size**: More negative samples â†’ better contrastive learning
3. **Projection Head**: Non-linear projection before contrastive loss improves quality
4. **Temperature**: Controls the concentration of embeddings (Ï„=0.5 works well)

### Advantages of Self-Supervised Learning

âœ… **No labels needed** for pre-training  
âœ… **Better generalization** - learns universal features  
âœ… **Data efficiency** - works well with limited labeled data  
âœ… **Transfer learning** - pre-trained encoder works for multiple tasks  

### When to Use SimCLR

**Good for:**
- Limited labeled data
- Transfer learning scenarios
- Learning robust representations
- Domain adaptation

**Consider alternatives when:**
- You have abundant labeled data
- Training time is critical (supervised is faster)
- Task requires very specific features

---

## ğŸ”¬ Hyperparameter Tuning Guide

### Critical Hyperparameters

| Parameter | Default | Range | Impact |
|-----------|---------|-------|--------|
| **Batch Size** | 512 | 256-1024 | â†‘ batch â†’ better but slower |
| **Temperature (Ï„)** | 0.5 | 0.1-1.0 | â†“ Ï„ â†’ harder negatives |
| **Learning Rate** | 0.3 Ã— BS/256 | 0.1-3.0 | Scale with batch size |
| **Projection Dim** | 128 | 64-256 | Higher â†’ more capacity |
| **Epochs** | 800 | 200-1000 | More â†’ better (plateau ~800) |

### Tuning Tips

1. **Batch Size**: Use largest that fits in GPU memory
2. **Learning Rate**: Scale linearly with batch size
3. **Temperature**: Lower = harder task, higher = easier
4. **Augmentation**: Stronger augmentation â†’ better performance

---

## ğŸ› Troubleshooting

### Common Issues

**Issue**: Out of Memory (OOM)
```
Solution: Reduce batch_size in train_simclr.py
- 1024 â†’ 512 â†’ 256 depending on GPU memory
```

**Issue**: Model not converging
```
Solution: Check learning rate
- Try: lr = 0.3 Ã— (batch_size / 256)
- Ensure cosine scheduler is active
```

**Issue**: Poor evaluation performance (<70% accuracy)
```
Solution: 
- Train longer (800+ epochs recommended)
- Check augmentation pipeline
- Verify NT-Xent loss implementation
```

**Issue**: FileNotFoundError during evaluation
```
Solution: Ensure training completed successfully
- Check for simclr_model_final.pth
- Update model_path in evaluate_simclr.py
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Development Guidelines

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“š References

### Resources

- [Official SimCLR Repository](https://github.com/google-research/simclr)
- [PyTorch SimCLR Implementation](https://github.com/sthalles/SimCLR)
- [Self-Supervised Learning Blog](https://lilianweng.github.io/posts/2021-05-31-contrastive/)

---

## â­ Star History

If you find this project helpful, please consider giving it a star! â­

---

**Last Updated**: January 2026
