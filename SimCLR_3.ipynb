{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nfrom torch.amp import autocast, GradScaler\n\nclass ContrastiveAugmentation:\n    \"\"\"Applies two random augmentations to create positive pairs\"\"\"\n    def __init__(self, img_size=32):\n        self.transform = transforms.Compose([\n            transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n    \n    def __call__(self, x):\n        return self.transform(x), self.transform(x)\n\nclass ProjectionHead(nn.Module):\n    \"\"\"MLP projection head with batch normalization for contrastive learning\"\"\"\n    def __init__(self, in_dim=512, hidden_dim=2048, out_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim, bias=False),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim, bias=False),\n            nn.BatchNorm1d(out_dim)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\nclass SimCLR(nn.Module):\n    \"\"\"SimCLR self-supervised learning model\"\"\"\n    def __init__(self, encoder_dim=512, projection_dim=128, hidden_dim=2048):\n        super().__init__()\n        # ResNet18 encoder\n        resnet = torchvision.models.resnet18(weights=None)\n        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection_head = ProjectionHead(encoder_dim, hidden_dim, projection_dim)\n    \n    def forward(self, x):\n        h = self.encoder(x)\n        h = h.view(h.size(0), -1)\n        z = self.projection_head(h)\n        return h, z\n\nclass NTXentLoss(nn.Module):\n    \"\"\"Normalized Temperature-scaled Cross Entropy Loss\"\"\"\n    def __init__(self, temperature=0.5):\n        super().__init__()\n        self.temperature = temperature\n    \n    def forward(self, z_i, z_j):\n        batch_size = z_i.size(0)\n        z = torch.cat([z_i, z_j], dim=0)  # 2N x D\n        \n        # Normalize embeddings\n        z = F.normalize(z, dim=1)\n        \n        # Compute similarity matrix\n        sim = torch.mm(z, z.t()) / self.temperature\n        \n        # Create mask to remove self-similarity\n        # Use -65500 instead of -9e15 for FP16 compatibility\n        # FP16 range is approximately ±65,504\n        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n        sim = sim.masked_fill(mask, -65500.0)\n        \n        # Positive pairs are at indices (i, i+N) and (i+N, i)\n        pos_sim = torch.cat([\n            torch.diag(sim, batch_size),\n            torch.diag(sim, -batch_size)\n        ])\n        \n        # Compute loss\n        loss = -pos_sim + torch.logsumexp(sim, dim=1)\n        return loss.mean()\n\nclass LARSOptimizer(torch.optim.Optimizer):\n    \"\"\"LARS optimizer for large batch training\"\"\"\n    def __init__(self, params, lr=0.1, momentum=0.9, weight_decay=1e-6, trust_coef=0.001):\n        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, trust_coef=trust_coef)\n        super().__init__(params, defaults)\n    \n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        \n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            trust_coef = group['trust_coef']\n            lr = group['lr']\n            \n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                \n                param_norm = torch.norm(p.data)\n                grad_norm = torch.norm(p.grad.data)\n                \n                if param_norm != 0 and grad_norm != 0:\n                    adaptive_lr = trust_coef * param_norm / (grad_norm + weight_decay * param_norm)\n                    adaptive_lr = min(adaptive_lr, lr)\n                else:\n                    adaptive_lr = lr\n                \n                if 'momentum_buffer' not in self.state[p]:\n                    buf = self.state[p]['momentum_buffer'] = torch.zeros_like(p.data)\n                else:\n                    buf = self.state[p]['momentum_buffer']\n                \n                buf.mul_(momentum).add_(p.grad.data + weight_decay * p.data, alpha=adaptive_lr)\n                p.data.add_(buf, alpha=-1)\n        \n        return loss\n\ndef train_simclr(model, dataloader, epochs=800, base_lr=0.3, temperature=0.5, device='cuda'):\n    \"\"\"Training loop for SimCLR with mixed precision and optimizations\"\"\"\n    model = model.to(device)\n    \n    # Use LARS optimizer for large batch training\n    optimizer = LARSOptimizer(\n        model.parameters(),\n        lr=base_lr,\n        momentum=0.9,\n        weight_decay=1e-6\n    )\n    \n    # Cosine annealing scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    criterion = NTXentLoss(temperature=temperature)\n    \n    # Mixed precision training - use updated API\n    scaler = GradScaler(device='cuda')\n    \n    # Enable cuDNN benchmarking for faster training\n    torch.backends.cudnn.benchmark = True\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        \n        for batch_idx, batch in enumerate(dataloader):\n            # Properly unpack the batch\n            (x_i, x_j), _ = batch\n            x_i, x_j = x_i.to(device, non_blocking=True), x_j.to(device, non_blocking=True)\n            \n            # Mixed precision forward pass\n            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n            \n            with autocast('cuda'):\n                _, z_i = model(x_i)\n                _, z_j = model(x_j)\n                loss = criterion(z_i, z_j)\n            \n            # Mixed precision backward pass\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            total_loss += loss.item()\n            \n            if batch_idx % 10 == 0:\n                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(dataloader)}], '\n                      f'Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n        \n        # Step the scheduler\n        scheduler.step()\n        \n        avg_loss = total_loss / len(dataloader)\n        print(f'Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}\\n')\n        \n        # Save checkpoint every 100 epochs\n        if (epoch + 1) % 100 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_loss,\n            }, f'simclr_checkpoint_epoch_{epoch+1}.pth')\n    \n    return model\n\n# Example usage with CIFAR-10 dataset\nif __name__ == \"__main__\":\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    \n    # Download CIFAR-10 dataset\n    dataset = torchvision.datasets.CIFAR10(\n        root='./data',\n        train=True,\n        download=True,\n        transform=ContrastiveAugmentation(img_size=32)\n    )\n    \n    # Determine optimal batch size based on available GPU memory\n    # For most modern GPUs (8GB+), 512 works well\n    # Adjust based on your GPU: 1024 for 16GB+, 256 for 4-6GB\n    batch_size = 512\n    \n    # Reduce num_workers to 4 as suggested by the warning\n    dataloader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=4,  # Reduced from 8 to match system recommendation\n        pin_memory=True,  # Faster data transfer to GPU\n        persistent_workers=True,  # Keep workers alive between epochs\n        prefetch_factor=2  # Prefetch batches\n    )\n    \n    # Initialize model with larger projection head\n    model = SimCLR(encoder_dim=512, projection_dim=128, hidden_dim=2048)\n    \n    # Calculate effective batch size (2N for contrastive pairs)\n    effective_batch_size = batch_size * 2\n    print(f\"\\nBatch size: {batch_size}\")\n    print(f\"Effective batch size (contrastive pairs): {effective_batch_size}\")\n    \n    # Scale learning rate with batch size (linear scaling rule)\n    base_lr = 0.3 * (batch_size / 256)\n    \n    print(\"\\nStarting self-supervised training...\")\n    print(f\"Training for 800 epochs with batch size {batch_size}\")\n    print(f\"Base learning rate: {base_lr:.4f}\")\n    print(f\"Temperature: 0.5\")\n    print(f\"Using mixed precision training (FP16)\")\n    \n    trained_model = train_simclr(\n        model, \n        dataloader, \n        epochs=800, \n        base_lr=base_lr,\n        temperature=0.5,\n        device=device\n    )\n    \n    # Save the final encoder\n    torch.save(trained_model.encoder.state_dict(), 'simclr_encoder_final.pth')\n    torch.save(trained_model.state_dict(), 'simclr_model_final.pth')\n    \n    print(\"\\nTraining complete!\")\n    print(\"Encoder saved to 'simclr_encoder_final.pth'\")\n    print(\"Full model saved to 'simclr_model_final.pth'\")\n    print(\"\\nYou can now use this encoder for downstream tasks like classification!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-22T12:54:17.413798Z","iopub.execute_input":"2026-01-22T12:54:17.414221Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nAvailable GPU memory: 15.83 GB\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:15<00:00, 11.2MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\nBatch size: 512\nEffective batch size (contrastive pairs): 1024\n\nStarting self-supervised training...\nTraining for 800 epochs with batch size 512\nBase learning rate: 0.6000\nTemperature: 0.5\nUsing mixed precision training (FP16)\nEpoch [1/800], Batch [0/98], Loss: 6.8303, LR: 0.600000\nEpoch [1/800], Batch [10/98], Loss: 6.5610, LR: 0.600000\nEpoch [1/800], Batch [20/98], Loss: 6.2221, LR: 0.600000\nEpoch [1/800], Batch [30/98], Loss: 6.1341, LR: 0.600000\nEpoch [1/800], Batch [40/98], Loss: 6.0815, LR: 0.600000\nEpoch [1/800], Batch [50/98], Loss: 6.0222, LR: 0.600000\nEpoch [1/800], Batch [60/98], Loss: 6.0302, LR: 0.600000\nEpoch [1/800], Batch [70/98], Loss: 6.0186, LR: 0.600000\nEpoch [1/800], Batch [80/98], Loss: 5.9904, LR: 0.600000\nEpoch [1/800], Batch [90/98], Loss: 5.9355, LR: 0.600000\nEpoch [1/800] Average Loss: 6.1256\n\nEpoch [2/800], Batch [0/98], Loss: 5.9618, LR: 0.599998\nEpoch [2/800], Batch [10/98], Loss: 5.9208, LR: 0.599998\nEpoch [2/800], Batch [20/98], Loss: 5.8605, LR: 0.599998\nEpoch [2/800], Batch [30/98], Loss: 5.8621, LR: 0.599998\nEpoch [2/800], Batch [40/98], Loss: 5.8322, LR: 0.599998\nEpoch [2/800], Batch [50/98], Loss: 5.8630, LR: 0.599998\nEpoch [2/800], Batch [60/98], Loss: 5.8910, LR: 0.599998\nEpoch [2/800], Batch [70/98], Loss: 5.8446, LR: 0.599998\nEpoch [2/800], Batch [80/98], Loss: 5.8685, LR: 0.599998\nEpoch [2/800], Batch [90/98], Loss: 5.7788, LR: 0.599998\nEpoch [2/800] Average Loss: 5.8598\n\nEpoch [3/800], Batch [0/98], Loss: 5.7628, LR: 0.599991\nEpoch [3/800], Batch [10/98], Loss: 5.8447, LR: 0.599991\nEpoch [3/800], Batch [20/98], Loss: 5.8258, LR: 0.599991\nEpoch [3/800], Batch [30/98], Loss: 5.7781, LR: 0.599991\nEpoch [3/800], Batch [40/98], Loss: 5.8057, LR: 0.599991\nEpoch [3/800], Batch [50/98], Loss: 5.7873, LR: 0.599991\nEpoch [3/800], Batch [60/98], Loss: 5.7713, LR: 0.599991\nEpoch [3/800], Batch [70/98], Loss: 5.8010, LR: 0.599991\nEpoch [3/800], Batch [80/98], Loss: 5.7839, LR: 0.599991\nEpoch [3/800], Batch [90/98], Loss: 5.7915, LR: 0.599991\nEpoch [3/800] Average Loss: 5.7959\n\nEpoch [4/800], Batch [0/98], Loss: 5.7569, LR: 0.599979\nEpoch [4/800], Batch [10/98], Loss: 5.8143, LR: 0.599979\nEpoch [4/800], Batch [20/98], Loss: 5.7489, LR: 0.599979\nEpoch [4/800], Batch [30/98], Loss: 5.7379, LR: 0.599979\nEpoch [4/800], Batch [40/98], Loss: 5.7812, LR: 0.599979\nEpoch [4/800], Batch [50/98], Loss: 5.7578, LR: 0.599979\nEpoch [4/800], Batch [60/98], Loss: 5.7679, LR: 0.599979\nEpoch [4/800], Batch [70/98], Loss: 5.7794, LR: 0.599979\nEpoch [4/800], Batch [80/98], Loss: 5.7544, LR: 0.599979\nEpoch [4/800], Batch [90/98], Loss: 5.7168, LR: 0.599979\nEpoch [4/800] Average Loss: 5.7564\n\nEpoch [5/800], Batch [0/98], Loss: 5.7397, LR: 0.599963\nEpoch [5/800], Batch [10/98], Loss: 5.7384, LR: 0.599963\nEpoch [5/800], Batch [20/98], Loss: 5.7166, LR: 0.599963\nEpoch [5/800], Batch [30/98], Loss: 5.7577, LR: 0.599963\nEpoch [5/800], Batch [40/98], Loss: 5.7178, LR: 0.599963\nEpoch [5/800], Batch [50/98], Loss: 5.7137, LR: 0.599963\nEpoch [5/800], Batch [60/98], Loss: 5.7862, LR: 0.599963\nEpoch [5/800], Batch [70/98], Loss: 5.7560, LR: 0.599963\nEpoch [5/800], Batch [80/98], Loss: 5.7595, LR: 0.599963\nEpoch [5/800], Batch [90/98], Loss: 5.7202, LR: 0.599963\nEpoch [5/800] Average Loss: 5.7273\n\nEpoch [6/800], Batch [0/98], Loss: 5.7706, LR: 0.599942\nEpoch [6/800], Batch [10/98], Loss: 5.6935, LR: 0.599942\nEpoch [6/800], Batch [20/98], Loss: 5.6955, LR: 0.599942\nEpoch [6/800], Batch [30/98], Loss: 5.7045, LR: 0.599942\nEpoch [6/800], Batch [40/98], Loss: 5.6967, LR: 0.599942\nEpoch [6/800], Batch [50/98], Loss: 5.6713, LR: 0.599942\nEpoch [6/800], Batch [60/98], Loss: 5.6721, LR: 0.599942\nEpoch [6/800], Batch [70/98], Loss: 5.7189, LR: 0.599942\nEpoch [6/800], Batch [80/98], Loss: 5.6914, LR: 0.599942\nEpoch [6/800], Batch [90/98], Loss: 5.7199, LR: 0.599942\nEpoch [6/800] Average Loss: 5.7054\n\nEpoch [7/800], Batch [0/98], Loss: 5.6864, LR: 0.599917\nEpoch [7/800], Batch [10/98], Loss: 5.7115, LR: 0.599917\nEpoch [7/800], Batch [20/98], Loss: 5.6825, LR: 0.599917\nEpoch [7/800], Batch [30/98], Loss: 5.6643, LR: 0.599917\nEpoch [7/800], Batch [40/98], Loss: 5.6888, LR: 0.599917\nEpoch [7/800], Batch [50/98], Loss: 5.6474, LR: 0.599917\nEpoch [7/800], Batch [60/98], Loss: 5.6806, LR: 0.599917\nEpoch [7/800], Batch [70/98], Loss: 5.6652, LR: 0.599917\nEpoch [7/800], Batch [80/98], Loss: 5.6715, LR: 0.599917\nEpoch [7/800], Batch [90/98], Loss: 5.7012, LR: 0.599917\nEpoch [7/800] Average Loss: 5.6830\n\nEpoch [8/800], Batch [0/98], Loss: 5.6560, LR: 0.599887\nEpoch [8/800], Batch [10/98], Loss: 5.6821, LR: 0.599887\nEpoch [8/800], Batch [20/98], Loss: 5.7001, LR: 0.599887\nEpoch [8/800], Batch [30/98], Loss: 5.6559, LR: 0.599887\nEpoch [8/800], Batch [40/98], Loss: 5.6379, LR: 0.599887\nEpoch [8/800], Batch [50/98], Loss: 5.6572, LR: 0.599887\nEpoch [8/800], Batch [60/98], Loss: 5.7109, LR: 0.599887\nEpoch [8/800], Batch [70/98], Loss: 5.6590, LR: 0.599887\nEpoch [8/800], Batch [80/98], Loss: 5.6593, LR: 0.599887\nEpoch [8/800], Batch [90/98], Loss: 5.6564, LR: 0.599887\nEpoch [8/800] Average Loss: 5.6671\n\nEpoch [9/800], Batch [0/98], Loss: 5.6806, LR: 0.599852\nEpoch [9/800], Batch [10/98], Loss: 5.6428, LR: 0.599852\nEpoch [9/800], Batch [20/98], Loss: 5.6310, LR: 0.599852\nEpoch [9/800], Batch [30/98], Loss: 5.6718, LR: 0.599852\nEpoch [9/800], Batch [40/98], Loss: 5.6857, LR: 0.599852\nEpoch [9/800], Batch [50/98], Loss: 5.6457, LR: 0.599852\nEpoch [9/800], Batch [60/98], Loss: 5.6408, LR: 0.599852\nEpoch [9/800], Batch [70/98], Loss: 5.6361, LR: 0.599852\nEpoch [9/800], Batch [80/98], Loss: 5.6341, LR: 0.599852\nEpoch [9/800], Batch [90/98], Loss: 5.6266, LR: 0.599852\nEpoch [9/800] Average Loss: 5.6485\n\nEpoch [10/800], Batch [0/98], Loss: 5.6221, LR: 0.599813\nEpoch [10/800], Batch [10/98], Loss: 5.6414, LR: 0.599813\nEpoch [10/800], Batch [20/98], Loss: 5.6326, LR: 0.599813\nEpoch [10/800], Batch [30/98], Loss: 5.6639, LR: 0.599813\nEpoch [10/800], Batch [40/98], Loss: 5.6429, LR: 0.599813\nEpoch [10/800], Batch [50/98], Loss: 5.6585, LR: 0.599813\nEpoch [10/800], Batch [60/98], Loss: 5.6415, LR: 0.599813\nEpoch [10/800], Batch [70/98], Loss: 5.6375, LR: 0.599813\nEpoch [10/800], Batch [80/98], Loss: 5.6021, LR: 0.599813\nEpoch [10/800], Batch [90/98], Loss: 5.6467, LR: 0.599813\nEpoch [10/800] Average Loss: 5.6331\n\nEpoch [11/800], Batch [0/98], Loss: 5.6216, LR: 0.599769\nEpoch [11/800], Batch [10/98], Loss: 5.6197, LR: 0.599769\nEpoch [11/800], Batch [20/98], Loss: 5.6289, LR: 0.599769\nEpoch [11/800], Batch [30/98], Loss: 5.6320, LR: 0.599769\nEpoch [11/800], Batch [40/98], Loss: 5.5996, LR: 0.599769\nEpoch [11/800], Batch [50/98], Loss: 5.6090, LR: 0.599769\nEpoch [11/800], Batch [60/98], Loss: 5.6265, LR: 0.599769\nEpoch [11/800], Batch [70/98], Loss: 5.6111, LR: 0.599769\nEpoch [11/800], Batch [80/98], Loss: 5.6247, LR: 0.599769\nEpoch [11/800], Batch [90/98], Loss: 5.6143, LR: 0.599769\nEpoch [11/800] Average Loss: 5.6168\n\nEpoch [12/800], Batch [0/98], Loss: 5.6537, LR: 0.599720\nEpoch [12/800], Batch [10/98], Loss: 5.6011, LR: 0.599720\nEpoch [12/800], Batch [20/98], Loss: 5.6472, LR: 0.599720\nEpoch [12/800], Batch [30/98], Loss: 5.6414, LR: 0.599720\nEpoch [12/800], Batch [40/98], Loss: 5.5791, LR: 0.599720\nEpoch [12/800], Batch [50/98], Loss: 5.6331, LR: 0.599720\nEpoch [12/800], Batch [60/98], Loss: 5.6352, LR: 0.599720\nEpoch [12/800], Batch [70/98], Loss: 5.6141, LR: 0.599720\nEpoch [12/800], Batch [80/98], Loss: 5.6116, LR: 0.599720\nEpoch [12/800], Batch [90/98], Loss: 5.5594, LR: 0.599720\nEpoch [12/800] Average Loss: 5.6030\n\nEpoch [13/800], Batch [0/98], Loss: 5.6234, LR: 0.599667\nEpoch [13/800], Batch [10/98], Loss: 5.5938, LR: 0.599667\nEpoch [13/800], Batch [20/98], Loss: 5.6148, LR: 0.599667\nEpoch [13/800], Batch [30/98], Loss: 5.6252, LR: 0.599667\nEpoch [13/800], Batch [40/98], Loss: 5.6208, LR: 0.599667\nEpoch [13/800], Batch [50/98], Loss: 5.6148, LR: 0.599667\nEpoch [13/800], Batch [60/98], Loss: 5.6041, LR: 0.599667\nEpoch [13/800], Batch [70/98], Loss: 5.5897, LR: 0.599667\nEpoch [13/800], Batch [80/98], Loss: 5.5912, LR: 0.599667\nEpoch [13/800], Batch [90/98], Loss: 5.5942, LR: 0.599667\nEpoch [13/800] Average Loss: 5.5977\n\nEpoch [14/800], Batch [0/98], Loss: 5.6073, LR: 0.599609\nEpoch [14/800], Batch [10/98], Loss: 5.5994, LR: 0.599609\nEpoch [14/800], Batch [20/98], Loss: 5.5615, LR: 0.599609\nEpoch [14/800], Batch [30/98], Loss: 5.5495, LR: 0.599609\nEpoch [14/800], Batch [40/98], Loss: 5.6073, LR: 0.599609\nEpoch [14/800], Batch [50/98], Loss: 5.5565, LR: 0.599609\nEpoch [14/800], Batch [60/98], Loss: 5.5874, LR: 0.599609\nEpoch [14/800], Batch [70/98], Loss: 5.5850, LR: 0.599609\nEpoch [14/800], Batch [80/98], Loss: 5.5736, LR: 0.599609\nEpoch [14/800], Batch [90/98], Loss: 5.5910, LR: 0.599609\nEpoch [14/800] Average Loss: 5.5854\n\nEpoch [15/800], Batch [0/98], Loss: 5.6048, LR: 0.599547\nEpoch [15/800], Batch [10/98], Loss: 5.5794, LR: 0.599547\nEpoch [15/800], Batch [20/98], Loss: 5.5936, LR: 0.599547\nEpoch [15/800], Batch [30/98], Loss: 5.6169, LR: 0.599547\nEpoch [15/800], Batch [40/98], Loss: 5.5629, LR: 0.599547\nEpoch [15/800], Batch [50/98], Loss: 5.5641, LR: 0.599547\nEpoch [15/800], Batch [60/98], Loss: 5.5799, LR: 0.599547\nEpoch [15/800], Batch [70/98], Loss: 5.5686, LR: 0.599547\nEpoch [15/800], Batch [80/98], Loss: 5.5737, LR: 0.599547\nEpoch [15/800], Batch [90/98], Loss: 5.5845, LR: 0.599547\nEpoch [15/800] Average Loss: 5.5793\n\nEpoch [16/800], Batch [0/98], Loss: 5.5763, LR: 0.599480\nEpoch [16/800], Batch [10/98], Loss: 5.5845, LR: 0.599480\nEpoch [16/800], Batch [20/98], Loss: 5.5442, LR: 0.599480\nEpoch [16/800], Batch [30/98], Loss: 5.5933, LR: 0.599480\nEpoch [16/800], Batch [40/98], Loss: 5.5564, LR: 0.599480\nEpoch [16/800], Batch [50/98], Loss: 5.5715, LR: 0.599480\nEpoch [16/800], Batch [60/98], Loss: 5.5699, LR: 0.599480\nEpoch [16/800], Batch [70/98], Loss: 5.5504, LR: 0.599480\nEpoch [16/800], Batch [80/98], Loss: 5.5508, LR: 0.599480\nEpoch [16/800], Batch [90/98], Loss: 5.5708, LR: 0.599480\nEpoch [16/800] Average Loss: 5.5697\n\nEpoch [17/800], Batch [0/98], Loss: 5.5829, LR: 0.599408\nEpoch [17/800], Batch [10/98], Loss: 5.5379, LR: 0.599408\nEpoch [17/800], Batch [20/98], Loss: 5.5217, LR: 0.599408\nEpoch [17/800], Batch [30/98], Loss: 5.5691, LR: 0.599408\nEpoch [17/800], Batch [40/98], Loss: 5.5462, LR: 0.599408\nEpoch [17/800], Batch [50/98], Loss: 5.5836, LR: 0.599408\nEpoch [17/800], Batch [60/98], Loss: 5.5754, LR: 0.599408\nEpoch [17/800], Batch [70/98], Loss: 5.5711, LR: 0.599408\nEpoch [17/800], Batch [80/98], Loss: 5.5589, LR: 0.599408\nEpoch [17/800], Batch [90/98], Loss: 5.5852, LR: 0.599408\nEpoch [17/800] Average Loss: 5.5624\n\nEpoch [18/800], Batch [0/98], Loss: 5.5789, LR: 0.599332\nEpoch [18/800], Batch [10/98], Loss: 5.5652, LR: 0.599332\nEpoch [18/800], Batch [20/98], Loss: 5.5834, LR: 0.599332\nEpoch [18/800], Batch [30/98], Loss: 5.5676, LR: 0.599332\nEpoch [18/800], Batch [40/98], Loss: 5.5618, LR: 0.599332\nEpoch [18/800], Batch [50/98], Loss: 5.5502, LR: 0.599332\nEpoch [18/800], Batch [60/98], Loss: 5.5782, LR: 0.599332\nEpoch [18/800], Batch [70/98], Loss: 5.5519, LR: 0.599332\nEpoch [18/800], Batch [80/98], Loss: 5.5706, LR: 0.599332\nEpoch [18/800], Batch [90/98], Loss: 5.5535, LR: 0.599332\nEpoch [18/800] Average Loss: 5.5563\n\nEpoch [19/800], Batch [0/98], Loss: 5.5716, LR: 0.599251\nEpoch [19/800], Batch [10/98], Loss: 5.5464, LR: 0.599251\nEpoch [19/800], Batch [20/98], Loss: 5.5682, LR: 0.599251\nEpoch [19/800], Batch [30/98], Loss: 5.5365, LR: 0.599251\nEpoch [19/800], Batch [40/98], Loss: 5.5409, LR: 0.599251\nEpoch [19/800], Batch [50/98], Loss: 5.5420, LR: 0.599251\nEpoch [19/800], Batch [60/98], Loss: 5.5357, LR: 0.599251\nEpoch [19/800], Batch [70/98], Loss: 5.5474, LR: 0.599251\nEpoch [19/800], Batch [80/98], Loss: 5.5641, LR: 0.599251\nEpoch [19/800], Batch [90/98], Loss: 5.5400, LR: 0.599251\nEpoch [19/800] Average Loss: 5.5511\n\nEpoch [20/800], Batch [0/98], Loss: 5.5631, LR: 0.599165\nEpoch [20/800], Batch [10/98], Loss: 5.5270, LR: 0.599165\nEpoch [20/800], Batch [20/98], Loss: 5.5427, LR: 0.599165\nEpoch [20/800], Batch [30/98], Loss: 5.5550, LR: 0.599165\nEpoch [20/800], Batch [40/98], Loss: 5.6096, LR: 0.599165\nEpoch [20/800], Batch [50/98], Loss: 5.5456, LR: 0.599165\nEpoch [20/800], Batch [60/98], Loss: 5.5475, LR: 0.599165\nEpoch [20/800], Batch [70/98], Loss: 5.5264, LR: 0.599165\nEpoch [20/800], Batch [80/98], Loss: 5.5237, LR: 0.599165\nEpoch [20/800], Batch [90/98], Loss: 5.5034, LR: 0.599165\nEpoch [20/800] Average Loss: 5.5436\n\nEpoch [21/800], Batch [0/98], Loss: 5.5900, LR: 0.599075\nEpoch [21/800], Batch [10/98], Loss: 5.5495, LR: 0.599075\nEpoch [21/800], Batch [20/98], Loss: 5.5508, LR: 0.599075\nEpoch [21/800], Batch [30/98], Loss: 5.5332, LR: 0.599075\nEpoch [21/800], Batch [40/98], Loss: 5.5457, LR: 0.599075\nEpoch [21/800], Batch [50/98], Loss: 5.5279, LR: 0.599075\nEpoch [21/800], Batch [60/98], Loss: 5.5245, LR: 0.599075\nEpoch [21/800], Batch [70/98], Loss: 5.5622, LR: 0.599075\nEpoch [21/800], Batch [80/98], Loss: 5.5530, LR: 0.599075\nEpoch [21/800], Batch [90/98], Loss: 5.5586, LR: 0.599075\nEpoch [21/800] Average Loss: 5.5399\n\nEpoch [22/800], Batch [0/98], Loss: 5.5687, LR: 0.598980\nEpoch [22/800], Batch [10/98], Loss: 5.5657, LR: 0.598980\nEpoch [22/800], Batch [20/98], Loss: 5.5200, LR: 0.598980\nEpoch [22/800], Batch [30/98], Loss: 5.5470, LR: 0.598980\nEpoch [22/800], Batch [40/98], Loss: 5.5446, LR: 0.598980\nEpoch [22/800], Batch [50/98], Loss: 5.5179, LR: 0.598980\nEpoch [22/800], Batch [60/98], Loss: 5.5141, LR: 0.598980\nEpoch [22/800], Batch [70/98], Loss: 5.5278, LR: 0.598980\nEpoch [22/800], Batch [80/98], Loss: 5.5062, LR: 0.598980\nEpoch [22/800], Batch [90/98], Loss: 5.5187, LR: 0.598980\nEpoch [22/800] Average Loss: 5.5307\n\nEpoch [23/800], Batch [0/98], Loss: 5.5028, LR: 0.598881\nEpoch [23/800], Batch [10/98], Loss: 5.5320, LR: 0.598881\nEpoch [23/800], Batch [20/98], Loss: 5.5267, LR: 0.598881\nEpoch [23/800], Batch [30/98], Loss: 5.5436, LR: 0.598881\nEpoch [23/800], Batch [40/98], Loss: 5.5341, LR: 0.598881\nEpoch [23/800], Batch [50/98], Loss: 5.5553, LR: 0.598881\nEpoch [23/800], Batch [60/98], Loss: 5.5310, LR: 0.598881\nEpoch [23/800], Batch [70/98], Loss: 5.5123, LR: 0.598881\nEpoch [23/800], Batch [80/98], Loss: 5.5123, LR: 0.598881\nEpoch [23/800], Batch [90/98], Loss: 5.5457, LR: 0.598881\nEpoch [23/800] Average Loss: 5.5291\n\nEpoch [24/800], Batch [0/98], Loss: 5.5315, LR: 0.598777\nEpoch [24/800], Batch [10/98], Loss: 5.5252, LR: 0.598777\nEpoch [24/800], Batch [20/98], Loss: 5.5311, LR: 0.598777\nEpoch [24/800], Batch [30/98], Loss: 5.4997, LR: 0.598777\nEpoch [24/800], Batch [40/98], Loss: 5.5380, LR: 0.598777\nEpoch [24/800], Batch [50/98], Loss: 5.5297, LR: 0.598777\nEpoch [24/800], Batch [60/98], Loss: 5.5124, LR: 0.598777\nEpoch [24/800], Batch [70/98], Loss: 5.5378, LR: 0.598777\nEpoch [24/800], Batch [80/98], Loss: 5.5577, LR: 0.598777\nEpoch [24/800], Batch [90/98], Loss: 5.5279, LR: 0.598777\nEpoch [24/800] Average Loss: 5.5243\n\nEpoch [25/800], Batch [0/98], Loss: 5.5417, LR: 0.598669\nEpoch [25/800], Batch [10/98], Loss: 5.4799, LR: 0.598669\nEpoch [25/800], Batch [20/98], Loss: 5.5151, LR: 0.598669\nEpoch [25/800], Batch [30/98], Loss: 5.5164, LR: 0.598669\nEpoch [25/800], Batch [40/98], Loss: 5.5066, LR: 0.598669\nEpoch [25/800], Batch [50/98], Loss: 5.5176, LR: 0.598669\nEpoch [25/800], Batch [60/98], Loss: 5.4908, LR: 0.598669\nEpoch [25/800], Batch [70/98], Loss: 5.5438, LR: 0.598669\nEpoch [25/800], Batch [80/98], Loss: 5.5361, LR: 0.598669\nEpoch [25/800], Batch [90/98], Loss: 5.5294, LR: 0.598669\nEpoch [25/800] Average Loss: 5.5176\n\nEpoch [26/800], Batch [0/98], Loss: 5.4853, LR: 0.598555\nEpoch [26/800], Batch [10/98], Loss: 5.5241, LR: 0.598555\nEpoch [26/800], Batch [20/98], Loss: 5.5397, LR: 0.598555\nEpoch [26/800], Batch [30/98], Loss: 5.5285, LR: 0.598555\nEpoch [26/800], Batch [40/98], Loss: 5.5129, LR: 0.598555\nEpoch [26/800], Batch [50/98], Loss: 5.5427, LR: 0.598555\nEpoch [26/800], Batch [60/98], Loss: 5.4996, LR: 0.598555\nEpoch [26/800], Batch [70/98], Loss: 5.5232, LR: 0.598555\nEpoch [26/800], Batch [80/98], Loss: 5.5394, LR: 0.598555\nEpoch [26/800], Batch [90/98], Loss: 5.4977, LR: 0.598555\nEpoch [26/800] Average Loss: 5.5143\n\nEpoch [27/800], Batch [0/98], Loss: 5.5359, LR: 0.598438\nEpoch [27/800], Batch [10/98], Loss: 5.5386, LR: 0.598438\nEpoch [27/800], Batch [20/98], Loss: 5.5051, LR: 0.598438\nEpoch [27/800], Batch [30/98], Loss: 5.5087, LR: 0.598438\nEpoch [27/800], Batch [40/98], Loss: 5.5069, LR: 0.598438\nEpoch [27/800], Batch [50/98], Loss: 5.5019, LR: 0.598438\nEpoch [27/800], Batch [60/98], Loss: 5.5211, LR: 0.598438\nEpoch [27/800], Batch [70/98], Loss: 5.5223, LR: 0.598438\nEpoch [27/800], Batch [80/98], Loss: 5.4876, LR: 0.598438\nEpoch [27/800], Batch [90/98], Loss: 5.5182, LR: 0.598438\nEpoch [27/800] Average Loss: 5.5141\n\nEpoch [28/800], Batch [0/98], Loss: 5.5410, LR: 0.598315\nEpoch [28/800], Batch [10/98], Loss: 5.5294, LR: 0.598315\nEpoch [28/800], Batch [20/98], Loss: 5.5007, LR: 0.598315\nEpoch [28/800], Batch [30/98], Loss: 5.5336, LR: 0.598315\nEpoch [28/800], Batch [40/98], Loss: 5.4842, LR: 0.598315\nEpoch [28/800], Batch [50/98], Loss: 5.4998, LR: 0.598315\nEpoch [28/800], Batch [60/98], Loss: 5.5136, LR: 0.598315\nEpoch [28/800], Batch [70/98], Loss: 5.5185, LR: 0.598315\nEpoch [28/800], Batch [80/98], Loss: 5.4975, LR: 0.598315\nEpoch [28/800], Batch [90/98], Loss: 5.5031, LR: 0.598315\nEpoch [28/800] Average Loss: 5.5084\n\nEpoch [29/800], Batch [0/98], Loss: 5.5274, LR: 0.598188\nEpoch [29/800], Batch [10/98], Loss: 5.5162, LR: 0.598188\nEpoch [29/800], Batch [20/98], Loss: 5.4986, LR: 0.598188\nEpoch [29/800], Batch [30/98], Loss: 5.5101, LR: 0.598188\nEpoch [29/800], Batch [40/98], Loss: 5.5150, LR: 0.598188\nEpoch [29/800], Batch [50/98], Loss: 5.5001, LR: 0.598188\nEpoch [29/800], Batch [60/98], Loss: 5.5106, LR: 0.598188\nEpoch [29/800], Batch [70/98], Loss: 5.5115, LR: 0.598188\nEpoch [29/800], Batch [80/98], Loss: 5.5216, LR: 0.598188\nEpoch [29/800], Batch [90/98], Loss: 5.4864, LR: 0.598188\nEpoch [29/800] Average Loss: 5.5043\n\nEpoch [30/800], Batch [0/98], Loss: 5.5463, LR: 0.598057\nEpoch [30/800], Batch [10/98], Loss: 5.5023, LR: 0.598057\nEpoch [30/800], Batch [20/98], Loss: 5.5014, LR: 0.598057\nEpoch [30/800], Batch [30/98], Loss: 5.5038, LR: 0.598057\nEpoch [30/800], Batch [40/98], Loss: 5.5020, LR: 0.598057\nEpoch [30/800], Batch [50/98], Loss: 5.5408, LR: 0.598057\nEpoch [30/800], Batch [60/98], Loss: 5.5426, LR: 0.598057\nEpoch [30/800], Batch [70/98], Loss: 5.4881, LR: 0.598057\nEpoch [30/800], Batch [80/98], Loss: 5.5504, LR: 0.598057\nEpoch [30/800], Batch [90/98], Loss: 5.5446, LR: 0.598057\nEpoch [30/800] Average Loss: 5.5055\n\nEpoch [31/800], Batch [0/98], Loss: 5.4886, LR: 0.597921\nEpoch [31/800], Batch [10/98], Loss: 5.4977, LR: 0.597921\nEpoch [31/800], Batch [20/98], Loss: 5.5167, LR: 0.597921\nEpoch [31/800], Batch [30/98], Loss: 5.4977, LR: 0.597921\nEpoch [31/800], Batch [40/98], Loss: 5.4825, LR: 0.597921\nEpoch [31/800], Batch [50/98], Loss: 5.4841, LR: 0.597921\nEpoch [31/800], Batch [60/98], Loss: 5.4732, LR: 0.597921\nEpoch [31/800], Batch [70/98], Loss: 5.5293, LR: 0.597921\nEpoch [31/800], Batch [80/98], Loss: 5.4664, LR: 0.597921\nEpoch [31/800], Batch [90/98], Loss: 5.4800, LR: 0.597921\nEpoch [31/800] Average Loss: 5.4969\n\nEpoch [32/800], Batch [0/98], Loss: 5.5139, LR: 0.597780\nEpoch [32/800], Batch [10/98], Loss: 5.4642, LR: 0.597780\nEpoch [32/800], Batch [20/98], Loss: 5.5048, LR: 0.597780\nEpoch [32/800], Batch [30/98], Loss: 5.4839, LR: 0.597780\nEpoch [32/800], Batch [40/98], Loss: 5.4944, LR: 0.597780\nEpoch [32/800], Batch [50/98], Loss: 5.4834, LR: 0.597780\nEpoch [32/800], Batch [60/98], Loss: 5.4901, LR: 0.597780\nEpoch [32/800], Batch [70/98], Loss: 5.4949, LR: 0.597780\nEpoch [32/800], Batch [80/98], Loss: 5.5007, LR: 0.597780\nEpoch [32/800], Batch [90/98], Loss: 5.4722, LR: 0.597780\nEpoch [32/800] Average Loss: 5.4928\n\nEpoch [33/800], Batch [0/98], Loss: 5.5036, LR: 0.597634\nEpoch [33/800], Batch [10/98], Loss: 5.4748, LR: 0.597634\nEpoch [33/800], Batch [20/98], Loss: 5.5177, LR: 0.597634\nEpoch [33/800], Batch [30/98], Loss: 5.5041, LR: 0.597634\nEpoch [33/800], Batch [40/98], Loss: 5.4830, LR: 0.597634\nEpoch [33/800], Batch [50/98], Loss: 5.4757, LR: 0.597634\nEpoch [33/800], Batch [60/98], Loss: 5.5119, LR: 0.597634\nEpoch [33/800], Batch [70/98], Loss: 5.4854, LR: 0.597634\nEpoch [33/800], Batch [80/98], Loss: 5.4783, LR: 0.597634\nEpoch [33/800], Batch [90/98], Loss: 5.4740, LR: 0.597634\nEpoch [33/800] Average Loss: 5.4938\n\nEpoch [34/800], Batch [0/98], Loss: 5.5148, LR: 0.597484\nEpoch [34/800], Batch [10/98], Loss: 5.4865, LR: 0.597484\nEpoch [34/800], Batch [20/98], Loss: 5.4814, LR: 0.597484\nEpoch [34/800], Batch [30/98], Loss: 5.4738, LR: 0.597484\nEpoch [34/800], Batch [40/98], Loss: 5.4968, LR: 0.597484\nEpoch [34/800], Batch [50/98], Loss: 5.4581, LR: 0.597484\nEpoch [34/800], Batch [60/98], Loss: 5.4789, LR: 0.597484\nEpoch [34/800], Batch [70/98], Loss: 5.4981, LR: 0.597484\nEpoch [34/800], Batch [80/98], Loss: 5.4860, LR: 0.597484\nEpoch [34/800], Batch [90/98], Loss: 5.4894, LR: 0.597484\nEpoch [34/800] Average Loss: 5.4865\n\nEpoch [35/800], Batch [0/98], Loss: 5.4686, LR: 0.597330\nEpoch [35/800], Batch [10/98], Loss: 5.4655, LR: 0.597330\nEpoch [35/800], Batch [20/98], Loss: 5.4934, LR: 0.597330\nEpoch [35/800], Batch [30/98], Loss: 5.4580, LR: 0.597330\nEpoch [35/800], Batch [40/98], Loss: 5.5037, LR: 0.597330\nEpoch [35/800], Batch [50/98], Loss: 5.4601, LR: 0.597330\nEpoch [35/800], Batch [60/98], Loss: 5.5170, LR: 0.597330\nEpoch [35/800], Batch [70/98], Loss: 5.4601, LR: 0.597330\nEpoch [35/800], Batch [80/98], Loss: 5.4587, LR: 0.597330\nEpoch [35/800], Batch [90/98], Loss: 5.4781, LR: 0.597330\nEpoch [35/800] Average Loss: 5.4837\n\nEpoch [36/800], Batch [0/98], Loss: 5.5068, LR: 0.597171\nEpoch [36/800], Batch [10/98], Loss: 5.4823, LR: 0.597171\nEpoch [36/800], Batch [20/98], Loss: 5.4848, LR: 0.597171\nEpoch [36/800], Batch [30/98], Loss: 5.5034, LR: 0.597171\nEpoch [36/800], Batch [40/98], Loss: 5.4778, LR: 0.597171\nEpoch [36/800], Batch [50/98], Loss: 5.4708, LR: 0.597171\nEpoch [36/800], Batch [60/98], Loss: 5.4873, LR: 0.597171\nEpoch [36/800], Batch [70/98], Loss: 5.4974, LR: 0.597171\nEpoch [36/800], Batch [80/98], Loss: 5.4650, LR: 0.597171\nEpoch [36/800], Batch [90/98], Loss: 5.4710, LR: 0.597171\nEpoch [36/800] Average Loss: 5.4826\n\nEpoch [37/800], Batch [0/98], Loss: 5.4850, LR: 0.597007\nEpoch [37/800], Batch [10/98], Loss: 5.4846, LR: 0.597007\nEpoch [37/800], Batch [20/98], Loss: 5.4670, LR: 0.597007\nEpoch [37/800], Batch [30/98], Loss: 5.4701, LR: 0.597007\nEpoch [37/800], Batch [40/98], Loss: 5.4461, LR: 0.597007\nEpoch [37/800], Batch [50/98], Loss: 5.4688, LR: 0.597007\nEpoch [37/800], Batch [60/98], Loss: 5.5188, LR: 0.597007\nEpoch [37/800], Batch [70/98], Loss: 5.4854, LR: 0.597007\nEpoch [37/800], Batch [80/98], Loss: 5.4704, LR: 0.597007\nEpoch [37/800], Batch [90/98], Loss: 5.4625, LR: 0.597007\nEpoch [37/800] Average Loss: 5.4784\n\nEpoch [38/800], Batch [0/98], Loss: 5.4971, LR: 0.596839\nEpoch [38/800], Batch [10/98], Loss: 5.4858, LR: 0.596839\nEpoch [38/800], Batch [20/98], Loss: 5.4688, LR: 0.596839\nEpoch [38/800], Batch [30/98], Loss: 5.4892, LR: 0.596839\nEpoch [38/800], Batch [40/98], Loss: 5.4847, LR: 0.596839\nEpoch [38/800], Batch [50/98], Loss: 5.4942, LR: 0.596839\nEpoch [38/800], Batch [60/98], Loss: 5.4830, LR: 0.596839\nEpoch [38/800], Batch [70/98], Loss: 5.4974, LR: 0.596839\nEpoch [38/800], Batch [80/98], Loss: 5.4651, LR: 0.596839\nEpoch [38/800], Batch [90/98], Loss: 5.4937, LR: 0.596839\nEpoch [38/800] Average Loss: 5.4764\n\nEpoch [39/800], Batch [0/98], Loss: 5.4905, LR: 0.596666\nEpoch [39/800], Batch [10/98], Loss: 5.4696, LR: 0.596666\nEpoch [39/800], Batch [20/98], Loss: 5.4746, LR: 0.596666\nEpoch [39/800], Batch [30/98], Loss: 5.5105, LR: 0.596666\nEpoch [39/800], Batch [40/98], Loss: 5.4549, LR: 0.596666\nEpoch [39/800], Batch [50/98], Loss: 5.4828, LR: 0.596666\nEpoch [39/800], Batch [60/98], Loss: 5.4713, LR: 0.596666\nEpoch [39/800], Batch [70/98], Loss: 5.4664, LR: 0.596666\nEpoch [39/800], Batch [80/98], Loss: 5.4543, LR: 0.596666\nEpoch [39/800], Batch [90/98], Loss: 5.4756, LR: 0.596666\nEpoch [39/800] Average Loss: 5.4749\n\nEpoch [40/800], Batch [0/98], Loss: 5.4514, LR: 0.596489\nEpoch [40/800], Batch [10/98], Loss: 5.4991, LR: 0.596489\nEpoch [40/800], Batch [20/98], Loss: 5.4786, LR: 0.596489\nEpoch [40/800], Batch [30/98], Loss: 5.5097, LR: 0.596489\nEpoch [40/800], Batch [40/98], Loss: 5.4800, LR: 0.596489\nEpoch [40/800], Batch [50/98], Loss: 5.4687, LR: 0.596489\nEpoch [40/800], Batch [60/98], Loss: 5.4789, LR: 0.596489\nEpoch [40/800], Batch [70/98], Loss: 5.4679, LR: 0.596489\nEpoch [40/800], Batch [80/98], Loss: 5.4915, LR: 0.596489\nEpoch [40/800], Batch [90/98], Loss: 5.4540, LR: 0.596489\nEpoch [40/800] Average Loss: 5.4724\n\nEpoch [41/800], Batch [0/98], Loss: 5.4897, LR: 0.596307\nEpoch [41/800], Batch [10/98], Loss: 5.4521, LR: 0.596307\nEpoch [41/800], Batch [20/98], Loss: 5.4644, LR: 0.596307\nEpoch [41/800], Batch [30/98], Loss: 5.4648, LR: 0.596307\nEpoch [41/800], Batch [40/98], Loss: 5.5058, LR: 0.596307\nEpoch [41/800], Batch [50/98], Loss: 5.4612, LR: 0.596307\nEpoch [41/800], Batch [60/98], Loss: 5.4524, LR: 0.596307\nEpoch [41/800], Batch [70/98], Loss: 5.4641, LR: 0.596307\nEpoch [41/800], Batch [80/98], Loss: 5.4983, LR: 0.596307\nEpoch [41/800], Batch [90/98], Loss: 5.4942, LR: 0.596307\nEpoch [41/800] Average Loss: 5.4689\n\nEpoch [42/800], Batch [0/98], Loss: 5.4565, LR: 0.596120\nEpoch [42/800], Batch [10/98], Loss: 5.4810, LR: 0.596120\nEpoch [42/800], Batch [20/98], Loss: 5.4650, LR: 0.596120\nEpoch [42/800], Batch [30/98], Loss: 5.5044, LR: 0.596120\nEpoch [42/800], Batch [40/98], Loss: 5.4524, LR: 0.596120\nEpoch [42/800], Batch [50/98], Loss: 5.5012, LR: 0.596120\nEpoch [42/800], Batch [60/98], Loss: 5.4918, LR: 0.596120\nEpoch [42/800], Batch [70/98], Loss: 5.5016, LR: 0.596120\nEpoch [42/800], Batch [80/98], Loss: 5.4635, LR: 0.596120\nEpoch [42/800], Batch [90/98], Loss: 5.4728, LR: 0.596120\nEpoch [42/800] Average Loss: 5.4667\n\nEpoch [43/800], Batch [0/98], Loss: 5.4573, LR: 0.595929\nEpoch [43/800], Batch [10/98], Loss: 5.5057, LR: 0.595929\nEpoch [43/800], Batch [20/98], Loss: 5.4567, LR: 0.595929\nEpoch [43/800], Batch [30/98], Loss: 5.4769, LR: 0.595929\nEpoch [43/800], Batch [40/98], Loss: 5.4695, LR: 0.595929\nEpoch [43/800], Batch [50/98], Loss: 5.4565, LR: 0.595929\nEpoch [43/800], Batch [60/98], Loss: 5.4670, LR: 0.595929\nEpoch [43/800], Batch [70/98], Loss: 5.4764, LR: 0.595929\nEpoch [43/800], Batch [80/98], Loss: 5.4884, LR: 0.595929\nEpoch [43/800], Batch [90/98], Loss: 5.4454, LR: 0.595929\nEpoch [43/800] Average Loss: 5.4650\n\nEpoch [44/800], Batch [0/98], Loss: 5.4794, LR: 0.595733\nEpoch [44/800], Batch [10/98], Loss: 5.4536, LR: 0.595733\nEpoch [44/800], Batch [20/98], Loss: 5.4704, LR: 0.595733\nEpoch [44/800], Batch [30/98], Loss: 5.4878, LR: 0.595733\nEpoch [44/800], Batch [40/98], Loss: 5.4341, LR: 0.595733\nEpoch [44/800], Batch [50/98], Loss: 5.4550, LR: 0.595733\nEpoch [44/800], Batch [60/98], Loss: 5.4553, LR: 0.595733\nEpoch [44/800], Batch [70/98], Loss: 5.4539, LR: 0.595733\nEpoch [44/800], Batch [80/98], Loss: 5.4851, LR: 0.595733\nEpoch [44/800], Batch [90/98], Loss: 5.4631, LR: 0.595733\nEpoch [44/800] Average Loss: 5.4630\n\nEpoch [45/800], Batch [0/98], Loss: 5.4456, LR: 0.595533\nEpoch [45/800], Batch [10/98], Loss: 5.4429, LR: 0.595533\nEpoch [45/800], Batch [20/98], Loss: 5.4333, LR: 0.595533\nEpoch [45/800], Batch [30/98], Loss: 5.4597, LR: 0.595533\nEpoch [45/800], Batch [40/98], Loss: 5.4681, LR: 0.595533\nEpoch [45/800], Batch [50/98], Loss: 5.4505, LR: 0.595533\nEpoch [45/800], Batch [60/98], Loss: 5.4782, LR: 0.595533\nEpoch [45/800], Batch [70/98], Loss: 5.4636, LR: 0.595533\nEpoch [45/800], Batch [80/98], Loss: 5.4619, LR: 0.595533\nEpoch [45/800], Batch [90/98], Loss: 5.4772, LR: 0.595533\nEpoch [45/800] Average Loss: 5.4569\n\nEpoch [46/800], Batch [0/98], Loss: 5.4409, LR: 0.595328\nEpoch [46/800], Batch [10/98], Loss: 5.4771, LR: 0.595328\nEpoch [46/800], Batch [20/98], Loss: 5.4603, LR: 0.595328\nEpoch [46/800], Batch [30/98], Loss: 5.4836, LR: 0.595328\nEpoch [46/800], Batch [40/98], Loss: 5.4481, LR: 0.595328\nEpoch [46/800], Batch [50/98], Loss: 5.4739, LR: 0.595328\nEpoch [46/800], Batch [60/98], Loss: 5.4459, LR: 0.595328\nEpoch [46/800], Batch [70/98], Loss: 5.4728, LR: 0.595328\nEpoch [46/800], Batch [80/98], Loss: 5.4669, LR: 0.595328\nEpoch [46/800], Batch [90/98], Loss: 5.4862, LR: 0.595328\nEpoch [46/800] Average Loss: 5.4580\n\nEpoch [47/800], Batch [0/98], Loss: 5.4976, LR: 0.595119\nEpoch [47/800], Batch [10/98], Loss: 5.4663, LR: 0.595119\nEpoch [47/800], Batch [20/98], Loss: 5.4431, LR: 0.595119\nEpoch [47/800], Batch [30/98], Loss: 5.4391, LR: 0.595119\nEpoch [47/800], Batch [40/98], Loss: 5.4708, LR: 0.595119\nEpoch [47/800], Batch [50/98], Loss: 5.4291, LR: 0.595119\nEpoch [47/800], Batch [60/98], Loss: 5.4604, LR: 0.595119\nEpoch [47/800], Batch [70/98], Loss: 5.4615, LR: 0.595119\nEpoch [47/800], Batch [80/98], Loss: 5.4649, LR: 0.595119\nEpoch [47/800], Batch [90/98], Loss: 5.4790, LR: 0.595119\nEpoch [47/800] Average Loss: 5.4555\n\nEpoch [48/800], Batch [0/98], Loss: 5.4540, LR: 0.594905\nEpoch [48/800], Batch [10/98], Loss: 5.4510, LR: 0.594905\nEpoch [48/800], Batch [20/98], Loss: 5.4523, LR: 0.594905\nEpoch [48/800], Batch [30/98], Loss: 5.4553, LR: 0.594905\nEpoch [48/800], Batch [40/98], Loss: 5.4631, LR: 0.594905\nEpoch [48/800], Batch [50/98], Loss: 5.4656, LR: 0.594905\nEpoch [48/800], Batch [60/98], Loss: 5.4677, LR: 0.594905\nEpoch [48/800], Batch [70/98], Loss: 5.4843, LR: 0.594905\nEpoch [48/800], Batch [80/98], Loss: 5.4460, LR: 0.594905\nEpoch [48/800], Batch [90/98], Loss: 5.4616, LR: 0.594905\nEpoch [48/800] Average Loss: 5.4535\n\nEpoch [49/800], Batch [0/98], Loss: 5.4942, LR: 0.594686\nEpoch [49/800], Batch [10/98], Loss: 5.4277, LR: 0.594686\nEpoch [49/800], Batch [20/98], Loss: 5.4653, LR: 0.594686\nEpoch [49/800], Batch [30/98], Loss: 5.4469, LR: 0.594686\nEpoch [49/800], Batch [40/98], Loss: 5.4221, LR: 0.594686\nEpoch [49/800], Batch [50/98], Loss: 5.4263, LR: 0.594686\nEpoch [49/800], Batch [60/98], Loss: 5.4748, LR: 0.594686\nEpoch [49/800], Batch [70/98], Loss: 5.4623, LR: 0.594686\nEpoch [49/800], Batch [80/98], Loss: 5.4765, LR: 0.594686\nEpoch [49/800], Batch [90/98], Loss: 5.4583, LR: 0.594686\nEpoch [49/800] Average Loss: 5.4514\n\nEpoch [50/800], Batch [0/98], Loss: 5.4864, LR: 0.594463\nEpoch [50/800], Batch [10/98], Loss: 5.4592, LR: 0.594463\nEpoch [50/800], Batch [20/98], Loss: 5.4517, LR: 0.594463\nEpoch [50/800], Batch [30/98], Loss: 5.4634, LR: 0.594463\nEpoch [50/800], Batch [40/98], Loss: 5.4299, LR: 0.594463\nEpoch [50/800], Batch [50/98], Loss: 5.4371, LR: 0.594463\nEpoch [50/800], Batch [60/98], Loss: 5.4374, LR: 0.594463\nEpoch [50/800], Batch [70/98], Loss: 5.4312, LR: 0.594463\nEpoch [50/800], Batch [80/98], Loss: 5.4607, LR: 0.594463\nEpoch [50/800], Batch [90/98], Loss: 5.4327, LR: 0.594463\nEpoch [50/800] Average Loss: 5.4469\n\nEpoch [51/800], Batch [0/98], Loss: 5.4341, LR: 0.594236\nEpoch [51/800], Batch [10/98], Loss: 5.4319, LR: 0.594236\nEpoch [51/800], Batch [20/98], Loss: 5.4609, LR: 0.594236\nEpoch [51/800], Batch [30/98], Loss: 5.4349, LR: 0.594236\nEpoch [51/800], Batch [40/98], Loss: 5.4543, LR: 0.594236\nEpoch [51/800], Batch [50/98], Loss: 5.4812, LR: 0.594236\nEpoch [51/800], Batch [60/98], Loss: 5.4719, LR: 0.594236\nEpoch [51/800], Batch [70/98], Loss: 5.4498, LR: 0.594236\nEpoch [51/800], Batch [80/98], Loss: 5.4566, LR: 0.594236\nEpoch [51/800], Batch [90/98], Loss: 5.4318, LR: 0.594236\nEpoch [51/800] Average Loss: 5.4472\n\nEpoch [52/800], Batch [0/98], Loss: 5.4408, LR: 0.594003\nEpoch [52/800], Batch [10/98], Loss: 5.4716, LR: 0.594003\nEpoch [52/800], Batch [20/98], Loss: 5.4684, LR: 0.594003\nEpoch [52/800], Batch [30/98], Loss: 5.4409, LR: 0.594003\nEpoch [52/800], Batch [40/98], Loss: 5.4655, LR: 0.594003\nEpoch [52/800], Batch [50/98], Loss: 5.4698, LR: 0.594003\nEpoch [52/800], Batch [60/98], Loss: 5.4509, LR: 0.594003\nEpoch [52/800], Batch [70/98], Loss: 5.4406, LR: 0.594003\nEpoch [52/800], Batch [80/98], Loss: 5.4389, LR: 0.594003\nEpoch [52/800], Batch [90/98], Loss: 5.4634, LR: 0.594003\nEpoch [52/800] Average Loss: 5.4444\n\nEpoch [53/800], Batch [0/98], Loss: 5.4329, LR: 0.593767\nEpoch [53/800], Batch [10/98], Loss: 5.4469, LR: 0.593767\nEpoch [53/800], Batch [20/98], Loss: 5.4221, LR: 0.593767\nEpoch [53/800], Batch [30/98], Loss: 5.4713, LR: 0.593767\nEpoch [53/800], Batch [40/98], Loss: 5.4427, LR: 0.593767\nEpoch [53/800], Batch [50/98], Loss: 5.4448, LR: 0.593767\nEpoch [53/800], Batch [60/98], Loss: 5.4312, LR: 0.593767\nEpoch [53/800], Batch [70/98], Loss: 5.4418, LR: 0.593767\nEpoch [53/800], Batch [80/98], Loss: 5.4395, LR: 0.593767\nEpoch [53/800], Batch [90/98], Loss: 5.4198, LR: 0.593767\nEpoch [53/800] Average Loss: 5.4432\n\nEpoch [54/800], Batch [0/98], Loss: 5.4429, LR: 0.593526\nEpoch [54/800], Batch [10/98], Loss: 5.4280, LR: 0.593526\nEpoch [54/800], Batch [20/98], Loss: 5.4635, LR: 0.593526\nEpoch [54/800], Batch [30/98], Loss: 5.4363, LR: 0.593526\nEpoch [54/800], Batch [40/98], Loss: 5.4572, LR: 0.593526\nEpoch [54/800], Batch [50/98], Loss: 5.4474, LR: 0.593526\nEpoch [54/800], Batch [60/98], Loss: 5.4373, LR: 0.593526\nEpoch [54/800], Batch [70/98], Loss: 5.4474, LR: 0.593526\nEpoch [54/800], Batch [80/98], Loss: 5.4476, LR: 0.593526\nEpoch [54/800], Batch [90/98], Loss: 5.4577, LR: 0.593526\nEpoch [54/800] Average Loss: 5.4420\n\nEpoch [55/800], Batch [0/98], Loss: 5.4138, LR: 0.593280\nEpoch [55/800], Batch [10/98], Loss: 5.4466, LR: 0.593280\nEpoch [55/800], Batch [20/98], Loss: 5.4656, LR: 0.593280\nEpoch [55/800], Batch [30/98], Loss: 5.4331, LR: 0.593280\nEpoch [55/800], Batch [40/98], Loss: 5.4504, LR: 0.593280\nEpoch [55/800], Batch [50/98], Loss: 5.4456, LR: 0.593280\nEpoch [55/800], Batch [60/98], Loss: 5.4440, LR: 0.593280\nEpoch [55/800], Batch [70/98], Loss: 5.4317, LR: 0.593280\nEpoch [55/800], Batch [80/98], Loss: 5.4446, LR: 0.593280\nEpoch [55/800], Batch [90/98], Loss: 5.4372, LR: 0.593280\nEpoch [55/800] Average Loss: 5.4372\n\nEpoch [56/800], Batch [0/98], Loss: 5.4374, LR: 0.593030\nEpoch [56/800], Batch [10/98], Loss: 5.4119, LR: 0.593030\nEpoch [56/800], Batch [20/98], Loss: 5.4418, LR: 0.593030\nEpoch [56/800], Batch [30/98], Loss: 5.4254, LR: 0.593030\nEpoch [56/800], Batch [40/98], Loss: 5.4761, LR: 0.593030\nEpoch [56/800], Batch [50/98], Loss: 5.4370, LR: 0.593030\nEpoch [56/800], Batch [60/98], Loss: 5.4386, LR: 0.593030\nEpoch [56/800], Batch [70/98], Loss: 5.4432, LR: 0.593030\nEpoch [56/800], Batch [80/98], Loss: 5.4543, LR: 0.593030\nEpoch [56/800], Batch [90/98], Loss: 5.4524, LR: 0.593030\nEpoch [56/800] Average Loss: 5.4364\n\nEpoch [57/800], Batch [0/98], Loss: 5.4319, LR: 0.592775\nEpoch [57/800], Batch [10/98], Loss: 5.4292, LR: 0.592775\nEpoch [57/800], Batch [20/98], Loss: 5.4566, LR: 0.592775\nEpoch [57/800], Batch [30/98], Loss: 5.4480, LR: 0.592775\nEpoch [57/800], Batch [40/98], Loss: 5.4434, LR: 0.592775\nEpoch [57/800], Batch [50/98], Loss: 5.4136, LR: 0.592775\nEpoch [57/800], Batch [60/98], Loss: 5.4250, LR: 0.592775\nEpoch [57/800], Batch [70/98], Loss: 5.4292, LR: 0.592775\nEpoch [57/800], Batch [80/98], Loss: 5.4653, LR: 0.592775\nEpoch [57/800], Batch [90/98], Loss: 5.4503, LR: 0.592775\nEpoch [57/800] Average Loss: 5.4389\n\nEpoch [58/800], Batch [0/98], Loss: 5.4518, LR: 0.592516\nEpoch [58/800], Batch [10/98], Loss: 5.4455, LR: 0.592516\nEpoch [58/800], Batch [20/98], Loss: 5.4282, LR: 0.592516\nEpoch [58/800], Batch [30/98], Loss: 5.4351, LR: 0.592516\nEpoch [58/800], Batch [40/98], Loss: 5.4409, LR: 0.592516\nEpoch [58/800], Batch [50/98], Loss: 5.4273, LR: 0.592516\nEpoch [58/800], Batch [60/98], Loss: 5.4565, LR: 0.592516\nEpoch [58/800], Batch [70/98], Loss: 5.4175, LR: 0.592516\nEpoch [58/800], Batch [80/98], Loss: 5.4206, LR: 0.592516\nEpoch [58/800], Batch [90/98], Loss: 5.4439, LR: 0.592516\nEpoch [58/800] Average Loss: 5.4308\n\nEpoch [59/800], Batch [0/98], Loss: 5.4171, LR: 0.592252\nEpoch [59/800], Batch [10/98], Loss: 5.4620, LR: 0.592252\nEpoch [59/800], Batch [20/98], Loss: 5.4403, LR: 0.592252\nEpoch [59/800], Batch [30/98], Loss: 5.4534, LR: 0.592252\nEpoch [59/800], Batch [40/98], Loss: 5.4601, LR: 0.592252\nEpoch [59/800], Batch [50/98], Loss: 5.4202, LR: 0.592252\nEpoch [59/800], Batch [60/98], Loss: 5.4334, LR: 0.592252\nEpoch [59/800], Batch [70/98], Loss: 5.4754, LR: 0.592252\nEpoch [59/800], Batch [80/98], Loss: 5.4382, LR: 0.592252\nEpoch [59/800], Batch [90/98], Loss: 5.4622, LR: 0.592252\nEpoch [59/800] Average Loss: 5.4347\n\nEpoch [60/800], Batch [0/98], Loss: 5.4681, LR: 0.591984\nEpoch [60/800], Batch [10/98], Loss: 5.4341, LR: 0.591984\nEpoch [60/800], Batch [20/98], Loss: 5.4672, LR: 0.591984\nEpoch [60/800], Batch [30/98], Loss: 5.4504, LR: 0.591984\nEpoch [60/800], Batch [40/98], Loss: 5.4532, LR: 0.591984\nEpoch [60/800], Batch [50/98], Loss: 5.4221, LR: 0.591984\nEpoch [60/800], Batch [60/98], Loss: 5.4787, LR: 0.591984\nEpoch [60/800], Batch [70/98], Loss: 5.4298, LR: 0.591984\nEpoch [60/800], Batch [80/98], Loss: 5.4063, LR: 0.591984\nEpoch [60/800], Batch [90/98], Loss: 5.4405, LR: 0.591984\nEpoch [60/800] Average Loss: 5.4321\n\nEpoch [61/800], Batch [0/98], Loss: 5.4467, LR: 0.591711\nEpoch [61/800], Batch [10/98], Loss: 5.4263, LR: 0.591711\nEpoch [61/800], Batch [20/98], Loss: 5.4327, LR: 0.591711\nEpoch [61/800], Batch [30/98], Loss: 5.4359, LR: 0.591711\nEpoch [61/800], Batch [40/98], Loss: 5.4293, LR: 0.591711\nEpoch [61/800], Batch [50/98], Loss: 5.4289, LR: 0.591711\nEpoch [61/800], Batch [60/98], Loss: 5.4022, LR: 0.591711\nEpoch [61/800], Batch [70/98], Loss: 5.4223, LR: 0.591711\nEpoch [61/800], Batch [80/98], Loss: 5.4300, LR: 0.591711\nEpoch [61/800], Batch [90/98], Loss: 5.4114, LR: 0.591711\nEpoch [61/800] Average Loss: 5.4283\n\nEpoch [62/800], Batch [0/98], Loss: 5.4274, LR: 0.591434\nEpoch [62/800], Batch [10/98], Loss: 5.4026, LR: 0.591434\nEpoch [62/800], Batch [20/98], Loss: 5.4091, LR: 0.591434\nEpoch [62/800], Batch [30/98], Loss: 5.4240, LR: 0.591434\nEpoch [62/800], Batch [40/98], Loss: 5.4470, LR: 0.591434\nEpoch [62/800], Batch [50/98], Loss: 5.4382, LR: 0.591434\nEpoch [62/800], Batch [60/98], Loss: 5.4351, LR: 0.591434\nEpoch [62/800], Batch [70/98], Loss: 5.4487, LR: 0.591434\nEpoch [62/800], Batch [80/98], Loss: 5.4371, LR: 0.591434\nEpoch [62/800], Batch [90/98], Loss: 5.4149, LR: 0.591434\nEpoch [62/800] Average Loss: 5.4261\n\nEpoch [63/800], Batch [0/98], Loss: 5.4593, LR: 0.591152\nEpoch [63/800], Batch [10/98], Loss: 5.4453, LR: 0.591152\nEpoch [63/800], Batch [20/98], Loss: 5.4503, LR: 0.591152\nEpoch [63/800], Batch [30/98], Loss: 5.4314, LR: 0.591152\nEpoch [63/800], Batch [40/98], Loss: 5.4120, LR: 0.591152\nEpoch [63/800], Batch [50/98], Loss: 5.4544, LR: 0.591152\nEpoch [63/800], Batch [60/98], Loss: 5.4412, LR: 0.591152\nEpoch [63/800], Batch [70/98], Loss: 5.4355, LR: 0.591152\nEpoch [63/800], Batch [80/98], Loss: 5.4378, LR: 0.591152\nEpoch [63/800], Batch [90/98], Loss: 5.4216, LR: 0.591152\nEpoch [63/800] Average Loss: 5.4282\n\nEpoch [64/800], Batch [0/98], Loss: 5.4531, LR: 0.590866\nEpoch [64/800], Batch [10/98], Loss: 5.4371, LR: 0.590866\nEpoch [64/800], Batch [20/98], Loss: 5.4458, LR: 0.590866\nEpoch [64/800], Batch [30/98], Loss: 5.4237, LR: 0.590866\nEpoch [64/800], Batch [40/98], Loss: 5.4163, LR: 0.590866\nEpoch [64/800], Batch [50/98], Loss: 5.4367, LR: 0.590866\nEpoch [64/800], Batch [60/98], Loss: 5.4651, LR: 0.590866\nEpoch [64/800], Batch [70/98], Loss: 5.4489, LR: 0.590866\nEpoch [64/800], Batch [80/98], Loss: 5.4377, LR: 0.590866\nEpoch [64/800], Batch [90/98], Loss: 5.4261, LR: 0.590866\nEpoch [64/800] Average Loss: 5.4218\n\nEpoch [65/800], Batch [0/98], Loss: 5.4062, LR: 0.590575\nEpoch [65/800], Batch [10/98], Loss: 5.4238, LR: 0.590575\nEpoch [65/800], Batch [20/98], Loss: 5.4057, LR: 0.590575\nEpoch [65/800], Batch [30/98], Loss: 5.4192, LR: 0.590575\nEpoch [65/800], Batch [40/98], Loss: 5.4045, LR: 0.590575\nEpoch [65/800], Batch [50/98], Loss: 5.4069, LR: 0.590575\nEpoch [65/800], Batch [60/98], Loss: 5.4459, LR: 0.590575\nEpoch [65/800], Batch [70/98], Loss: 5.4113, LR: 0.590575\nEpoch [65/800], Batch [80/98], Loss: 5.4417, LR: 0.590575\nEpoch [65/800], Batch [90/98], Loss: 5.4331, LR: 0.590575\nEpoch [65/800] Average Loss: 5.4212\n\nEpoch [66/800], Batch [0/98], Loss: 5.4314, LR: 0.590280\nEpoch [66/800], Batch [10/98], Loss: 5.4201, LR: 0.590280\nEpoch [66/800], Batch [20/98], Loss: 5.4292, LR: 0.590280\nEpoch [66/800], Batch [30/98], Loss: 5.4071, LR: 0.590280\nEpoch [66/800], Batch [40/98], Loss: 5.4770, LR: 0.590280\nEpoch [66/800], Batch [50/98], Loss: 5.4484, LR: 0.590280\nEpoch [66/800], Batch [60/98], Loss: 5.4234, LR: 0.590280\nEpoch [66/800], Batch [70/98], Loss: 5.4396, LR: 0.590280\nEpoch [66/800], Batch [80/98], Loss: 5.4418, LR: 0.590280\nEpoch [66/800], Batch [90/98], Loss: 5.4165, LR: 0.590280\nEpoch [66/800] Average Loss: 5.4248\n\nEpoch [67/800], Batch [0/98], Loss: 5.4229, LR: 0.589980\nEpoch [67/800], Batch [10/98], Loss: 5.3799, LR: 0.589980\nEpoch [67/800], Batch [20/98], Loss: 5.4180, LR: 0.589980\nEpoch [67/800], Batch [30/98], Loss: 5.4222, LR: 0.589980\nEpoch [67/800], Batch [40/98], Loss: 5.4462, LR: 0.589980\nEpoch [67/800], Batch [50/98], Loss: 5.4267, LR: 0.589980\nEpoch [67/800], Batch [60/98], Loss: 5.4320, LR: 0.589980\nEpoch [67/800], Batch [70/98], Loss: 5.4435, LR: 0.589980\nEpoch [67/800], Batch [80/98], Loss: 5.4576, LR: 0.589980\nEpoch [67/800], Batch [90/98], Loss: 5.4504, LR: 0.589980\nEpoch [67/800] Average Loss: 5.4213\n\nEpoch [68/800], Batch [0/98], Loss: 5.4486, LR: 0.589676\nEpoch [68/800], Batch [10/98], Loss: 5.4162, LR: 0.589676\nEpoch [68/800], Batch [20/98], Loss: 5.4259, LR: 0.589676\nEpoch [68/800], Batch [30/98], Loss: 5.4504, LR: 0.589676\nEpoch [68/800], Batch [40/98], Loss: 5.4281, LR: 0.589676\nEpoch [68/800], Batch [50/98], Loss: 5.4241, LR: 0.589676\nEpoch [68/800], Batch [60/98], Loss: 5.4115, LR: 0.589676\nEpoch [68/800], Batch [70/98], Loss: 5.4286, LR: 0.589676\nEpoch [68/800], Batch [80/98], Loss: 5.3901, LR: 0.589676\nEpoch [68/800], Batch [90/98], Loss: 5.4566, LR: 0.589676\nEpoch [68/800] Average Loss: 5.4175\n\nEpoch [69/800], Batch [0/98], Loss: 5.4144, LR: 0.589367\nEpoch [69/800], Batch [10/98], Loss: 5.3847, LR: 0.589367\nEpoch [69/800], Batch [20/98], Loss: 5.3788, LR: 0.589367\nEpoch [69/800], Batch [30/98], Loss: 5.4221, LR: 0.589367\nEpoch [69/800], Batch [40/98], Loss: 5.4402, LR: 0.589367\nEpoch [69/800], Batch [50/98], Loss: 5.4082, LR: 0.589367\nEpoch [69/800], Batch [60/98], Loss: 5.4014, LR: 0.589367\nEpoch [69/800], Batch [70/98], Loss: 5.4141, LR: 0.589367\nEpoch [69/800], Batch [80/98], Loss: 5.4419, LR: 0.589367\nEpoch [69/800], Batch [90/98], Loss: 5.4462, LR: 0.589367\nEpoch [69/800] Average Loss: 5.4184\n\nEpoch [70/800], Batch [0/98], Loss: 5.4361, LR: 0.589054\nEpoch [70/800], Batch [10/98], Loss: 5.4281, LR: 0.589054\nEpoch [70/800], Batch [20/98], Loss: 5.4185, LR: 0.589054\nEpoch [70/800], Batch [30/98], Loss: 5.4100, LR: 0.589054\nEpoch [70/800], Batch [40/98], Loss: 5.4107, LR: 0.589054\nEpoch [70/800], Batch [50/98], Loss: 5.4084, LR: 0.589054\nEpoch [70/800], Batch [60/98], Loss: 5.3923, LR: 0.589054\nEpoch [70/800], Batch [70/98], Loss: 5.4222, LR: 0.589054\nEpoch [70/800], Batch [80/98], Loss: 5.4009, LR: 0.589054\nEpoch [70/800], Batch [90/98], Loss: 5.4433, LR: 0.589054\nEpoch [70/800] Average Loss: 5.4131\n\nEpoch [71/800], Batch [0/98], Loss: 5.4273, LR: 0.588737\nEpoch [71/800], Batch [10/98], Loss: 5.4240, LR: 0.588737\nEpoch [71/800], Batch [20/98], Loss: 5.4281, LR: 0.588737\nEpoch [71/800], Batch [30/98], Loss: 5.4038, LR: 0.588737\nEpoch [71/800], Batch [40/98], Loss: 5.4029, LR: 0.588737\nEpoch [71/800], Batch [50/98], Loss: 5.4024, LR: 0.588737\nEpoch [71/800], Batch [60/98], Loss: 5.4304, LR: 0.588737\nEpoch [71/800], Batch [70/98], Loss: 5.4211, LR: 0.588737\nEpoch [71/800], Batch [80/98], Loss: 5.4343, LR: 0.588737\nEpoch [71/800], Batch [90/98], Loss: 5.4266, LR: 0.588737\nEpoch [71/800] Average Loss: 5.4149\n\nEpoch [72/800], Batch [0/98], Loss: 5.4330, LR: 0.588415\nEpoch [72/800], Batch [10/98], Loss: 5.4284, LR: 0.588415\nEpoch [72/800], Batch [20/98], Loss: 5.4377, LR: 0.588415\nEpoch [72/800], Batch [30/98], Loss: 5.4178, LR: 0.588415\nEpoch [72/800], Batch [40/98], Loss: 5.4189, LR: 0.588415\nEpoch [72/800], Batch [50/98], Loss: 5.4222, LR: 0.588415\nEpoch [72/800], Batch [60/98], Loss: 5.4005, LR: 0.588415\nEpoch [72/800], Batch [70/98], Loss: 5.4266, LR: 0.588415\nEpoch [72/800], Batch [80/98], Loss: 5.4373, LR: 0.588415\nEpoch [72/800], Batch [90/98], Loss: 5.4351, LR: 0.588415\nEpoch [72/800] Average Loss: 5.4155\n\nEpoch [73/800], Batch [0/98], Loss: 5.4284, LR: 0.588088\nEpoch [73/800], Batch [10/98], Loss: 5.4058, LR: 0.588088\nEpoch [73/800], Batch [20/98], Loss: 5.4200, LR: 0.588088\nEpoch [73/800], Batch [30/98], Loss: 5.3914, LR: 0.588088\nEpoch [73/800], Batch [40/98], Loss: 5.4274, LR: 0.588088\nEpoch [73/800], Batch [50/98], Loss: 5.4230, LR: 0.588088\nEpoch [73/800], Batch [60/98], Loss: 5.3832, LR: 0.588088\nEpoch [73/800], Batch [70/98], Loss: 5.3991, LR: 0.588088\nEpoch [73/800], Batch [80/98], Loss: 5.4115, LR: 0.588088\nEpoch [73/800], Batch [90/98], Loss: 5.4144, LR: 0.588088\nEpoch [73/800] Average Loss: 5.4131\n\nEpoch [74/800], Batch [0/98], Loss: 5.3891, LR: 0.587757\nEpoch [74/800], Batch [10/98], Loss: 5.4175, LR: 0.587757\nEpoch [74/800], Batch [20/98], Loss: 5.4264, LR: 0.587757\nEpoch [74/800], Batch [30/98], Loss: 5.3885, LR: 0.587757\nEpoch [74/800], Batch [40/98], Loss: 5.4613, LR: 0.587757\nEpoch [74/800], Batch [50/98], Loss: 5.3869, LR: 0.587757\nEpoch [74/800], Batch [60/98], Loss: 5.4256, LR: 0.587757\nEpoch [74/800], Batch [70/98], Loss: 5.4412, LR: 0.587757\nEpoch [74/800], Batch [80/98], Loss: 5.4113, LR: 0.587757\nEpoch [74/800], Batch [90/98], Loss: 5.4482, LR: 0.587757\nEpoch [74/800] Average Loss: 5.4118\n\nEpoch [75/800], Batch [0/98], Loss: 5.4310, LR: 0.587422\nEpoch [75/800], Batch [10/98], Loss: 5.4233, LR: 0.587422\nEpoch [75/800], Batch [20/98], Loss: 5.4261, LR: 0.587422\nEpoch [75/800], Batch [30/98], Loss: 5.4394, LR: 0.587422\nEpoch [75/800], Batch [40/98], Loss: 5.4173, LR: 0.587422\nEpoch [75/800], Batch [50/98], Loss: 5.4064, LR: 0.587422\nEpoch [75/800], Batch [60/98], Loss: 5.3768, LR: 0.587422\nEpoch [75/800], Batch [70/98], Loss: 5.4122, LR: 0.587422\nEpoch [75/800], Batch [80/98], Loss: 5.3943, LR: 0.587422\nEpoch [75/800], Batch [90/98], Loss: 5.4295, LR: 0.587422\nEpoch [75/800] Average Loss: 5.4087\n\nEpoch [76/800], Batch [0/98], Loss: 5.3842, LR: 0.587082\nEpoch [76/800], Batch [10/98], Loss: 5.4277, LR: 0.587082\nEpoch [76/800], Batch [20/98], Loss: 5.3744, LR: 0.587082\nEpoch [76/800], Batch [30/98], Loss: 5.4397, LR: 0.587082\nEpoch [76/800], Batch [40/98], Loss: 5.4000, LR: 0.587082\nEpoch [76/800], Batch [50/98], Loss: 5.4091, LR: 0.587082\nEpoch [76/800], Batch [60/98], Loss: 5.3829, LR: 0.587082\nEpoch [76/800], Batch [70/98], Loss: 5.4156, LR: 0.587082\nEpoch [76/800], Batch [80/98], Loss: 5.4163, LR: 0.587082\nEpoch [76/800], Batch [90/98], Loss: 5.4064, LR: 0.587082\nEpoch [76/800] Average Loss: 5.4078\n\nEpoch [77/800], Batch [0/98], Loss: 5.4038, LR: 0.586738\nEpoch [77/800], Batch [10/98], Loss: 5.4000, LR: 0.586738\nEpoch [77/800], Batch [20/98], Loss: 5.4530, LR: 0.586738\nEpoch [77/800], Batch [30/98], Loss: 5.4120, LR: 0.586738\nEpoch [77/800], Batch [40/98], Loss: 5.3945, LR: 0.586738\nEpoch [77/800], Batch [50/98], Loss: 5.4195, LR: 0.586738\nEpoch [77/800], Batch [60/98], Loss: 5.4134, LR: 0.586738\nEpoch [77/800], Batch [70/98], Loss: 5.4071, LR: 0.586738\nEpoch [77/800], Batch [80/98], Loss: 5.4132, LR: 0.586738\nEpoch [77/800], Batch [90/98], Loss: 5.3995, LR: 0.586738\nEpoch [77/800] Average Loss: 5.4097\n\nEpoch [78/800], Batch [0/98], Loss: 5.3848, LR: 0.586389\nEpoch [78/800], Batch [10/98], Loss: 5.4088, LR: 0.586389\nEpoch [78/800], Batch [20/98], Loss: 5.4198, LR: 0.586389\nEpoch [78/800], Batch [30/98], Loss: 5.3995, LR: 0.586389\nEpoch [78/800], Batch [40/98], Loss: 5.4103, LR: 0.586389\nEpoch [78/800], Batch [50/98], Loss: 5.4197, LR: 0.586389\nEpoch [78/800], Batch [60/98], Loss: 5.4195, LR: 0.586389\nEpoch [78/800], Batch [70/98], Loss: 5.4110, LR: 0.586389\nEpoch [78/800], Batch [80/98], Loss: 5.3912, LR: 0.586389\nEpoch [78/800], Batch [90/98], Loss: 5.3995, LR: 0.586389\nEpoch [78/800] Average Loss: 5.4063\n\nEpoch [79/800], Batch [0/98], Loss: 5.4176, LR: 0.586036\nEpoch [79/800], Batch [10/98], Loss: 5.4150, LR: 0.586036\nEpoch [79/800], Batch [20/98], Loss: 5.4328, LR: 0.586036\nEpoch [79/800], Batch [30/98], Loss: 5.4025, LR: 0.586036\nEpoch [79/800], Batch [40/98], Loss: 5.4135, LR: 0.586036\nEpoch [79/800], Batch [50/98], Loss: 5.4210, LR: 0.586036\nEpoch [79/800], Batch [60/98], Loss: 5.3957, LR: 0.586036\nEpoch [79/800], Batch [70/98], Loss: 5.4076, LR: 0.586036\nEpoch [79/800], Batch [80/98], Loss: 5.3928, LR: 0.586036\nEpoch [79/800], Batch [90/98], Loss: 5.4214, LR: 0.586036\nEpoch [79/800] Average Loss: 5.4066\n\nEpoch [80/800], Batch [0/98], Loss: 5.4023, LR: 0.585679\nEpoch [80/800], Batch [10/98], Loss: 5.4120, LR: 0.585679\nEpoch [80/800], Batch [20/98], Loss: 5.4010, LR: 0.585679\nEpoch [80/800], Batch [30/98], Loss: 5.3928, LR: 0.585679\nEpoch [80/800], Batch [40/98], Loss: 5.4067, LR: 0.585679\nEpoch [80/800], Batch [50/98], Loss: 5.3960, LR: 0.585679\nEpoch [80/800], Batch [60/98], Loss: 5.4095, LR: 0.585679\nEpoch [80/800], Batch [70/98], Loss: 5.4155, LR: 0.585679\nEpoch [80/800], Batch [80/98], Loss: 5.4044, LR: 0.585679\nEpoch [80/800], Batch [90/98], Loss: 5.4531, LR: 0.585679\nEpoch [80/800] Average Loss: 5.4069\n\nEpoch [81/800], Batch [0/98], Loss: 5.4097, LR: 0.585317\nEpoch [81/800], Batch [10/98], Loss: 5.3977, LR: 0.585317\nEpoch [81/800], Batch [20/98], Loss: 5.4030, LR: 0.585317\nEpoch [81/800], Batch [30/98], Loss: 5.4080, LR: 0.585317\nEpoch [81/800], Batch [40/98], Loss: 5.4122, LR: 0.585317\nEpoch [81/800], Batch [50/98], Loss: 5.4318, LR: 0.585317\nEpoch [81/800], Batch [60/98], Loss: 5.3845, LR: 0.585317\nEpoch [81/800], Batch [70/98], Loss: 5.4188, LR: 0.585317\nEpoch [81/800], Batch [80/98], Loss: 5.4205, LR: 0.585317\nEpoch [81/800], Batch [90/98], Loss: 5.4134, LR: 0.585317\nEpoch [81/800] Average Loss: 5.4019\n\nEpoch [82/800], Batch [0/98], Loss: 5.3920, LR: 0.584951\nEpoch [82/800], Batch [10/98], Loss: 5.3937, LR: 0.584951\nEpoch [82/800], Batch [20/98], Loss: 5.4129, LR: 0.584951\nEpoch [82/800], Batch [30/98], Loss: 5.4068, LR: 0.584951\nEpoch [82/800], Batch [40/98], Loss: 5.3917, LR: 0.584951\nEpoch [82/800], Batch [50/98], Loss: 5.3882, LR: 0.584951\nEpoch [82/800], Batch [60/98], Loss: 5.4266, LR: 0.584951\nEpoch [82/800], Batch [70/98], Loss: 5.3808, LR: 0.584951\nEpoch [82/800], Batch [80/98], Loss: 5.4201, LR: 0.584951\nEpoch [82/800], Batch [90/98], Loss: 5.4110, LR: 0.584951\nEpoch [82/800] Average Loss: 5.4021\n\nEpoch [83/800], Batch [0/98], Loss: 5.3903, LR: 0.584580\nEpoch [83/800], Batch [10/98], Loss: 5.3765, LR: 0.584580\nEpoch [83/800], Batch [20/98], Loss: 5.3945, LR: 0.584580\nEpoch [83/800], Batch [30/98], Loss: 5.4083, LR: 0.584580\nEpoch [83/800], Batch [40/98], Loss: 5.4061, LR: 0.584580\nEpoch [83/800], Batch [50/98], Loss: 5.3930, LR: 0.584580\nEpoch [83/800], Batch [60/98], Loss: 5.4242, LR: 0.584580\nEpoch [83/800], Batch [70/98], Loss: 5.4296, LR: 0.584580\nEpoch [83/800], Batch [80/98], Loss: 5.4230, LR: 0.584580\nEpoch [83/800], Batch [90/98], Loss: 5.4132, LR: 0.584580\nEpoch [83/800] Average Loss: 5.4018\n\nEpoch [84/800], Batch [0/98], Loss: 5.3978, LR: 0.584205\nEpoch [84/800], Batch [10/98], Loss: 5.3956, LR: 0.584205\nEpoch [84/800], Batch [20/98], Loss: 5.4189, LR: 0.584205\nEpoch [84/800], Batch [30/98], Loss: 5.3970, LR: 0.584205\nEpoch [84/800], Batch [40/98], Loss: 5.4128, LR: 0.584205\nEpoch [84/800], Batch [50/98], Loss: 5.4009, LR: 0.584205\nEpoch [84/800], Batch [60/98], Loss: 5.3971, LR: 0.584205\nEpoch [84/800], Batch [70/98], Loss: 5.3884, LR: 0.584205\nEpoch [84/800], Batch [80/98], Loss: 5.3661, LR: 0.584205\nEpoch [84/800], Batch [90/98], Loss: 5.4025, LR: 0.584205\nEpoch [84/800] Average Loss: 5.3955\n\nEpoch [85/800], Batch [0/98], Loss: 5.4120, LR: 0.583826\nEpoch [85/800], Batch [10/98], Loss: 5.4208, LR: 0.583826\nEpoch [85/800], Batch [20/98], Loss: 5.3889, LR: 0.583826\nEpoch [85/800], Batch [30/98], Loss: 5.4104, LR: 0.583826\nEpoch [85/800], Batch [40/98], Loss: 5.4009, LR: 0.583826\nEpoch [85/800], Batch [50/98], Loss: 5.3849, LR: 0.583826\nEpoch [85/800], Batch [60/98], Loss: 5.3873, LR: 0.583826\nEpoch [85/800], Batch [70/98], Loss: 5.3947, LR: 0.583826\nEpoch [85/800], Batch [80/98], Loss: 5.4155, LR: 0.583826\nEpoch [85/800], Batch [90/98], Loss: 5.3899, LR: 0.583826\nEpoch [85/800] Average Loss: 5.3968\n\nEpoch [86/800], Batch [0/98], Loss: 5.3767, LR: 0.583442\nEpoch [86/800], Batch [10/98], Loss: 5.3958, LR: 0.583442\nEpoch [86/800], Batch [20/98], Loss: 5.4151, LR: 0.583442\nEpoch [86/800], Batch [30/98], Loss: 5.3975, LR: 0.583442\nEpoch [86/800], Batch [40/98], Loss: 5.4070, LR: 0.583442\nEpoch [86/800], Batch [50/98], Loss: 5.3989, LR: 0.583442\nEpoch [86/800], Batch [60/98], Loss: 5.3978, LR: 0.583442\nEpoch [86/800], Batch [70/98], Loss: 5.3939, LR: 0.583442\nEpoch [86/800], Batch [80/98], Loss: 5.3946, LR: 0.583442\nEpoch [86/800], Batch [90/98], Loss: 5.3918, LR: 0.583442\nEpoch [86/800] Average Loss: 5.3980\n\nEpoch [87/800], Batch [0/98], Loss: 5.3881, LR: 0.583054\nEpoch [87/800], Batch [10/98], Loss: 5.4048, LR: 0.583054\nEpoch [87/800], Batch [20/98], Loss: 5.3957, LR: 0.583054\nEpoch [87/800], Batch [30/98], Loss: 5.3984, LR: 0.583054\nEpoch [87/800], Batch [40/98], Loss: 5.4186, LR: 0.583054\nEpoch [87/800], Batch [50/98], Loss: 5.4180, LR: 0.583054\nEpoch [87/800], Batch [60/98], Loss: 5.4040, LR: 0.583054\nEpoch [87/800], Batch [70/98], Loss: 5.3901, LR: 0.583054\nEpoch [87/800], Batch [80/98], Loss: 5.3895, LR: 0.583054\nEpoch [87/800], Batch [90/98], Loss: 5.3890, LR: 0.583054\nEpoch [87/800] Average Loss: 5.3944\n\nEpoch [88/800], Batch [0/98], Loss: 5.4104, LR: 0.582661\nEpoch [88/800], Batch [10/98], Loss: 5.4093, LR: 0.582661\nEpoch [88/800], Batch [20/98], Loss: 5.4148, LR: 0.582661\nEpoch [88/800], Batch [30/98], Loss: 5.3875, LR: 0.582661\nEpoch [88/800], Batch [40/98], Loss: 5.4009, LR: 0.582661\nEpoch [88/800], Batch [50/98], Loss: 5.3982, LR: 0.582661\nEpoch [88/800], Batch [60/98], Loss: 5.4135, LR: 0.582661\nEpoch [88/800], Batch [70/98], Loss: 5.3629, LR: 0.582661\nEpoch [88/800], Batch [80/98], Loss: 5.3896, LR: 0.582661\nEpoch [88/800], Batch [90/98], Loss: 5.4159, LR: 0.582661\nEpoch [88/800] Average Loss: 5.3970\n\nEpoch [89/800], Batch [0/98], Loss: 5.4059, LR: 0.582264\nEpoch [89/800], Batch [10/98], Loss: 5.3974, LR: 0.582264\nEpoch [89/800], Batch [20/98], Loss: 5.3994, LR: 0.582264\nEpoch [89/800], Batch [30/98], Loss: 5.4274, LR: 0.582264\nEpoch [89/800], Batch [40/98], Loss: 5.3992, LR: 0.582264\nEpoch [89/800], Batch [50/98], Loss: 5.3776, LR: 0.582264\nEpoch [89/800], Batch [60/98], Loss: 5.4250, LR: 0.582264\nEpoch [89/800], Batch [70/98], Loss: 5.3931, LR: 0.582264\nEpoch [89/800], Batch [80/98], Loss: 5.3919, LR: 0.582264\nEpoch [89/800], Batch [90/98], Loss: 5.4104, LR: 0.582264\nEpoch [89/800] Average Loss: 5.3956\n\nEpoch [90/800], Batch [0/98], Loss: 5.3847, LR: 0.581863\nEpoch [90/800], Batch [10/98], Loss: 5.4029, LR: 0.581863\nEpoch [90/800], Batch [20/98], Loss: 5.4190, LR: 0.581863\nEpoch [90/800], Batch [30/98], Loss: 5.3935, LR: 0.581863\nEpoch [90/800], Batch [40/98], Loss: 5.3973, LR: 0.581863\nEpoch [90/800], Batch [50/98], Loss: 5.3995, LR: 0.581863\nEpoch [90/800], Batch [60/98], Loss: 5.4032, LR: 0.581863\nEpoch [90/800], Batch [70/98], Loss: 5.3955, LR: 0.581863\nEpoch [90/800], Batch [80/98], Loss: 5.3660, LR: 0.581863\nEpoch [90/800], Batch [90/98], Loss: 5.3985, LR: 0.581863\nEpoch [90/800] Average Loss: 5.3944\n\nEpoch [91/800], Batch [0/98], Loss: 5.4007, LR: 0.581457\nEpoch [91/800], Batch [10/98], Loss: 5.4185, LR: 0.581457\nEpoch [91/800], Batch [20/98], Loss: 5.3952, LR: 0.581457\nEpoch [91/800], Batch [30/98], Loss: 5.3825, LR: 0.581457\nEpoch [91/800], Batch [40/98], Loss: 5.4195, LR: 0.581457\nEpoch [91/800], Batch [50/98], Loss: 5.4074, LR: 0.581457\nEpoch [91/800], Batch [60/98], Loss: 5.3989, LR: 0.581457\nEpoch [91/800], Batch [70/98], Loss: 5.3962, LR: 0.581457\nEpoch [91/800], Batch [80/98], Loss: 5.3937, LR: 0.581457\nEpoch [91/800], Batch [90/98], Loss: 5.3691, LR: 0.581457\nEpoch [91/800] Average Loss: 5.3926\n\nEpoch [92/800], Batch [0/98], Loss: 5.4016, LR: 0.581047\nEpoch [92/800], Batch [10/98], Loss: 5.4067, LR: 0.581047\nEpoch [92/800], Batch [20/98], Loss: 5.3833, LR: 0.581047\nEpoch [92/800], Batch [30/98], Loss: 5.4016, LR: 0.581047\nEpoch [92/800], Batch [40/98], Loss: 5.3778, LR: 0.581047\nEpoch [92/800], Batch [50/98], Loss: 5.3910, LR: 0.581047\nEpoch [92/800], Batch [60/98], Loss: 5.3798, LR: 0.581047\nEpoch [92/800], Batch [70/98], Loss: 5.3815, LR: 0.581047\nEpoch [92/800], Batch [80/98], Loss: 5.4056, LR: 0.581047\nEpoch [92/800], Batch [90/98], Loss: 5.3716, LR: 0.581047\nEpoch [92/800] Average Loss: 5.3924\n\nEpoch [93/800], Batch [0/98], Loss: 5.3998, LR: 0.580633\nEpoch [93/800], Batch [10/98], Loss: 5.3826, LR: 0.580633\nEpoch [93/800], Batch [20/98], Loss: 5.3965, LR: 0.580633\nEpoch [93/800], Batch [30/98], Loss: 5.4087, LR: 0.580633\nEpoch [93/800], Batch [40/98], Loss: 5.4030, LR: 0.580633\nEpoch [93/800], Batch [50/98], Loss: 5.3884, LR: 0.580633\nEpoch [93/800], Batch [60/98], Loss: 5.4029, LR: 0.580633\nEpoch [93/800], Batch [70/98], Loss: 5.3857, LR: 0.580633\nEpoch [93/800], Batch [80/98], Loss: 5.3863, LR: 0.580633\nEpoch [93/800], Batch [90/98], Loss: 5.4017, LR: 0.580633\nEpoch [93/800] Average Loss: 5.3934\n\nEpoch [94/800], Batch [0/98], Loss: 5.4045, LR: 0.580215\nEpoch [94/800], Batch [10/98], Loss: 5.3743, LR: 0.580215\nEpoch [94/800], Batch [20/98], Loss: 5.4133, LR: 0.580215\nEpoch [94/800], Batch [30/98], Loss: 5.3917, LR: 0.580215\nEpoch [94/800], Batch [40/98], Loss: 5.3881, LR: 0.580215\nEpoch [94/800], Batch [50/98], Loss: 5.4216, LR: 0.580215\nEpoch [94/800], Batch [60/98], Loss: 5.3937, LR: 0.580215\nEpoch [94/800], Batch [70/98], Loss: 5.4370, LR: 0.580215\nEpoch [94/800], Batch [80/98], Loss: 5.3881, LR: 0.580215\nEpoch [94/800], Batch [90/98], Loss: 5.4138, LR: 0.580215\nEpoch [94/800] Average Loss: 5.3890\n\nEpoch [95/800], Batch [0/98], Loss: 5.4016, LR: 0.579792\nEpoch [95/800], Batch [10/98], Loss: 5.3704, LR: 0.579792\nEpoch [95/800], Batch [20/98], Loss: 5.3973, LR: 0.579792\nEpoch [95/800], Batch [30/98], Loss: 5.3787, LR: 0.579792\nEpoch [95/800], Batch [40/98], Loss: 5.3946, LR: 0.579792\nEpoch [95/800], Batch [50/98], Loss: 5.4117, LR: 0.579792\nEpoch [95/800], Batch [60/98], Loss: 5.3958, LR: 0.579792\nEpoch [95/800], Batch [70/98], Loss: 5.4105, LR: 0.579792\nEpoch [95/800], Batch [80/98], Loss: 5.4084, LR: 0.579792\nEpoch [95/800], Batch [90/98], Loss: 5.3668, LR: 0.579792\nEpoch [95/800] Average Loss: 5.3905\n\nEpoch [96/800], Batch [0/98], Loss: 5.4085, LR: 0.579364\nEpoch [96/800], Batch [10/98], Loss: 5.3677, LR: 0.579364\nEpoch [96/800], Batch [20/98], Loss: 5.3860, LR: 0.579364\nEpoch [96/800], Batch [30/98], Loss: 5.3665, LR: 0.579364\nEpoch [96/800], Batch [40/98], Loss: 5.3772, LR: 0.579364\nEpoch [96/800], Batch [50/98], Loss: 5.3725, LR: 0.579364\nEpoch [96/800], Batch [60/98], Loss: 5.3900, LR: 0.579364\nEpoch [96/800], Batch [70/98], Loss: 5.3724, LR: 0.579364\nEpoch [96/800], Batch [80/98], Loss: 5.3644, LR: 0.579364\nEpoch [96/800], Batch [90/98], Loss: 5.3854, LR: 0.579364\nEpoch [96/800] Average Loss: 5.3878\n\nEpoch [97/800], Batch [0/98], Loss: 5.3910, LR: 0.578933\nEpoch [97/800], Batch [10/98], Loss: 5.3688, LR: 0.578933\nEpoch [97/800], Batch [20/98], Loss: 5.4056, LR: 0.578933\nEpoch [97/800], Batch [30/98], Loss: 5.3959, LR: 0.578933\nEpoch [97/800], Batch [40/98], Loss: 5.4003, LR: 0.578933\nEpoch [97/800], Batch [50/98], Loss: 5.3996, LR: 0.578933\nEpoch [97/800], Batch [60/98], Loss: 5.3948, LR: 0.578933\nEpoch [97/800], Batch [70/98], Loss: 5.3901, LR: 0.578933\nEpoch [97/800], Batch [80/98], Loss: 5.3647, LR: 0.578933\nEpoch [97/800], Batch [90/98], Loss: 5.3631, LR: 0.578933\nEpoch [97/800] Average Loss: 5.3885\n\nEpoch [98/800], Batch [0/98], Loss: 5.4025, LR: 0.578497\nEpoch [98/800], Batch [10/98], Loss: 5.3772, LR: 0.578497\nEpoch [98/800], Batch [20/98], Loss: 5.4145, LR: 0.578497\nEpoch [98/800], Batch [30/98], Loss: 5.3892, LR: 0.578497\nEpoch [98/800], Batch [40/98], Loss: 5.3831, LR: 0.578497\nEpoch [98/800], Batch [50/98], Loss: 5.3844, LR: 0.578497\nEpoch [98/800], Batch [60/98], Loss: 5.3848, LR: 0.578497\nEpoch [98/800], Batch [70/98], Loss: 5.4037, LR: 0.578497\nEpoch [98/800], Batch [80/98], Loss: 5.3821, LR: 0.578497\nEpoch [98/800], Batch [90/98], Loss: 5.3982, LR: 0.578497\nEpoch [98/800] Average Loss: 5.3863\n\nEpoch [99/800], Batch [0/98], Loss: 5.4076, LR: 0.578057\nEpoch [99/800], Batch [10/98], Loss: 5.4065, LR: 0.578057\nEpoch [99/800], Batch [20/98], Loss: 5.3741, LR: 0.578057\nEpoch [99/800], Batch [30/98], Loss: 5.3799, LR: 0.578057\nEpoch [99/800], Batch [40/98], Loss: 5.3781, LR: 0.578057\nEpoch [99/800], Batch [50/98], Loss: 5.3932, LR: 0.578057\nEpoch [99/800], Batch [60/98], Loss: 5.3846, LR: 0.578057\nEpoch [99/800], Batch [70/98], Loss: 5.3855, LR: 0.578057\nEpoch [99/800], Batch [80/98], Loss: 5.3822, LR: 0.578057\nEpoch [99/800], Batch [90/98], Loss: 5.3911, LR: 0.578057\nEpoch [99/800] Average Loss: 5.3875\n\nEpoch [100/800], Batch [0/98], Loss: 5.4028, LR: 0.577613\nEpoch [100/800], Batch [10/98], Loss: 5.3451, LR: 0.577613\nEpoch [100/800], Batch [20/98], Loss: 5.3851, LR: 0.577613\nEpoch [100/800], Batch [30/98], Loss: 5.3890, LR: 0.577613\nEpoch [100/800], Batch [40/98], Loss: 5.4197, LR: 0.577613\nEpoch [100/800], Batch [50/98], Loss: 5.3978, LR: 0.577613\nEpoch [100/800], Batch [60/98], Loss: 5.3757, LR: 0.577613\nEpoch [100/800], Batch [70/98], Loss: 5.3940, LR: 0.577613\nEpoch [100/800], Batch [80/98], Loss: 5.4055, LR: 0.577613\nEpoch [100/800], Batch [90/98], Loss: 5.4096, LR: 0.577613\nEpoch [100/800] Average Loss: 5.3874\n\nEpoch [101/800], Batch [0/98], Loss: 5.3967, LR: 0.577164\nEpoch [101/800], Batch [10/98], Loss: 5.4016, LR: 0.577164\nEpoch [101/800], Batch [20/98], Loss: 5.3785, LR: 0.577164\nEpoch [101/800], Batch [30/98], Loss: 5.3797, LR: 0.577164\nEpoch [101/800], Batch [40/98], Loss: 5.3900, LR: 0.577164\nEpoch [101/800], Batch [50/98], Loss: 5.3989, LR: 0.577164\nEpoch [101/800], Batch [60/98], Loss: 5.3702, LR: 0.577164\nEpoch [101/800], Batch [70/98], Loss: 5.4249, LR: 0.577164\nEpoch [101/800], Batch [80/98], Loss: 5.4060, LR: 0.577164\nEpoch [101/800], Batch [90/98], Loss: 5.3921, LR: 0.577164\nEpoch [101/800] Average Loss: 5.3865\n\nEpoch [102/800], Batch [0/98], Loss: 5.3872, LR: 0.576711\nEpoch [102/800], Batch [10/98], Loss: 5.3664, LR: 0.576711\nEpoch [102/800], Batch [20/98], Loss: 5.3979, LR: 0.576711\nEpoch [102/800], Batch [30/98], Loss: 5.3739, LR: 0.576711\nEpoch [102/800], Batch [40/98], Loss: 5.3725, LR: 0.576711\nEpoch [102/800], Batch [50/98], Loss: 5.4002, LR: 0.576711\nEpoch [102/800], Batch [60/98], Loss: 5.4137, LR: 0.576711\nEpoch [102/800], Batch [70/98], Loss: 5.3659, LR: 0.576711\nEpoch [102/800], Batch [80/98], Loss: 5.4220, LR: 0.576711\nEpoch [102/800], Batch [90/98], Loss: 5.3643, LR: 0.576711\nEpoch [102/800] Average Loss: 5.3853\n\nEpoch [103/800], Batch [0/98], Loss: 5.3945, LR: 0.576254\nEpoch [103/800], Batch [10/98], Loss: 5.4180, LR: 0.576254\nEpoch [103/800], Batch [20/98], Loss: 5.3946, LR: 0.576254\nEpoch [103/800], Batch [30/98], Loss: 5.4053, LR: 0.576254\nEpoch [103/800], Batch [40/98], Loss: 5.3803, LR: 0.576254\nEpoch [103/800], Batch [50/98], Loss: 5.3851, LR: 0.576254\nEpoch [103/800], Batch [60/98], Loss: 5.3950, LR: 0.576254\nEpoch [103/800], Batch [70/98], Loss: 5.3735, LR: 0.576254\nEpoch [103/800], Batch [80/98], Loss: 5.3810, LR: 0.576254\nEpoch [103/800], Batch [90/98], Loss: 5.3845, LR: 0.576254\nEpoch [103/800] Average Loss: 5.3858\n\nEpoch [104/800], Batch [0/98], Loss: 5.3686, LR: 0.575792\nEpoch [104/800], Batch [10/98], Loss: 5.3719, LR: 0.575792\nEpoch [104/800], Batch [20/98], Loss: 5.3749, LR: 0.575792\nEpoch [104/800], Batch [30/98], Loss: 5.3849, LR: 0.575792\nEpoch [104/800], Batch [40/98], Loss: 5.3777, LR: 0.575792\nEpoch [104/800], Batch [50/98], Loss: 5.3929, LR: 0.575792\nEpoch [104/800], Batch [60/98], Loss: 5.3894, LR: 0.575792\nEpoch [104/800], Batch [70/98], Loss: 5.4023, LR: 0.575792\nEpoch [104/800], Batch [80/98], Loss: 5.3882, LR: 0.575792\nEpoch [104/800], Batch [90/98], Loss: 5.3989, LR: 0.575792\nEpoch [104/800] Average Loss: 5.3818\n\nEpoch [105/800], Batch [0/98], Loss: 5.3987, LR: 0.575326\nEpoch [105/800], Batch [10/98], Loss: 5.3928, LR: 0.575326\nEpoch [105/800], Batch [20/98], Loss: 5.3706, LR: 0.575326\nEpoch [105/800], Batch [30/98], Loss: 5.3779, LR: 0.575326\nEpoch [105/800], Batch [40/98], Loss: 5.3818, LR: 0.575326\nEpoch [105/800], Batch [50/98], Loss: 5.3605, LR: 0.575326\nEpoch [105/800], Batch [60/98], Loss: 5.3850, LR: 0.575326\nEpoch [105/800], Batch [70/98], Loss: 5.3829, LR: 0.575326\nEpoch [105/800], Batch [80/98], Loss: 5.4040, LR: 0.575326\nEpoch [105/800], Batch [90/98], Loss: 5.3851, LR: 0.575326\nEpoch [105/800] Average Loss: 5.3807\n\nEpoch [106/800], Batch [0/98], Loss: 5.3772, LR: 0.574856\nEpoch [106/800], Batch [10/98], Loss: 5.3938, LR: 0.574856\nEpoch [106/800], Batch [20/98], Loss: 5.3758, LR: 0.574856\nEpoch [106/800], Batch [30/98], Loss: 5.3849, LR: 0.574856\nEpoch [106/800], Batch [40/98], Loss: 5.4040, LR: 0.574856\nEpoch [106/800], Batch [50/98], Loss: 5.3678, LR: 0.574856\nEpoch [106/800], Batch [60/98], Loss: 5.3996, LR: 0.574856\nEpoch [106/800], Batch [70/98], Loss: 5.3887, LR: 0.574856\nEpoch [106/800], Batch [80/98], Loss: 5.3984, LR: 0.574856\nEpoch [106/800], Batch [90/98], Loss: 5.3863, LR: 0.574856\nEpoch [106/800] Average Loss: 5.3814\n\nEpoch [107/800], Batch [0/98], Loss: 5.4053, LR: 0.574382\nEpoch [107/800], Batch [10/98], Loss: 5.3617, LR: 0.574382\nEpoch [107/800], Batch [20/98], Loss: 5.3842, LR: 0.574382\nEpoch [107/800], Batch [30/98], Loss: 5.4022, LR: 0.574382\nEpoch [107/800], Batch [40/98], Loss: 5.3779, LR: 0.574382\nEpoch [107/800], Batch [50/98], Loss: 5.3570, LR: 0.574382\nEpoch [107/800], Batch [60/98], Loss: 5.3904, LR: 0.574382\nEpoch [107/800], Batch [70/98], Loss: 5.3947, LR: 0.574382\nEpoch [107/800], Batch [80/98], Loss: 5.3616, LR: 0.574382\nEpoch [107/800], Batch [90/98], Loss: 5.3996, LR: 0.574382\nEpoch [107/800] Average Loss: 5.3776\n\nEpoch [108/800], Batch [0/98], Loss: 5.3906, LR: 0.573904\nEpoch [108/800], Batch [10/98], Loss: 5.3906, LR: 0.573904\nEpoch [108/800], Batch [20/98], Loss: 5.3884, LR: 0.573904\nEpoch [108/800], Batch [30/98], Loss: 5.3489, LR: 0.573904\nEpoch [108/800], Batch [40/98], Loss: 5.4060, LR: 0.573904\nEpoch [108/800], Batch [50/98], Loss: 5.3981, LR: 0.573904\nEpoch [108/800], Batch [60/98], Loss: 5.3720, LR: 0.573904\nEpoch [108/800], Batch [70/98], Loss: 5.3978, LR: 0.573904\nEpoch [108/800], Batch [80/98], Loss: 5.4044, LR: 0.573904\nEpoch [108/800], Batch [90/98], Loss: 5.3942, LR: 0.573904\nEpoch [108/800] Average Loss: 5.3785\n\nEpoch [109/800], Batch [0/98], Loss: 5.3605, LR: 0.573421\nEpoch [109/800], Batch [10/98], Loss: 5.3823, LR: 0.573421\nEpoch [109/800], Batch [20/98], Loss: 5.3713, LR: 0.573421\nEpoch [109/800], Batch [30/98], Loss: 5.3971, LR: 0.573421\nEpoch [109/800], Batch [40/98], Loss: 5.3828, LR: 0.573421\nEpoch [109/800], Batch [50/98], Loss: 5.3693, LR: 0.573421\nEpoch [109/800], Batch [60/98], Loss: 5.3686, LR: 0.573421\nEpoch [109/800], Batch [70/98], Loss: 5.3946, LR: 0.573421\nEpoch [109/800], Batch [80/98], Loss: 5.3611, LR: 0.573421\nEpoch [109/800], Batch [90/98], Loss: 5.4089, LR: 0.573421\nEpoch [109/800] Average Loss: 5.3766\n\nEpoch [110/800], Batch [0/98], Loss: 5.3792, LR: 0.572934\nEpoch [110/800], Batch [10/98], Loss: 5.3859, LR: 0.572934\nEpoch [110/800], Batch [20/98], Loss: 5.3793, LR: 0.572934\nEpoch [110/800], Batch [30/98], Loss: 5.3878, LR: 0.572934\nEpoch [110/800], Batch [40/98], Loss: 5.4028, LR: 0.572934\nEpoch [110/800], Batch [50/98], Loss: 5.3886, LR: 0.572934\nEpoch [110/800], Batch [60/98], Loss: 5.3867, LR: 0.572934\nEpoch [110/800], Batch [70/98], Loss: 5.3649, LR: 0.572934\nEpoch [110/800], Batch [80/98], Loss: 5.3742, LR: 0.572934\nEpoch [110/800], Batch [90/98], Loss: 5.3929, LR: 0.572934\nEpoch [110/800] Average Loss: 5.3795\n\nEpoch [111/800], Batch [0/98], Loss: 5.3901, LR: 0.572443\nEpoch [111/800], Batch [10/98], Loss: 5.3725, LR: 0.572443\nEpoch [111/800], Batch [20/98], Loss: 5.3695, LR: 0.572443\nEpoch [111/800], Batch [30/98], Loss: 5.3918, LR: 0.572443\nEpoch [111/800], Batch [40/98], Loss: 5.3782, LR: 0.572443\nEpoch [111/800], Batch [50/98], Loss: 5.3737, LR: 0.572443\nEpoch [111/800], Batch [60/98], Loss: 5.3650, LR: 0.572443\nEpoch [111/800], Batch [70/98], Loss: 5.3884, LR: 0.572443\nEpoch [111/800], Batch [80/98], Loss: 5.3820, LR: 0.572443\nEpoch [111/800], Batch [90/98], Loss: 5.3854, LR: 0.572443\nEpoch [111/800] Average Loss: 5.3768\n\nEpoch [112/800], Batch [0/98], Loss: 5.3882, LR: 0.571948\nEpoch [112/800], Batch [10/98], Loss: 5.3550, LR: 0.571948\nEpoch [112/800], Batch [20/98], Loss: 5.3873, LR: 0.571948\nEpoch [112/800], Batch [30/98], Loss: 5.3824, LR: 0.571948\nEpoch [112/800], Batch [40/98], Loss: 5.3949, LR: 0.571948\nEpoch [112/800], Batch [50/98], Loss: 5.4140, LR: 0.571948\nEpoch [112/800], Batch [60/98], Loss: 5.3610, LR: 0.571948\nEpoch [112/800], Batch [70/98], Loss: 5.4149, LR: 0.571948\nEpoch [112/800], Batch [80/98], Loss: 5.3894, LR: 0.571948\nEpoch [112/800], Batch [90/98], Loss: 5.3950, LR: 0.571948\nEpoch [112/800] Average Loss: 5.3795\n\nEpoch [113/800], Batch [0/98], Loss: 5.4056, LR: 0.571448\nEpoch [113/800], Batch [10/98], Loss: 5.3660, LR: 0.571448\nEpoch [113/800], Batch [20/98], Loss: 5.3781, LR: 0.571448\nEpoch [113/800], Batch [30/98], Loss: 5.3868, LR: 0.571448\nEpoch [113/800], Batch [40/98], Loss: 5.3771, LR: 0.571448\nEpoch [113/800], Batch [50/98], Loss: 5.3792, LR: 0.571448\nEpoch [113/800], Batch [60/98], Loss: 5.3473, LR: 0.571448\nEpoch [113/800], Batch [70/98], Loss: 5.3696, LR: 0.571448\nEpoch [113/800], Batch [80/98], Loss: 5.3853, LR: 0.571448\nEpoch [113/800], Batch [90/98], Loss: 5.3578, LR: 0.571448\nEpoch [113/800] Average Loss: 5.3768\n\nEpoch [114/800], Batch [0/98], Loss: 5.3429, LR: 0.570944\nEpoch [114/800], Batch [10/98], Loss: 5.3851, LR: 0.570944\nEpoch [114/800], Batch [20/98], Loss: 5.4150, LR: 0.570944\nEpoch [114/800], Batch [30/98], Loss: 5.3806, LR: 0.570944\nEpoch [114/800], Batch [40/98], Loss: 5.3513, LR: 0.570944\nEpoch [114/800], Batch [50/98], Loss: 5.3858, LR: 0.570944\nEpoch [114/800], Batch [60/98], Loss: 5.3546, LR: 0.570944\nEpoch [114/800], Batch [70/98], Loss: 5.3556, LR: 0.570944\nEpoch [114/800], Batch [80/98], Loss: 5.4031, LR: 0.570944\nEpoch [114/800], Batch [90/98], Loss: 5.3885, LR: 0.570944\nEpoch [114/800] Average Loss: 5.3723\n\nEpoch [115/800], Batch [0/98], Loss: 5.3688, LR: 0.570437\nEpoch [115/800], Batch [10/98], Loss: 5.3852, LR: 0.570437\nEpoch [115/800], Batch [20/98], Loss: 5.3763, LR: 0.570437\nEpoch [115/800], Batch [30/98], Loss: 5.3957, LR: 0.570437\nEpoch [115/800], Batch [40/98], Loss: 5.3705, LR: 0.570437\nEpoch [115/800], Batch [50/98], Loss: 5.3752, LR: 0.570437\nEpoch [115/800], Batch [60/98], Loss: 5.3807, LR: 0.570437\nEpoch [115/800], Batch [70/98], Loss: 5.3493, LR: 0.570437\nEpoch [115/800], Batch [80/98], Loss: 5.3817, LR: 0.570437\nEpoch [115/800], Batch [90/98], Loss: 5.3844, LR: 0.570437\nEpoch [115/800] Average Loss: 5.3752\n\nEpoch [116/800], Batch [0/98], Loss: 5.3749, LR: 0.569924\nEpoch [116/800], Batch [10/98], Loss: 5.3659, LR: 0.569924\nEpoch [116/800], Batch [20/98], Loss: 5.3944, LR: 0.569924\nEpoch [116/800], Batch [30/98], Loss: 5.3993, LR: 0.569924\nEpoch [116/800], Batch [40/98], Loss: 5.3945, LR: 0.569924\nEpoch [116/800], Batch [50/98], Loss: 5.3659, LR: 0.569924\nEpoch [116/800], Batch [60/98], Loss: 5.3686, LR: 0.569924\nEpoch [116/800], Batch [70/98], Loss: 5.4045, LR: 0.569924\nEpoch [116/800], Batch [80/98], Loss: 5.3669, LR: 0.569924\nEpoch [116/800], Batch [90/98], Loss: 5.4093, LR: 0.569924\nEpoch [116/800] Average Loss: 5.3761\n\nEpoch [117/800], Batch [0/98], Loss: 5.3748, LR: 0.569408\nEpoch [117/800], Batch [10/98], Loss: 5.3533, LR: 0.569408\nEpoch [117/800], Batch [20/98], Loss: 5.4010, LR: 0.569408\nEpoch [117/800], Batch [30/98], Loss: 5.3773, LR: 0.569408\nEpoch [117/800], Batch [40/98], Loss: 5.3608, LR: 0.569408\nEpoch [117/800], Batch [50/98], Loss: 5.3862, LR: 0.569408\nEpoch [117/800], Batch [60/98], Loss: 5.3584, LR: 0.569408\nEpoch [117/800], Batch [70/98], Loss: 5.3659, LR: 0.569408\nEpoch [117/800], Batch [80/98], Loss: 5.3663, LR: 0.569408\nEpoch [117/800], Batch [90/98], Loss: 5.3966, LR: 0.569408\nEpoch [117/800] Average Loss: 5.3761\n\nEpoch [118/800], Batch [0/98], Loss: 5.3543, LR: 0.568888\nEpoch [118/800], Batch [10/98], Loss: 5.3674, LR: 0.568888\nEpoch [118/800], Batch [20/98], Loss: 5.3869, LR: 0.568888\nEpoch [118/800], Batch [30/98], Loss: 5.3674, LR: 0.568888\nEpoch [118/800], Batch [40/98], Loss: 5.3953, LR: 0.568888\nEpoch [118/800], Batch [50/98], Loss: 5.3659, LR: 0.568888\nEpoch [118/800], Batch [60/98], Loss: 5.3465, LR: 0.568888\nEpoch [118/800], Batch [70/98], Loss: 5.3784, LR: 0.568888\nEpoch [118/800], Batch [80/98], Loss: 5.3940, LR: 0.568888\nEpoch [118/800], Batch [90/98], Loss: 5.3645, LR: 0.568888\nEpoch [118/800] Average Loss: 5.3735\n\nEpoch [119/800], Batch [0/98], Loss: 5.3811, LR: 0.568363\nEpoch [119/800], Batch [10/98], Loss: 5.3703, LR: 0.568363\nEpoch [119/800], Batch [20/98], Loss: 5.3985, LR: 0.568363\nEpoch [119/800], Batch [30/98], Loss: 5.3591, LR: 0.568363\nEpoch [119/800], Batch [40/98], Loss: 5.3766, LR: 0.568363\nEpoch [119/800], Batch [50/98], Loss: 5.3743, LR: 0.568363\nEpoch [119/800], Batch [60/98], Loss: 5.3741, LR: 0.568363\nEpoch [119/800], Batch [70/98], Loss: 5.3653, LR: 0.568363\nEpoch [119/800], Batch [80/98], Loss: 5.3641, LR: 0.568363\nEpoch [119/800], Batch [90/98], Loss: 5.3670, LR: 0.568363\nEpoch [119/800] Average Loss: 5.3725\n\nEpoch [120/800], Batch [0/98], Loss: 5.4009, LR: 0.567835\nEpoch [120/800], Batch [10/98], Loss: 5.3953, LR: 0.567835\nEpoch [120/800], Batch [20/98], Loss: 5.3784, LR: 0.567835\nEpoch [120/800], Batch [30/98], Loss: 5.3835, LR: 0.567835\nEpoch [120/800], Batch [40/98], Loss: 5.3498, LR: 0.567835\nEpoch [120/800], Batch [50/98], Loss: 5.3711, LR: 0.567835\nEpoch [120/800], Batch [60/98], Loss: 5.3954, LR: 0.567835\nEpoch [120/800], Batch [70/98], Loss: 5.3520, LR: 0.567835\nEpoch [120/800], Batch [80/98], Loss: 5.3656, LR: 0.567835\nEpoch [120/800], Batch [90/98], Loss: 5.3788, LR: 0.567835\nEpoch [120/800] Average Loss: 5.3722\n\nEpoch [121/800], Batch [0/98], Loss: 5.3785, LR: 0.567302\nEpoch [121/800], Batch [10/98], Loss: 5.3642, LR: 0.567302\nEpoch [121/800], Batch [20/98], Loss: 5.3821, LR: 0.567302\nEpoch [121/800], Batch [30/98], Loss: 5.3996, LR: 0.567302\nEpoch [121/800], Batch [40/98], Loss: 5.3555, LR: 0.567302\nEpoch [121/800], Batch [50/98], Loss: 5.3630, LR: 0.567302\nEpoch [121/800], Batch [60/98], Loss: 5.3689, LR: 0.567302\nEpoch [121/800], Batch [70/98], Loss: 5.3648, LR: 0.567302\nEpoch [121/800], Batch [80/98], Loss: 5.3917, LR: 0.567302\nEpoch [121/800], Batch [90/98], Loss: 5.3394, LR: 0.567302\nEpoch [121/800] Average Loss: 5.3692\n\nEpoch [122/800], Batch [0/98], Loss: 5.3816, LR: 0.566765\nEpoch [122/800], Batch [10/98], Loss: 5.3592, LR: 0.566765\nEpoch [122/800], Batch [20/98], Loss: 5.3762, LR: 0.566765\nEpoch [122/800], Batch [30/98], Loss: 5.3966, LR: 0.566765\nEpoch [122/800], Batch [40/98], Loss: 5.3611, LR: 0.566765\nEpoch [122/800], Batch [50/98], Loss: 5.3746, LR: 0.566765\nEpoch [122/800], Batch [60/98], Loss: 5.4077, LR: 0.566765\nEpoch [122/800], Batch [70/98], Loss: 5.3910, LR: 0.566765\nEpoch [122/800], Batch [80/98], Loss: 5.3877, LR: 0.566765\nEpoch [122/800], Batch [90/98], Loss: 5.3835, LR: 0.566765\nEpoch [122/800] Average Loss: 5.3722\n\nEpoch [123/800], Batch [0/98], Loss: 5.3858, LR: 0.566224\nEpoch [123/800], Batch [10/98], Loss: 5.3712, LR: 0.566224\nEpoch [123/800], Batch [20/98], Loss: 5.3809, LR: 0.566224\nEpoch [123/800], Batch [30/98], Loss: 5.3572, LR: 0.566224\nEpoch [123/800], Batch [40/98], Loss: 5.4022, LR: 0.566224\nEpoch [123/800], Batch [50/98], Loss: 5.3804, LR: 0.566224\nEpoch [123/800], Batch [60/98], Loss: 5.3542, LR: 0.566224\nEpoch [123/800], Batch [70/98], Loss: 5.3660, LR: 0.566224\nEpoch [123/800], Batch [80/98], Loss: 5.3804, LR: 0.566224\nEpoch [123/800], Batch [90/98], Loss: 5.3812, LR: 0.566224\nEpoch [123/800] Average Loss: 5.3743\n\nEpoch [124/800], Batch [0/98], Loss: 5.3979, LR: 0.565679\nEpoch [124/800], Batch [10/98], Loss: 5.3755, LR: 0.565679\nEpoch [124/800], Batch [20/98], Loss: 5.3553, LR: 0.565679\nEpoch [124/800], Batch [30/98], Loss: 5.3803, LR: 0.565679\nEpoch [124/800], Batch [40/98], Loss: 5.3871, LR: 0.565679\nEpoch [124/800], Batch [50/98], Loss: 5.3798, LR: 0.565679\nEpoch [124/800], Batch [60/98], Loss: 5.3935, LR: 0.565679\nEpoch [124/800], Batch [70/98], Loss: 5.3620, LR: 0.565679\nEpoch [124/800], Batch [80/98], Loss: 5.3886, LR: 0.565679\nEpoch [124/800], Batch [90/98], Loss: 5.3665, LR: 0.565679\nEpoch [124/800] Average Loss: 5.3712\n\nEpoch [125/800], Batch [0/98], Loss: 5.3562, LR: 0.565130\nEpoch [125/800], Batch [10/98], Loss: 5.3816, LR: 0.565130\nEpoch [125/800], Batch [20/98], Loss: 5.3551, LR: 0.565130\nEpoch [125/800], Batch [30/98], Loss: 5.3697, LR: 0.565130\nEpoch [125/800], Batch [40/98], Loss: 5.3794, LR: 0.565130\nEpoch [125/800], Batch [50/98], Loss: 5.3671, LR: 0.565130\nEpoch [125/800], Batch [60/98], Loss: 5.3786, LR: 0.565130\nEpoch [125/800], Batch [70/98], Loss: 5.4063, LR: 0.565130\nEpoch [125/800], Batch [80/98], Loss: 5.3627, LR: 0.565130\nEpoch [125/800], Batch [90/98], Loss: 5.3773, LR: 0.565130\nEpoch [125/800] Average Loss: 5.3675\n\nEpoch [126/800], Batch [0/98], Loss: 5.3884, LR: 0.564576\nEpoch [126/800], Batch [10/98], Loss: 5.3607, LR: 0.564576\nEpoch [126/800], Batch [20/98], Loss: 5.3946, LR: 0.564576\nEpoch [126/800], Batch [30/98], Loss: 5.3638, LR: 0.564576\nEpoch [126/800], Batch [40/98], Loss: 5.3742, LR: 0.564576\nEpoch [126/800], Batch [50/98], Loss: 5.3863, LR: 0.564576\nEpoch [126/800], Batch [60/98], Loss: 5.3557, LR: 0.564576\nEpoch [126/800], Batch [70/98], Loss: 5.3745, LR: 0.564576\nEpoch [126/800], Batch [80/98], Loss: 5.3888, LR: 0.564576\nEpoch [126/800], Batch [90/98], Loss: 5.3593, LR: 0.564576\nEpoch [126/800] Average Loss: 5.3677\n\nEpoch [127/800], Batch [0/98], Loss: 5.3567, LR: 0.564019\nEpoch [127/800], Batch [10/98], Loss: 5.3641, LR: 0.564019\nEpoch [127/800], Batch [20/98], Loss: 5.3680, LR: 0.564019\nEpoch [127/800], Batch [30/98], Loss: 5.3474, LR: 0.564019\nEpoch [127/800], Batch [40/98], Loss: 5.3631, LR: 0.564019\nEpoch [127/800], Batch [50/98], Loss: 5.3546, LR: 0.564019\nEpoch [127/800], Batch [60/98], Loss: 5.3916, LR: 0.564019\nEpoch [127/800], Batch [70/98], Loss: 5.3814, LR: 0.564019\nEpoch [127/800], Batch [80/98], Loss: 5.3512, LR: 0.564019\nEpoch [127/800], Batch [90/98], Loss: 5.3906, LR: 0.564019\nEpoch [127/800] Average Loss: 5.3668\n\nEpoch [128/800], Batch [0/98], Loss: 5.3715, LR: 0.563458\nEpoch [128/800], Batch [10/98], Loss: 5.3698, LR: 0.563458\nEpoch [128/800], Batch [20/98], Loss: 5.3579, LR: 0.563458\nEpoch [128/800], Batch [30/98], Loss: 5.3528, LR: 0.563458\nEpoch [128/800], Batch [40/98], Loss: 5.3908, LR: 0.563458\nEpoch [128/800], Batch [50/98], Loss: 5.3299, LR: 0.563458\nEpoch [128/800], Batch [60/98], Loss: 5.3794, LR: 0.563458\nEpoch [128/800], Batch [70/98], Loss: 5.4033, LR: 0.563458\nEpoch [128/800], Batch [80/98], Loss: 5.3684, LR: 0.563458\nEpoch [128/800], Batch [90/98], Loss: 5.3776, LR: 0.563458\nEpoch [128/800] Average Loss: 5.3663\n\nEpoch [129/800], Batch [0/98], Loss: 5.3841, LR: 0.562892\nEpoch [129/800], Batch [10/98], Loss: 5.3769, LR: 0.562892\nEpoch [129/800], Batch [20/98], Loss: 5.3543, LR: 0.562892\nEpoch [129/800], Batch [30/98], Loss: 5.3642, LR: 0.562892\nEpoch [129/800], Batch [40/98], Loss: 5.3717, LR: 0.562892\nEpoch [129/800], Batch [50/98], Loss: 5.3623, LR: 0.562892\nEpoch [129/800], Batch [60/98], Loss: 5.3515, LR: 0.562892\nEpoch [129/800], Batch [70/98], Loss: 5.3552, LR: 0.562892\nEpoch [129/800], Batch [80/98], Loss: 5.3819, LR: 0.562892\nEpoch [129/800], Batch [90/98], Loss: 5.3644, LR: 0.562892\nEpoch [129/800] Average Loss: 5.3648\n\nEpoch [130/800], Batch [0/98], Loss: 5.3868, LR: 0.562322\nEpoch [130/800], Batch [10/98], Loss: 5.3654, LR: 0.562322\nEpoch [130/800], Batch [20/98], Loss: 5.3823, LR: 0.562322\nEpoch [130/800], Batch [30/98], Loss: 5.3803, LR: 0.562322\nEpoch [130/800], Batch [40/98], Loss: 5.3873, LR: 0.562322\nEpoch [130/800], Batch [50/98], Loss: 5.3593, LR: 0.562322\nEpoch [130/800], Batch [60/98], Loss: 5.3692, LR: 0.562322\nEpoch [130/800], Batch [70/98], Loss: 5.3670, LR: 0.562322\nEpoch [130/800], Batch [80/98], Loss: 5.3514, LR: 0.562322\nEpoch [130/800], Batch [90/98], Loss: 5.3755, LR: 0.562322\nEpoch [130/800] Average Loss: 5.3643\n\nEpoch [131/800], Batch [0/98], Loss: 5.3762, LR: 0.561749\nEpoch [131/800], Batch [10/98], Loss: 5.3765, LR: 0.561749\nEpoch [131/800], Batch [20/98], Loss: 5.3643, LR: 0.561749\nEpoch [131/800], Batch [30/98], Loss: 5.3780, LR: 0.561749\nEpoch [131/800], Batch [40/98], Loss: 5.3676, LR: 0.561749\nEpoch [131/800], Batch [50/98], Loss: 5.3726, LR: 0.561749\nEpoch [131/800], Batch [60/98], Loss: 5.3417, LR: 0.561749\nEpoch [131/800], Batch [70/98], Loss: 5.3878, LR: 0.561749\nEpoch [131/800], Batch [80/98], Loss: 5.3905, LR: 0.561749\nEpoch [131/800], Batch [90/98], Loss: 5.3503, LR: 0.561749\nEpoch [131/800] Average Loss: 5.3645\n\nEpoch [132/800], Batch [0/98], Loss: 5.3958, LR: 0.561171\nEpoch [132/800], Batch [10/98], Loss: 5.3346, LR: 0.561171\nEpoch [132/800], Batch [20/98], Loss: 5.3955, LR: 0.561171\nEpoch [132/800], Batch [30/98], Loss: 5.3640, LR: 0.561171\nEpoch [132/800], Batch [40/98], Loss: 5.3596, LR: 0.561171\nEpoch [132/800], Batch [50/98], Loss: 5.3635, LR: 0.561171\nEpoch [132/800], Batch [60/98], Loss: 5.3813, LR: 0.561171\nEpoch [132/800], Batch [70/98], Loss: 5.3750, LR: 0.561171\nEpoch [132/800], Batch [80/98], Loss: 5.3594, LR: 0.561171\nEpoch [132/800], Batch [90/98], Loss: 5.3864, LR: 0.561171\nEpoch [132/800] Average Loss: 5.3643\n\nEpoch [133/800], Batch [0/98], Loss: 5.3682, LR: 0.560589\nEpoch [133/800], Batch [10/98], Loss: 5.3997, LR: 0.560589\nEpoch [133/800], Batch [20/98], Loss: 5.3893, LR: 0.560589\nEpoch [133/800], Batch [30/98], Loss: 5.3863, LR: 0.560589\nEpoch [133/800], Batch [40/98], Loss: 5.3769, LR: 0.560589\nEpoch [133/800], Batch [50/98], Loss: 5.3730, LR: 0.560589\nEpoch [133/800], Batch [60/98], Loss: 5.3774, LR: 0.560589\nEpoch [133/800], Batch [70/98], Loss: 5.3396, LR: 0.560589\nEpoch [133/800], Batch [80/98], Loss: 5.3613, LR: 0.560589\nEpoch [133/800], Batch [90/98], Loss: 5.3707, LR: 0.560589\nEpoch [133/800] Average Loss: 5.3628\n\nEpoch [134/800], Batch [0/98], Loss: 5.3639, LR: 0.560004\nEpoch [134/800], Batch [10/98], Loss: 5.3571, LR: 0.560004\nEpoch [134/800], Batch [20/98], Loss: 5.3747, LR: 0.560004\nEpoch [134/800], Batch [30/98], Loss: 5.3590, LR: 0.560004\nEpoch [134/800], Batch [40/98], Loss: 5.3809, LR: 0.560004\nEpoch [134/800], Batch [50/98], Loss: 5.3306, LR: 0.560004\nEpoch [134/800], Batch [60/98], Loss: 5.3647, LR: 0.560004\nEpoch [134/800], Batch [70/98], Loss: 5.3582, LR: 0.560004\nEpoch [134/800], Batch [80/98], Loss: 5.3677, LR: 0.560004\nEpoch [134/800], Batch [90/98], Loss: 5.3645, LR: 0.560004\nEpoch [134/800] Average Loss: 5.3647\n\nEpoch [135/800], Batch [0/98], Loss: 5.3609, LR: 0.559414\nEpoch [135/800], Batch [10/98], Loss: 5.3710, LR: 0.559414\nEpoch [135/800], Batch [20/98], Loss: 5.3628, LR: 0.559414\nEpoch [135/800], Batch [30/98], Loss: 5.3714, LR: 0.559414\nEpoch [135/800], Batch [40/98], Loss: 5.3620, LR: 0.559414\nEpoch [135/800], Batch [50/98], Loss: 5.3955, LR: 0.559414\nEpoch [135/800], Batch [60/98], Loss: 5.3498, LR: 0.559414\nEpoch [135/800], Batch [70/98], Loss: 5.3907, LR: 0.559414\nEpoch [135/800], Batch [80/98], Loss: 5.3684, LR: 0.559414\nEpoch [135/800], Batch [90/98], Loss: 5.3477, LR: 0.559414\nEpoch [135/800] Average Loss: 5.3630\n\nEpoch [136/800], Batch [0/98], Loss: 5.3764, LR: 0.558820\nEpoch [136/800], Batch [10/98], Loss: 5.3760, LR: 0.558820\nEpoch [136/800], Batch [20/98], Loss: 5.3736, LR: 0.558820\nEpoch [136/800], Batch [30/98], Loss: 5.3668, LR: 0.558820\nEpoch [136/800], Batch [40/98], Loss: 5.3313, LR: 0.558820\nEpoch [136/800], Batch [50/98], Loss: 5.3697, LR: 0.558820\nEpoch [136/800], Batch [60/98], Loss: 5.3536, LR: 0.558820\nEpoch [136/800], Batch [70/98], Loss: 5.3645, LR: 0.558820\nEpoch [136/800], Batch [80/98], Loss: 5.3735, LR: 0.558820\nEpoch [136/800], Batch [90/98], Loss: 5.3750, LR: 0.558820\nEpoch [136/800] Average Loss: 5.3642\n\nEpoch [137/800], Batch [0/98], Loss: 5.3843, LR: 0.558223\nEpoch [137/800], Batch [10/98], Loss: 5.3744, LR: 0.558223\nEpoch [137/800], Batch [20/98], Loss: 5.4019, LR: 0.558223\nEpoch [137/800], Batch [30/98], Loss: 5.3912, LR: 0.558223\nEpoch [137/800], Batch [40/98], Loss: 5.3725, LR: 0.558223\nEpoch [137/800], Batch [50/98], Loss: 5.3493, LR: 0.558223\nEpoch [137/800], Batch [60/98], Loss: 5.3590, LR: 0.558223\nEpoch [137/800], Batch [70/98], Loss: 5.3418, LR: 0.558223\nEpoch [137/800], Batch [80/98], Loss: 5.3460, LR: 0.558223\nEpoch [137/800], Batch [90/98], Loss: 5.3608, LR: 0.558223\nEpoch [137/800] Average Loss: 5.3641\n\nEpoch [138/800], Batch [0/98], Loss: 5.3789, LR: 0.557621\nEpoch [138/800], Batch [10/98], Loss: 5.3781, LR: 0.557621\nEpoch [138/800], Batch [20/98], Loss: 5.3478, LR: 0.557621\nEpoch [138/800], Batch [30/98], Loss: 5.3662, LR: 0.557621\nEpoch [138/800], Batch [40/98], Loss: 5.3631, LR: 0.557621\nEpoch [138/800], Batch [50/98], Loss: 5.3637, LR: 0.557621\nEpoch [138/800], Batch [60/98], Loss: 5.3357, LR: 0.557621\nEpoch [138/800], Batch [70/98], Loss: 5.3649, LR: 0.557621\nEpoch [138/800], Batch [80/98], Loss: 5.3775, LR: 0.557621\nEpoch [138/800], Batch [90/98], Loss: 5.3468, LR: 0.557621\nEpoch [138/800] Average Loss: 5.3618\n\nEpoch [139/800], Batch [0/98], Loss: 5.3319, LR: 0.557015\nEpoch [139/800], Batch [10/98], Loss: 5.3607, LR: 0.557015\nEpoch [139/800], Batch [20/98], Loss: 5.3616, LR: 0.557015\nEpoch [139/800], Batch [30/98], Loss: 5.3614, LR: 0.557015\nEpoch [139/800], Batch [40/98], Loss: 5.3770, LR: 0.557015\nEpoch [139/800], Batch [50/98], Loss: 5.3628, LR: 0.557015\nEpoch [139/800], Batch [60/98], Loss: 5.3714, LR: 0.557015\nEpoch [139/800], Batch [70/98], Loss: 5.3675, LR: 0.557015\nEpoch [139/800], Batch [80/98], Loss: 5.3548, LR: 0.557015\nEpoch [139/800], Batch [90/98], Loss: 5.3728, LR: 0.557015\nEpoch [139/800] Average Loss: 5.3619\n\nEpoch [140/800], Batch [0/98], Loss: 5.3631, LR: 0.556406\nEpoch [140/800], Batch [10/98], Loss: 5.3728, LR: 0.556406\nEpoch [140/800], Batch [20/98], Loss: 5.3456, LR: 0.556406\nEpoch [140/800], Batch [30/98], Loss: 5.3864, LR: 0.556406\nEpoch [140/800], Batch [40/98], Loss: 5.3496, LR: 0.556406\nEpoch [140/800], Batch [50/98], Loss: 5.3793, LR: 0.556406\nEpoch [140/800], Batch [60/98], Loss: 5.3636, LR: 0.556406\nEpoch [140/800], Batch [70/98], Loss: 5.3570, LR: 0.556406\nEpoch [140/800], Batch [80/98], Loss: 5.3858, LR: 0.556406\nEpoch [140/800], Batch [90/98], Loss: 5.3644, LR: 0.556406\nEpoch [140/800] Average Loss: 5.3622\n\nEpoch [141/800], Batch [0/98], Loss: 5.3417, LR: 0.555792\nEpoch [141/800], Batch [10/98], Loss: 5.3675, LR: 0.555792\nEpoch [141/800], Batch [20/98], Loss: 5.3552, LR: 0.555792\nEpoch [141/800], Batch [30/98], Loss: 5.3514, LR: 0.555792\nEpoch [141/800], Batch [40/98], Loss: 5.3630, LR: 0.555792\nEpoch [141/800], Batch [50/98], Loss: 5.3864, LR: 0.555792\nEpoch [141/800], Batch [60/98], Loss: 5.3704, LR: 0.555792\nEpoch [141/800], Batch [70/98], Loss: 5.3728, LR: 0.555792\nEpoch [141/800], Batch [80/98], Loss: 5.3552, LR: 0.555792\nEpoch [141/800], Batch [90/98], Loss: 5.3680, LR: 0.555792\nEpoch [141/800] Average Loss: 5.3618\n\nEpoch [142/800], Batch [0/98], Loss: 5.3766, LR: 0.555175\nEpoch [142/800], Batch [10/98], Loss: 5.3641, LR: 0.555175\nEpoch [142/800], Batch [20/98], Loss: 5.3714, LR: 0.555175\nEpoch [142/800], Batch [30/98], Loss: 5.3778, LR: 0.555175\nEpoch [142/800], Batch [40/98], Loss: 5.3625, LR: 0.555175\nEpoch [142/800], Batch [50/98], Loss: 5.3618, LR: 0.555175\nEpoch [142/800], Batch [60/98], Loss: 5.3623, LR: 0.555175\nEpoch [142/800], Batch [70/98], Loss: 5.3647, LR: 0.555175\nEpoch [142/800], Batch [80/98], Loss: 5.3827, LR: 0.555175\nEpoch [142/800], Batch [90/98], Loss: 5.3658, LR: 0.555175\nEpoch [142/800] Average Loss: 5.3565\n\nEpoch [143/800], Batch [0/98], Loss: 5.3462, LR: 0.554553\nEpoch [143/800], Batch [10/98], Loss: 5.3633, LR: 0.554553\nEpoch [143/800], Batch [20/98], Loss: 5.3856, LR: 0.554553\nEpoch [143/800], Batch [30/98], Loss: 5.3725, LR: 0.554553\nEpoch [143/800], Batch [40/98], Loss: 5.3774, LR: 0.554553\nEpoch [143/800], Batch [50/98], Loss: 5.3351, LR: 0.554553\nEpoch [143/800], Batch [60/98], Loss: 5.3538, LR: 0.554553\nEpoch [143/800], Batch [70/98], Loss: 5.3750, LR: 0.554553\nEpoch [143/800], Batch [80/98], Loss: 5.3544, LR: 0.554553\nEpoch [143/800], Batch [90/98], Loss: 5.3549, LR: 0.554553\nEpoch [143/800] Average Loss: 5.3593\n\nEpoch [144/800], Batch [0/98], Loss: 5.3830, LR: 0.553928\nEpoch [144/800], Batch [10/98], Loss: 5.3371, LR: 0.553928\nEpoch [144/800], Batch [20/98], Loss: 5.3448, LR: 0.553928\nEpoch [144/800], Batch [30/98], Loss: 5.3378, LR: 0.553928\nEpoch [144/800], Batch [40/98], Loss: 5.3601, LR: 0.553928\nEpoch [144/800], Batch [50/98], Loss: 5.3815, LR: 0.553928\nEpoch [144/800], Batch [60/98], Loss: 5.3655, LR: 0.553928\nEpoch [144/800], Batch [70/98], Loss: 5.3378, LR: 0.553928\nEpoch [144/800], Batch [80/98], Loss: 5.3830, LR: 0.553928\nEpoch [144/800], Batch [90/98], Loss: 5.3505, LR: 0.553928\nEpoch [144/800] Average Loss: 5.3574\n\nEpoch [145/800], Batch [0/98], Loss: 5.3438, LR: 0.553298\nEpoch [145/800], Batch [10/98], Loss: 5.3442, LR: 0.553298\nEpoch [145/800], Batch [20/98], Loss: 5.3604, LR: 0.553298\nEpoch [145/800], Batch [30/98], Loss: 5.3633, LR: 0.553298\nEpoch [145/800], Batch [40/98], Loss: 5.3698, LR: 0.553298\nEpoch [145/800], Batch [50/98], Loss: 5.3557, LR: 0.553298\nEpoch [145/800], Batch [60/98], Loss: 5.3837, LR: 0.553298\nEpoch [145/800], Batch [70/98], Loss: 5.3611, LR: 0.553298\nEpoch [145/800], Batch [80/98], Loss: 5.3646, LR: 0.553298\nEpoch [145/800], Batch [90/98], Loss: 5.3909, LR: 0.553298\nEpoch [145/800] Average Loss: 5.3572\n\nEpoch [146/800], Batch [0/98], Loss: 5.3518, LR: 0.552665\nEpoch [146/800], Batch [10/98], Loss: 5.3774, LR: 0.552665\nEpoch [146/800], Batch [20/98], Loss: 5.3531, LR: 0.552665\nEpoch [146/800], Batch [30/98], Loss: 5.3808, LR: 0.552665\nEpoch [146/800], Batch [40/98], Loss: 5.3549, LR: 0.552665\nEpoch [146/800], Batch [50/98], Loss: 5.3574, LR: 0.552665\nEpoch [146/800], Batch [60/98], Loss: 5.3545, LR: 0.552665\nEpoch [146/800], Batch [70/98], Loss: 5.3325, LR: 0.552665\nEpoch [146/800], Batch [80/98], Loss: 5.3998, LR: 0.552665\nEpoch [146/800], Batch [90/98], Loss: 5.3439, LR: 0.552665\nEpoch [146/800] Average Loss: 5.3585\n\nEpoch [147/800], Batch [0/98], Loss: 5.3675, LR: 0.552028\nEpoch [147/800], Batch [10/98], Loss: 5.3548, LR: 0.552028\nEpoch [147/800], Batch [20/98], Loss: 5.3154, LR: 0.552028\nEpoch [147/800], Batch [30/98], Loss: 5.3817, LR: 0.552028\nEpoch [147/800], Batch [40/98], Loss: 5.3933, LR: 0.552028\nEpoch [147/800], Batch [50/98], Loss: 5.3379, LR: 0.552028\nEpoch [147/800], Batch [60/98], Loss: 5.3545, LR: 0.552028\nEpoch [147/800], Batch [70/98], Loss: 5.3729, LR: 0.552028\nEpoch [147/800], Batch [80/98], Loss: 5.3554, LR: 0.552028\nEpoch [147/800], Batch [90/98], Loss: 5.3830, LR: 0.552028\nEpoch [147/800] Average Loss: 5.3559\n\nEpoch [148/800], Batch [0/98], Loss: 5.3566, LR: 0.551387\nEpoch [148/800], Batch [10/98], Loss: 5.3767, LR: 0.551387\nEpoch [148/800], Batch [20/98], Loss: 5.3535, LR: 0.551387\nEpoch [148/800], Batch [30/98], Loss: 5.3592, LR: 0.551387\nEpoch [148/800], Batch [40/98], Loss: 5.3502, LR: 0.551387\nEpoch [148/800], Batch [50/98], Loss: 5.3435, LR: 0.551387\nEpoch [148/800], Batch [60/98], Loss: 5.3561, LR: 0.551387\nEpoch [148/800], Batch [70/98], Loss: 5.3640, LR: 0.551387\nEpoch [148/800], Batch [80/98], Loss: 5.3307, LR: 0.551387\nEpoch [148/800], Batch [90/98], Loss: 5.3483, LR: 0.551387\nEpoch [148/800] Average Loss: 5.3545\n\nEpoch [149/800], Batch [0/98], Loss: 5.3813, LR: 0.550742\nEpoch [149/800], Batch [10/98], Loss: 5.3597, LR: 0.550742\nEpoch [149/800], Batch [20/98], Loss: 5.3803, LR: 0.550742\nEpoch [149/800], Batch [30/98], Loss: 5.3802, LR: 0.550742\nEpoch [149/800], Batch [40/98], Loss: 5.3756, LR: 0.550742\nEpoch [149/800], Batch [50/98], Loss: 5.3825, LR: 0.550742\nEpoch [149/800], Batch [60/98], Loss: 5.3491, LR: 0.550742\nEpoch [149/800], Batch [70/98], Loss: 5.3793, LR: 0.550742\nEpoch [149/800], Batch [80/98], Loss: 5.3865, LR: 0.550742\nEpoch [149/800], Batch [90/98], Loss: 5.3714, LR: 0.550742\nEpoch [149/800] Average Loss: 5.3573\n\nEpoch [150/800], Batch [0/98], Loss: 5.3778, LR: 0.550093\nEpoch [150/800], Batch [10/98], Loss: 5.3812, LR: 0.550093\nEpoch [150/800], Batch [20/98], Loss: 5.3449, LR: 0.550093\nEpoch [150/800], Batch [30/98], Loss: 5.3643, LR: 0.550093\nEpoch [150/800], Batch [40/98], Loss: 5.3652, LR: 0.550093\nEpoch [150/800], Batch [50/98], Loss: 5.3614, LR: 0.550093\nEpoch [150/800], Batch [60/98], Loss: 5.3706, LR: 0.550093\nEpoch [150/800], Batch [70/98], Loss: 5.3675, LR: 0.550093\nEpoch [150/800], Batch [80/98], Loss: 5.3946, LR: 0.550093\nEpoch [150/800], Batch [90/98], Loss: 5.3444, LR: 0.550093\nEpoch [150/800] Average Loss: 5.3557\n\nEpoch [151/800], Batch [0/98], Loss: 5.3589, LR: 0.549441\nEpoch [151/800], Batch [10/98], Loss: 5.3456, LR: 0.549441\nEpoch [151/800], Batch [20/98], Loss: 5.3371, LR: 0.549441\nEpoch [151/800], Batch [30/98], Loss: 5.3840, LR: 0.549441\nEpoch [151/800], Batch [40/98], Loss: 5.3567, LR: 0.549441\nEpoch [151/800], Batch [50/98], Loss: 5.3823, LR: 0.549441\nEpoch [151/800], Batch [60/98], Loss: 5.3696, LR: 0.549441\nEpoch [151/800], Batch [70/98], Loss: 5.3920, LR: 0.549441\nEpoch [151/800], Batch [80/98], Loss: 5.3770, LR: 0.549441\nEpoch [151/800], Batch [90/98], Loss: 5.3724, LR: 0.549441\nEpoch [151/800] Average Loss: 5.3544\n\nEpoch [152/800], Batch [0/98], Loss: 5.3678, LR: 0.548784\nEpoch [152/800], Batch [10/98], Loss: 5.3666, LR: 0.548784\nEpoch [152/800], Batch [20/98], Loss: 5.3522, LR: 0.548784\nEpoch [152/800], Batch [30/98], Loss: 5.3853, LR: 0.548784\nEpoch [152/800], Batch [40/98], Loss: 5.3395, LR: 0.548784\nEpoch [152/800], Batch [50/98], Loss: 5.3721, LR: 0.548784\nEpoch [152/800], Batch [60/98], Loss: 5.3624, LR: 0.548784\nEpoch [152/800], Batch [70/98], Loss: 5.3670, LR: 0.548784\nEpoch [152/800], Batch [80/98], Loss: 5.3996, LR: 0.548784\nEpoch [152/800], Batch [90/98], Loss: 5.3630, LR: 0.548784\nEpoch [152/800] Average Loss: 5.3558\n\nEpoch [153/800], Batch [0/98], Loss: 5.3644, LR: 0.548124\nEpoch [153/800], Batch [10/98], Loss: 5.3778, LR: 0.548124\nEpoch [153/800], Batch [20/98], Loss: 5.3709, LR: 0.548124\nEpoch [153/800], Batch [30/98], Loss: 5.3441, LR: 0.548124\nEpoch [153/800], Batch [40/98], Loss: 5.3363, LR: 0.548124\nEpoch [153/800], Batch [50/98], Loss: 5.3359, LR: 0.548124\nEpoch [153/800], Batch [60/98], Loss: 5.3570, LR: 0.548124\nEpoch [153/800], Batch [70/98], Loss: 5.3420, LR: 0.548124\nEpoch [153/800], Batch [80/98], Loss: 5.3554, LR: 0.548124\nEpoch [153/800], Batch [90/98], Loss: 5.3611, LR: 0.548124\nEpoch [153/800] Average Loss: 5.3515\n\nEpoch [154/800], Batch [0/98], Loss: 5.3749, LR: 0.547460\nEpoch [154/800], Batch [10/98], Loss: 5.3673, LR: 0.547460\nEpoch [154/800], Batch [20/98], Loss: 5.3524, LR: 0.547460\nEpoch [154/800], Batch [30/98], Loss: 5.3383, LR: 0.547460\nEpoch [154/800], Batch [40/98], Loss: 5.3613, LR: 0.547460\nEpoch [154/800], Batch [50/98], Loss: 5.3488, LR: 0.547460\nEpoch [154/800], Batch [60/98], Loss: 5.3688, LR: 0.547460\nEpoch [154/800], Batch [70/98], Loss: 5.3534, LR: 0.547460\nEpoch [154/800], Batch [80/98], Loss: 5.3471, LR: 0.547460\nEpoch [154/800], Batch [90/98], Loss: 5.3735, LR: 0.547460\nEpoch [154/800] Average Loss: 5.3560\n\nEpoch [155/800], Batch [0/98], Loss: 5.3609, LR: 0.546792\nEpoch [155/800], Batch [10/98], Loss: 5.3453, LR: 0.546792\nEpoch [155/800], Batch [20/98], Loss: 5.3725, LR: 0.546792\nEpoch [155/800], Batch [30/98], Loss: 5.3491, LR: 0.546792\nEpoch [155/800], Batch [40/98], Loss: 5.3358, LR: 0.546792\nEpoch [155/800], Batch [50/98], Loss: 5.3760, LR: 0.546792\nEpoch [155/800], Batch [60/98], Loss: 5.3359, LR: 0.546792\nEpoch [155/800], Batch [70/98], Loss: 5.3414, LR: 0.546792\nEpoch [155/800], Batch [80/98], Loss: 5.3800, LR: 0.546792\nEpoch [155/800], Batch [90/98], Loss: 5.3697, LR: 0.546792\nEpoch [155/800] Average Loss: 5.3516\n\nEpoch [156/800], Batch [0/98], Loss: 5.3721, LR: 0.546120\nEpoch [156/800], Batch [10/98], Loss: 5.3691, LR: 0.546120\nEpoch [156/800], Batch [20/98], Loss: 5.3665, LR: 0.546120\nEpoch [156/800], Batch [30/98], Loss: 5.3535, LR: 0.546120\nEpoch [156/800], Batch [40/98], Loss: 5.3629, LR: 0.546120\nEpoch [156/800], Batch [50/98], Loss: 5.3419, LR: 0.546120\nEpoch [156/800], Batch [60/98], Loss: 5.3393, LR: 0.546120\nEpoch [156/800], Batch [70/98], Loss: 5.3800, LR: 0.546120\nEpoch [156/800], Batch [80/98], Loss: 5.3369, LR: 0.546120\nEpoch [156/800], Batch [90/98], Loss: 5.3360, LR: 0.546120\nEpoch [156/800] Average Loss: 5.3535\n\nEpoch [157/800], Batch [0/98], Loss: 5.3703, LR: 0.545445\nEpoch [157/800], Batch [10/98], Loss: 5.3620, LR: 0.545445\nEpoch [157/800], Batch [20/98], Loss: 5.3734, LR: 0.545445\nEpoch [157/800], Batch [30/98], Loss: 5.3606, LR: 0.545445\nEpoch [157/800], Batch [40/98], Loss: 5.3357, LR: 0.545445\nEpoch [157/800], Batch [50/98], Loss: 5.3704, LR: 0.545445\nEpoch [157/800], Batch [60/98], Loss: 5.3663, LR: 0.545445\nEpoch [157/800], Batch [70/98], Loss: 5.3556, LR: 0.545445\nEpoch [157/800], Batch [80/98], Loss: 5.3330, LR: 0.545445\nEpoch [157/800], Batch [90/98], Loss: 5.3514, LR: 0.545445\nEpoch [157/800] Average Loss: 5.3522\n\nEpoch [158/800], Batch [0/98], Loss: 5.3341, LR: 0.544766\nEpoch [158/800], Batch [10/98], Loss: 5.3774, LR: 0.544766\nEpoch [158/800], Batch [20/98], Loss: 5.3509, LR: 0.544766\nEpoch [158/800], Batch [30/98], Loss: 5.3281, LR: 0.544766\nEpoch [158/800], Batch [40/98], Loss: 5.3426, LR: 0.544766\nEpoch [158/800], Batch [50/98], Loss: 5.3546, LR: 0.544766\nEpoch [158/800], Batch [60/98], Loss: 5.3502, LR: 0.544766\nEpoch [158/800], Batch [70/98], Loss: 5.3395, LR: 0.544766\nEpoch [158/800], Batch [80/98], Loss: 5.3483, LR: 0.544766\nEpoch [158/800], Batch [90/98], Loss: 5.3662, LR: 0.544766\nEpoch [158/800] Average Loss: 5.3510\n\nEpoch [159/800], Batch [0/98], Loss: 5.3512, LR: 0.544083\nEpoch [159/800], Batch [10/98], Loss: 5.3531, LR: 0.544083\nEpoch [159/800], Batch [20/98], Loss: 5.3397, LR: 0.544083\nEpoch [159/800], Batch [30/98], Loss: 5.3500, LR: 0.544083\nEpoch [159/800], Batch [40/98], Loss: 5.3746, LR: 0.544083\nEpoch [159/800], Batch [50/98], Loss: 5.3485, LR: 0.544083\nEpoch [159/800], Batch [60/98], Loss: 5.3622, LR: 0.544083\nEpoch [159/800], Batch [70/98], Loss: 5.3727, LR: 0.544083\nEpoch [159/800], Batch [80/98], Loss: 5.3320, LR: 0.544083\nEpoch [159/800], Batch [90/98], Loss: 5.3496, LR: 0.544083\nEpoch [159/800] Average Loss: 5.3534\n\nEpoch [160/800], Batch [0/98], Loss: 5.3204, LR: 0.543396\nEpoch [160/800], Batch [10/98], Loss: 5.3761, LR: 0.543396\nEpoch [160/800], Batch [20/98], Loss: 5.3300, LR: 0.543396\nEpoch [160/800], Batch [30/98], Loss: 5.3504, LR: 0.543396\nEpoch [160/800], Batch [40/98], Loss: 5.3493, LR: 0.543396\nEpoch [160/800], Batch [50/98], Loss: 5.3318, LR: 0.543396\nEpoch [160/800], Batch [60/98], Loss: 5.3811, LR: 0.543396\nEpoch [160/800], Batch [70/98], Loss: 5.3516, LR: 0.543396\nEpoch [160/800], Batch [80/98], Loss: 5.3516, LR: 0.543396\nEpoch [160/800], Batch [90/98], Loss: 5.3578, LR: 0.543396\nEpoch [160/800] Average Loss: 5.3512\n\nEpoch [161/800], Batch [0/98], Loss: 5.3445, LR: 0.542705\nEpoch [161/800], Batch [10/98], Loss: 5.3517, LR: 0.542705\nEpoch [161/800], Batch [20/98], Loss: 5.3792, LR: 0.542705\nEpoch [161/800], Batch [30/98], Loss: 5.3692, LR: 0.542705\nEpoch [161/800], Batch [40/98], Loss: 5.3580, LR: 0.542705\nEpoch [161/800], Batch [50/98], Loss: 5.3875, LR: 0.542705\nEpoch [161/800], Batch [60/98], Loss: 5.3677, LR: 0.542705\nEpoch [161/800], Batch [70/98], Loss: 5.3477, LR: 0.542705\nEpoch [161/800], Batch [80/98], Loss: 5.3583, LR: 0.542705\nEpoch [161/800], Batch [90/98], Loss: 5.3524, LR: 0.542705\nEpoch [161/800] Average Loss: 5.3529\n\nEpoch [162/800], Batch [0/98], Loss: 5.3414, LR: 0.542011\nEpoch [162/800], Batch [10/98], Loss: 5.3651, LR: 0.542011\nEpoch [162/800], Batch [20/98], Loss: 5.3511, LR: 0.542011\nEpoch [162/800], Batch [30/98], Loss: 5.3650, LR: 0.542011\nEpoch [162/800], Batch [40/98], Loss: 5.3511, LR: 0.542011\nEpoch [162/800], Batch [50/98], Loss: 5.3726, LR: 0.542011\nEpoch [162/800], Batch [60/98], Loss: 5.3499, LR: 0.542011\nEpoch [162/800], Batch [70/98], Loss: 5.3722, LR: 0.542011\nEpoch [162/800], Batch [80/98], Loss: 5.3357, LR: 0.542011\nEpoch [162/800], Batch [90/98], Loss: 5.3721, LR: 0.542011\nEpoch [162/800] Average Loss: 5.3512\n\nEpoch [163/800], Batch [0/98], Loss: 5.3544, LR: 0.541313\nEpoch [163/800], Batch [10/98], Loss: 5.3284, LR: 0.541313\nEpoch [163/800], Batch [20/98], Loss: 5.3466, LR: 0.541313\nEpoch [163/800], Batch [30/98], Loss: 5.3547, LR: 0.541313\nEpoch [163/800], Batch [40/98], Loss: 5.3434, LR: 0.541313\nEpoch [163/800], Batch [50/98], Loss: 5.3376, LR: 0.541313\nEpoch [163/800], Batch [60/98], Loss: 5.3499, LR: 0.541313\nEpoch [163/800], Batch [70/98], Loss: 5.3484, LR: 0.541313\nEpoch [163/800], Batch [80/98], Loss: 5.3475, LR: 0.541313\nEpoch [163/800], Batch [90/98], Loss: 5.3312, LR: 0.541313\nEpoch [163/800] Average Loss: 5.3513\n\nEpoch [164/800], Batch [0/98], Loss: 5.3598, LR: 0.540611\nEpoch [164/800], Batch [10/98], Loss: 5.3733, LR: 0.540611\nEpoch [164/800], Batch [20/98], Loss: 5.3750, LR: 0.540611\nEpoch [164/800], Batch [30/98], Loss: 5.3593, LR: 0.540611\nEpoch [164/800], Batch [40/98], Loss: 5.3262, LR: 0.540611\nEpoch [164/800], Batch [50/98], Loss: 5.3683, LR: 0.540611\nEpoch [164/800], Batch [60/98], Loss: 5.3492, LR: 0.540611\nEpoch [164/800], Batch [70/98], Loss: 5.3605, LR: 0.540611\nEpoch [164/800], Batch [80/98], Loss: 5.3534, LR: 0.540611\nEpoch [164/800], Batch [90/98], Loss: 5.3347, LR: 0.540611\nEpoch [164/800] Average Loss: 5.3497\n\nEpoch [165/800], Batch [0/98], Loss: 5.3411, LR: 0.539905\nEpoch [165/800], Batch [10/98], Loss: 5.3544, LR: 0.539905\nEpoch [165/800], Batch [20/98], Loss: 5.3518, LR: 0.539905\nEpoch [165/800], Batch [30/98], Loss: 5.3575, LR: 0.539905\nEpoch [165/800], Batch [40/98], Loss: 5.3242, LR: 0.539905\nEpoch [165/800], Batch [50/98], Loss: 5.3822, LR: 0.539905\nEpoch [165/800], Batch [60/98], Loss: 5.3738, LR: 0.539905\nEpoch [165/800], Batch [70/98], Loss: 5.3473, LR: 0.539905\nEpoch [165/800], Batch [80/98], Loss: 5.3700, LR: 0.539905\nEpoch [165/800], Batch [90/98], Loss: 5.3383, LR: 0.539905\nEpoch [165/800] Average Loss: 5.3465\n\nEpoch [166/800], Batch [0/98], Loss: 5.3743, LR: 0.539196\nEpoch [166/800], Batch [10/98], Loss: 5.3816, LR: 0.539196\nEpoch [166/800], Batch [20/98], Loss: 5.3630, LR: 0.539196\nEpoch [166/800], Batch [30/98], Loss: 5.3958, LR: 0.539196\nEpoch [166/800], Batch [40/98], Loss: 5.3356, LR: 0.539196\nEpoch [166/800], Batch [50/98], Loss: 5.3580, LR: 0.539196\nEpoch [166/800], Batch [60/98], Loss: 5.3595, LR: 0.539196\nEpoch [166/800], Batch [70/98], Loss: 5.3456, LR: 0.539196\nEpoch [166/800], Batch [80/98], Loss: 5.3626, LR: 0.539196\nEpoch [166/800], Batch [90/98], Loss: 5.3191, LR: 0.539196\nEpoch [166/800] Average Loss: 5.3497\n\nEpoch [167/800], Batch [0/98], Loss: 5.3394, LR: 0.538483\nEpoch [167/800], Batch [10/98], Loss: 5.3888, LR: 0.538483\nEpoch [167/800], Batch [20/98], Loss: 5.3621, LR: 0.538483\nEpoch [167/800], Batch [30/98], Loss: 5.3265, LR: 0.538483\nEpoch [167/800], Batch [40/98], Loss: 5.3635, LR: 0.538483\nEpoch [167/800], Batch [50/98], Loss: 5.3458, LR: 0.538483\nEpoch [167/800], Batch [60/98], Loss: 5.3558, LR: 0.538483\nEpoch [167/800], Batch [70/98], Loss: 5.3544, LR: 0.538483\nEpoch [167/800], Batch [80/98], Loss: 5.3717, LR: 0.538483\nEpoch [167/800], Batch [90/98], Loss: 5.3357, LR: 0.538483\nEpoch [167/800] Average Loss: 5.3484\n\nEpoch [168/800], Batch [0/98], Loss: 5.3536, LR: 0.537767\nEpoch [168/800], Batch [10/98], Loss: 5.3714, LR: 0.537767\nEpoch [168/800], Batch [20/98], Loss: 5.3758, LR: 0.537767\nEpoch [168/800], Batch [30/98], Loss: 5.3486, LR: 0.537767\nEpoch [168/800], Batch [40/98], Loss: 5.3587, LR: 0.537767\nEpoch [168/800], Batch [50/98], Loss: 5.3886, LR: 0.537767\nEpoch [168/800], Batch [60/98], Loss: 5.3576, LR: 0.537767\nEpoch [168/800], Batch [70/98], Loss: 5.3491, LR: 0.537767\nEpoch [168/800], Batch [80/98], Loss: 5.3467, LR: 0.537767\nEpoch [168/800], Batch [90/98], Loss: 5.3511, LR: 0.537767\nEpoch [168/800] Average Loss: 5.3501\n\nEpoch [169/800], Batch [0/98], Loss: 5.3524, LR: 0.537047\nEpoch [169/800], Batch [10/98], Loss: 5.3502, LR: 0.537047\nEpoch [169/800], Batch [20/98], Loss: 5.3420, LR: 0.537047\nEpoch [169/800], Batch [30/98], Loss: 5.3472, LR: 0.537047\nEpoch [169/800], Batch [40/98], Loss: 5.3596, LR: 0.537047\nEpoch [169/800], Batch [50/98], Loss: 5.3380, LR: 0.537047\nEpoch [169/800], Batch [60/98], Loss: 5.3317, LR: 0.537047\nEpoch [169/800], Batch [70/98], Loss: 5.3475, LR: 0.537047\nEpoch [169/800], Batch [80/98], Loss: 5.3539, LR: 0.537047\nEpoch [169/800], Batch [90/98], Loss: 5.3391, LR: 0.537047\nEpoch [169/800] Average Loss: 5.3451\n\nEpoch [170/800], Batch [0/98], Loss: 5.3457, LR: 0.536323\nEpoch [170/800], Batch [10/98], Loss: 5.3618, LR: 0.536323\nEpoch [170/800], Batch [20/98], Loss: 5.3365, LR: 0.536323\nEpoch [170/800], Batch [30/98], Loss: 5.3431, LR: 0.536323\nEpoch [170/800], Batch [40/98], Loss: 5.3390, LR: 0.536323\nEpoch [170/800], Batch [50/98], Loss: 5.3637, LR: 0.536323\nEpoch [170/800], Batch [60/98], Loss: 5.3586, LR: 0.536323\nEpoch [170/800], Batch [70/98], Loss: 5.3721, LR: 0.536323\nEpoch [170/800], Batch [80/98], Loss: 5.3636, LR: 0.536323\nEpoch [170/800], Batch [90/98], Loss: 5.3320, LR: 0.536323\nEpoch [170/800] Average Loss: 5.3461\n\nEpoch [171/800], Batch [0/98], Loss: 5.3582, LR: 0.535595\nEpoch [171/800], Batch [10/98], Loss: 5.3494, LR: 0.535595\nEpoch [171/800], Batch [20/98], Loss: 5.3473, LR: 0.535595\nEpoch [171/800], Batch [30/98], Loss: 5.3250, LR: 0.535595\nEpoch [171/800], Batch [40/98], Loss: 5.3706, LR: 0.535595\nEpoch [171/800], Batch [50/98], Loss: 5.3503, LR: 0.535595\nEpoch [171/800], Batch [60/98], Loss: 5.3374, LR: 0.535595\nEpoch [171/800], Batch [70/98], Loss: 5.3345, LR: 0.535595\nEpoch [171/800], Batch [80/98], Loss: 5.3244, LR: 0.535595\nEpoch [171/800], Batch [90/98], Loss: 5.3526, LR: 0.535595\nEpoch [171/800] Average Loss: 5.3432\n\nEpoch [172/800], Batch [0/98], Loss: 5.3361, LR: 0.534864\nEpoch [172/800], Batch [10/98], Loss: 5.3658, LR: 0.534864\nEpoch [172/800], Batch [20/98], Loss: 5.3496, LR: 0.534864\nEpoch [172/800], Batch [30/98], Loss: 5.3330, LR: 0.534864\nEpoch [172/800], Batch [40/98], Loss: 5.3388, LR: 0.534864\nEpoch [172/800], Batch [50/98], Loss: 5.3432, LR: 0.534864\nEpoch [172/800], Batch [60/98], Loss: 5.3296, LR: 0.534864\nEpoch [172/800], Batch [70/98], Loss: 5.3411, LR: 0.534864\nEpoch [172/800], Batch [80/98], Loss: 5.3582, LR: 0.534864\nEpoch [172/800], Batch [90/98], Loss: 5.3398, LR: 0.534864\nEpoch [172/800] Average Loss: 5.3471\n\nEpoch [173/800], Batch [0/98], Loss: 5.3377, LR: 0.534129\nEpoch [173/800], Batch [10/98], Loss: 5.3483, LR: 0.534129\nEpoch [173/800], Batch [20/98], Loss: 5.3351, LR: 0.534129\nEpoch [173/800], Batch [30/98], Loss: 5.3296, LR: 0.534129\nEpoch [173/800], Batch [40/98], Loss: 5.3581, LR: 0.534129\nEpoch [173/800], Batch [50/98], Loss: 5.3779, LR: 0.534129\nEpoch [173/800], Batch [60/98], Loss: 5.3328, LR: 0.534129\nEpoch [173/800], Batch [70/98], Loss: 5.3496, LR: 0.534129\nEpoch [173/800], Batch [80/98], Loss: 5.3217, LR: 0.534129\nEpoch [173/800], Batch [90/98], Loss: 5.3459, LR: 0.534129\nEpoch [173/800] Average Loss: 5.3460\n\nEpoch [174/800], Batch [0/98], Loss: 5.3570, LR: 0.533391\nEpoch [174/800], Batch [10/98], Loss: 5.3265, LR: 0.533391\nEpoch [174/800], Batch [20/98], Loss: 5.3457, LR: 0.533391\nEpoch [174/800], Batch [30/98], Loss: 5.3755, LR: 0.533391\nEpoch [174/800], Batch [40/98], Loss: 5.3597, LR: 0.533391\nEpoch [174/800], Batch [50/98], Loss: 5.3297, LR: 0.533391\nEpoch [174/800], Batch [60/98], Loss: 5.3304, LR: 0.533391\nEpoch [174/800], Batch [70/98], Loss: 5.3621, LR: 0.533391\nEpoch [174/800], Batch [80/98], Loss: 5.3578, LR: 0.533391\nEpoch [174/800], Batch [90/98], Loss: 5.3471, LR: 0.533391\nEpoch [174/800] Average Loss: 5.3465\n\nEpoch [175/800], Batch [0/98], Loss: 5.3337, LR: 0.532649\nEpoch [175/800], Batch [10/98], Loss: 5.3529, LR: 0.532649\nEpoch [175/800], Batch [20/98], Loss: 5.3310, LR: 0.532649\nEpoch [175/800], Batch [30/98], Loss: 5.3155, LR: 0.532649\nEpoch [175/800], Batch [40/98], Loss: 5.3502, LR: 0.532649\nEpoch [175/800], Batch [50/98], Loss: 5.3596, LR: 0.532649\nEpoch [175/800], Batch [60/98], Loss: 5.3696, LR: 0.532649\nEpoch [175/800], Batch [70/98], Loss: 5.3463, LR: 0.532649\nEpoch [175/800], Batch [80/98], Loss: 5.3466, LR: 0.532649\nEpoch [175/800], Batch [90/98], Loss: 5.3833, LR: 0.532649\nEpoch [175/800] Average Loss: 5.3442\n\nEpoch [176/800], Batch [0/98], Loss: 5.3610, LR: 0.531903\nEpoch [176/800], Batch [10/98], Loss: 5.3644, LR: 0.531903\nEpoch [176/800], Batch [20/98], Loss: 5.3560, LR: 0.531903\nEpoch [176/800], Batch [30/98], Loss: 5.3284, LR: 0.531903\nEpoch [176/800], Batch [40/98], Loss: 5.3306, LR: 0.531903\nEpoch [176/800], Batch [50/98], Loss: 5.3222, LR: 0.531903\nEpoch [176/800], Batch [60/98], Loss: 5.3384, LR: 0.531903\nEpoch [176/800], Batch [70/98], Loss: 5.3640, LR: 0.531903\nEpoch [176/800], Batch [80/98], Loss: 5.3317, LR: 0.531903\nEpoch [176/800], Batch [90/98], Loss: 5.3528, LR: 0.531903\nEpoch [176/800] Average Loss: 5.3449\n\nEpoch [177/800], Batch [0/98], Loss: 5.3493, LR: 0.531154\nEpoch [177/800], Batch [10/98], Loss: 5.3724, LR: 0.531154\nEpoch [177/800], Batch [20/98], Loss: 5.3551, LR: 0.531154\nEpoch [177/800], Batch [30/98], Loss: 5.3626, LR: 0.531154\nEpoch [177/800], Batch [40/98], Loss: 5.3608, LR: 0.531154\nEpoch [177/800], Batch [50/98], Loss: 5.3341, LR: 0.531154\nEpoch [177/800], Batch [60/98], Loss: 5.3431, LR: 0.531154\nEpoch [177/800], Batch [70/98], Loss: 5.3287, LR: 0.531154\nEpoch [177/800], Batch [80/98], Loss: 5.3372, LR: 0.531154\nEpoch [177/800], Batch [90/98], Loss: 5.3440, LR: 0.531154\nEpoch [177/800] Average Loss: 5.3429\n\nEpoch [178/800], Batch [0/98], Loss: 5.3437, LR: 0.530401\nEpoch [178/800], Batch [10/98], Loss: 5.3550, LR: 0.530401\nEpoch [178/800], Batch [20/98], Loss: 5.3327, LR: 0.530401\nEpoch [178/800], Batch [30/98], Loss: 5.3700, LR: 0.530401\nEpoch [178/800], Batch [40/98], Loss: 5.3991, LR: 0.530401\nEpoch [178/800], Batch [50/98], Loss: 5.3382, LR: 0.530401\nEpoch [178/800], Batch [60/98], Loss: 5.3477, LR: 0.530401\nEpoch [178/800], Batch [70/98], Loss: 5.3353, LR: 0.530401\nEpoch [178/800], Batch [80/98], Loss: 5.3213, LR: 0.530401\nEpoch [178/800], Batch [90/98], Loss: 5.3261, LR: 0.530401\nEpoch [178/800] Average Loss: 5.3423\n\nEpoch [179/800], Batch [0/98], Loss: 5.3516, LR: 0.529645\nEpoch [179/800], Batch [10/98], Loss: 5.3442, LR: 0.529645\nEpoch [179/800], Batch [20/98], Loss: 5.3355, LR: 0.529645\nEpoch [179/800], Batch [30/98], Loss: 5.3473, LR: 0.529645\nEpoch [179/800], Batch [40/98], Loss: 5.3501, LR: 0.529645\nEpoch [179/800], Batch [50/98], Loss: 5.3190, LR: 0.529645\nEpoch [179/800], Batch [60/98], Loss: 5.3695, LR: 0.529645\nEpoch [179/800], Batch [70/98], Loss: 5.3410, LR: 0.529645\nEpoch [179/800], Batch [80/98], Loss: 5.3204, LR: 0.529645\nEpoch [179/800], Batch [90/98], Loss: 5.3749, LR: 0.529645\nEpoch [179/800] Average Loss: 5.3431\n\nEpoch [180/800], Batch [0/98], Loss: 5.3454, LR: 0.528885\nEpoch [180/800], Batch [10/98], Loss: 5.3541, LR: 0.528885\nEpoch [180/800], Batch [20/98], Loss: 5.3453, LR: 0.528885\nEpoch [180/800], Batch [30/98], Loss: 5.3574, LR: 0.528885\nEpoch [180/800], Batch [40/98], Loss: 5.3461, LR: 0.528885\nEpoch [180/800], Batch [50/98], Loss: 5.3499, LR: 0.528885\nEpoch [180/800], Batch [60/98], Loss: 5.3321, LR: 0.528885\nEpoch [180/800], Batch [70/98], Loss: 5.3370, LR: 0.528885\nEpoch [180/800], Batch [80/98], Loss: 5.3603, LR: 0.528885\nEpoch [180/800], Batch [90/98], Loss: 5.3216, LR: 0.528885\nEpoch [180/800] Average Loss: 5.3400\n\nEpoch [181/800], Batch [0/98], Loss: 5.3397, LR: 0.528122\nEpoch [181/800], Batch [10/98], Loss: 5.3331, LR: 0.528122\nEpoch [181/800], Batch [20/98], Loss: 5.3298, LR: 0.528122\nEpoch [181/800], Batch [30/98], Loss: 5.3502, LR: 0.528122\nEpoch [181/800], Batch [40/98], Loss: 5.3835, LR: 0.528122\nEpoch [181/800], Batch [50/98], Loss: 5.3375, LR: 0.528122\nEpoch [181/800], Batch [60/98], Loss: 5.3680, LR: 0.528122\nEpoch [181/800], Batch [70/98], Loss: 5.3373, LR: 0.528122\nEpoch [181/800], Batch [80/98], Loss: 5.3280, LR: 0.528122\nEpoch [181/800], Batch [90/98], Loss: 5.3938, LR: 0.528122\nEpoch [181/800] Average Loss: 5.3424\n\nEpoch [182/800], Batch [0/98], Loss: 5.3277, LR: 0.527355\nEpoch [182/800], Batch [10/98], Loss: 5.3414, LR: 0.527355\nEpoch [182/800], Batch [20/98], Loss: 5.3427, LR: 0.527355\nEpoch [182/800], Batch [30/98], Loss: 5.3682, LR: 0.527355\nEpoch [182/800], Batch [40/98], Loss: 5.3409, LR: 0.527355\nEpoch [182/800], Batch [50/98], Loss: 5.3659, LR: 0.527355\nEpoch [182/800], Batch [60/98], Loss: 5.3595, LR: 0.527355\nEpoch [182/800], Batch [70/98], Loss: 5.3440, LR: 0.527355\nEpoch [182/800], Batch [80/98], Loss: 5.3410, LR: 0.527355\nEpoch [182/800], Batch [90/98], Loss: 5.3646, LR: 0.527355\nEpoch [182/800] Average Loss: 5.3404\n\nEpoch [183/800], Batch [0/98], Loss: 5.3396, LR: 0.526585\nEpoch [183/800], Batch [10/98], Loss: 5.3512, LR: 0.526585\nEpoch [183/800], Batch [20/98], Loss: 5.3494, LR: 0.526585\nEpoch [183/800], Batch [30/98], Loss: 5.3504, LR: 0.526585\nEpoch [183/800], Batch [40/98], Loss: 5.3204, LR: 0.526585\nEpoch [183/800], Batch [50/98], Loss: 5.3405, LR: 0.526585\nEpoch [183/800], Batch [60/98], Loss: 5.3600, LR: 0.526585\nEpoch [183/800], Batch [70/98], Loss: 5.3119, LR: 0.526585\nEpoch [183/800], Batch [80/98], Loss: 5.3270, LR: 0.526585\nEpoch [183/800], Batch [90/98], Loss: 5.3661, LR: 0.526585\nEpoch [183/800] Average Loss: 5.3385\n\nEpoch [184/800], Batch [0/98], Loss: 5.3314, LR: 0.525811\nEpoch [184/800], Batch [10/98], Loss: 5.3743, LR: 0.525811\nEpoch [184/800], Batch [20/98], Loss: 5.3461, LR: 0.525811\nEpoch [184/800], Batch [30/98], Loss: 5.3578, LR: 0.525811\nEpoch [184/800], Batch [40/98], Loss: 5.3229, LR: 0.525811\nEpoch [184/800], Batch [50/98], Loss: 5.3790, LR: 0.525811\nEpoch [184/800], Batch [60/98], Loss: 5.3430, LR: 0.525811\nEpoch [184/800], Batch [70/98], Loss: 5.3742, LR: 0.525811\nEpoch [184/800], Batch [80/98], Loss: 5.3480, LR: 0.525811\nEpoch [184/800], Batch [90/98], Loss: 5.3389, LR: 0.525811\nEpoch [184/800] Average Loss: 5.3415\n\nEpoch [185/800], Batch [0/98], Loss: 5.3644, LR: 0.525033\nEpoch [185/800], Batch [10/98], Loss: 5.3397, LR: 0.525033\nEpoch [185/800], Batch [20/98], Loss: 5.3367, LR: 0.525033\nEpoch [185/800], Batch [30/98], Loss: 5.3432, LR: 0.525033\nEpoch [185/800], Batch [40/98], Loss: 5.3453, LR: 0.525033\nEpoch [185/800], Batch [50/98], Loss: 5.3454, LR: 0.525033\nEpoch [185/800], Batch [60/98], Loss: 5.3635, LR: 0.525033\nEpoch [185/800], Batch [70/98], Loss: 5.3400, LR: 0.525033\nEpoch [185/800], Batch [80/98], Loss: 5.3581, LR: 0.525033\nEpoch [185/800], Batch [90/98], Loss: 5.3465, LR: 0.525033\nEpoch [185/800] Average Loss: 5.3404\n\nEpoch [186/800], Batch [0/98], Loss: 5.3368, LR: 0.524252\nEpoch [186/800], Batch [10/98], Loss: 5.3281, LR: 0.524252\nEpoch [186/800], Batch [20/98], Loss: 5.3642, LR: 0.524252\nEpoch [186/800], Batch [30/98], Loss: 5.3442, LR: 0.524252\nEpoch [186/800], Batch [40/98], Loss: 5.3499, LR: 0.524252\nEpoch [186/800], Batch [50/98], Loss: 5.3474, LR: 0.524252\nEpoch [186/800], Batch [60/98], Loss: 5.3561, LR: 0.524252\nEpoch [186/800], Batch [70/98], Loss: 5.3285, LR: 0.524252\nEpoch [186/800], Batch [80/98], Loss: 5.3561, LR: 0.524252\nEpoch [186/800], Batch [90/98], Loss: 5.3598, LR: 0.524252\nEpoch [186/800] Average Loss: 5.3405\n\nEpoch [187/800], Batch [0/98], Loss: 5.3471, LR: 0.523468\nEpoch [187/800], Batch [10/98], Loss: 5.3493, LR: 0.523468\nEpoch [187/800], Batch [20/98], Loss: 5.3353, LR: 0.523468\nEpoch [187/800], Batch [30/98], Loss: 5.3518, LR: 0.523468\nEpoch [187/800], Batch [40/98], Loss: 5.3470, LR: 0.523468\nEpoch [187/800], Batch [50/98], Loss: 5.3248, LR: 0.523468\nEpoch [187/800], Batch [60/98], Loss: 5.3524, LR: 0.523468\nEpoch [187/800], Batch [70/98], Loss: 5.3429, LR: 0.523468\nEpoch [187/800], Batch [80/98], Loss: 5.3394, LR: 0.523468\nEpoch [187/800], Batch [90/98], Loss: 5.3514, LR: 0.523468\nEpoch [187/800] Average Loss: 5.3419\n\nEpoch [188/800], Batch [0/98], Loss: 5.3607, LR: 0.522680\nEpoch [188/800], Batch [10/98], Loss: 5.3551, LR: 0.522680\nEpoch [188/800], Batch [20/98], Loss: 5.3241, LR: 0.522680\nEpoch [188/800], Batch [30/98], Loss: 5.3562, LR: 0.522680\nEpoch [188/800], Batch [40/98], Loss: 5.3297, LR: 0.522680\nEpoch [188/800], Batch [50/98], Loss: 5.3345, LR: 0.522680\nEpoch [188/800], Batch [60/98], Loss: 5.3243, LR: 0.522680\nEpoch [188/800], Batch [70/98], Loss: 5.3443, LR: 0.522680\nEpoch [188/800], Batch [80/98], Loss: 5.3309, LR: 0.522680\nEpoch [188/800], Batch [90/98], Loss: 5.3511, LR: 0.522680\nEpoch [188/800] Average Loss: 5.3376\n\nEpoch [189/800], Batch [0/98], Loss: 5.3785, LR: 0.521889\nEpoch [189/800], Batch [10/98], Loss: 5.3117, LR: 0.521889\nEpoch [189/800], Batch [20/98], Loss: 5.3605, LR: 0.521889\nEpoch [189/800], Batch [30/98], Loss: 5.3257, LR: 0.521889\nEpoch [189/800], Batch [40/98], Loss: 5.3405, LR: 0.521889\nEpoch [189/800], Batch [50/98], Loss: 5.3241, LR: 0.521889\nEpoch [189/800], Batch [60/98], Loss: 5.3604, LR: 0.521889\nEpoch [189/800], Batch [70/98], Loss: 5.3345, LR: 0.521889\nEpoch [189/800], Batch [80/98], Loss: 5.3590, LR: 0.521889\nEpoch [189/800], Batch [90/98], Loss: 5.3348, LR: 0.521889\nEpoch [189/800] Average Loss: 5.3378\n\nEpoch [190/800], Batch [0/98], Loss: 5.3400, LR: 0.521095\nEpoch [190/800], Batch [10/98], Loss: 5.3329, LR: 0.521095\nEpoch [190/800], Batch [20/98], Loss: 5.3200, LR: 0.521095\nEpoch [190/800], Batch [30/98], Loss: 5.3569, LR: 0.521095\nEpoch [190/800], Batch [40/98], Loss: 5.3614, LR: 0.521095\nEpoch [190/800], Batch [50/98], Loss: 5.3311, LR: 0.521095\nEpoch [190/800], Batch [60/98], Loss: 5.3406, LR: 0.521095\nEpoch [190/800], Batch [70/98], Loss: 5.3187, LR: 0.521095\nEpoch [190/800], Batch [80/98], Loss: 5.3541, LR: 0.521095\nEpoch [190/800], Batch [90/98], Loss: 5.3445, LR: 0.521095\nEpoch [190/800] Average Loss: 5.3376\n\nEpoch [191/800], Batch [0/98], Loss: 5.3505, LR: 0.520297\nEpoch [191/800], Batch [10/98], Loss: 5.3512, LR: 0.520297\nEpoch [191/800], Batch [20/98], Loss: 5.3295, LR: 0.520297\nEpoch [191/800], Batch [30/98], Loss: 5.3317, LR: 0.520297\nEpoch [191/800], Batch [40/98], Loss: 5.3307, LR: 0.520297\nEpoch [191/800], Batch [50/98], Loss: 5.3243, LR: 0.520297\nEpoch [191/800], Batch [60/98], Loss: 5.3373, LR: 0.520297\nEpoch [191/800], Batch [70/98], Loss: 5.3420, LR: 0.520297\nEpoch [191/800], Batch [80/98], Loss: 5.3484, LR: 0.520297\nEpoch [191/800], Batch [90/98], Loss: 5.3276, LR: 0.520297\nEpoch [191/800] Average Loss: 5.3374\n\nEpoch [192/800], Batch [0/98], Loss: 5.3517, LR: 0.519495\nEpoch [192/800], Batch [10/98], Loss: 5.3581, LR: 0.519495\nEpoch [192/800], Batch [20/98], Loss: 5.3325, LR: 0.519495\nEpoch [192/800], Batch [30/98], Loss: 5.3337, LR: 0.519495\nEpoch [192/800], Batch [40/98], Loss: 5.3512, LR: 0.519495\nEpoch [192/800], Batch [50/98], Loss: 5.3346, LR: 0.519495\nEpoch [192/800], Batch [60/98], Loss: 5.3357, LR: 0.519495\nEpoch [192/800], Batch [70/98], Loss: 5.3332, LR: 0.519495\nEpoch [192/800], Batch [80/98], Loss: 5.3302, LR: 0.519495\nEpoch [192/800], Batch [90/98], Loss: 5.3418, LR: 0.519495\nEpoch [192/800] Average Loss: 5.3354\n\nEpoch [193/800], Batch [0/98], Loss: 5.3502, LR: 0.518691\nEpoch [193/800], Batch [10/98], Loss: 5.3658, LR: 0.518691\nEpoch [193/800], Batch [20/98], Loss: 5.3419, LR: 0.518691\nEpoch [193/800], Batch [30/98], Loss: 5.3418, LR: 0.518691\nEpoch [193/800], Batch [40/98], Loss: 5.3336, LR: 0.518691\nEpoch [193/800], Batch [50/98], Loss: 5.3498, LR: 0.518691\nEpoch [193/800], Batch [60/98], Loss: 5.3316, LR: 0.518691\nEpoch [193/800], Batch [70/98], Loss: 5.3440, LR: 0.518691\nEpoch [193/800], Batch [80/98], Loss: 5.3359, LR: 0.518691\nEpoch [193/800], Batch [90/98], Loss: 5.3624, LR: 0.518691\nEpoch [193/800] Average Loss: 5.3371\n\nEpoch [194/800], Batch [0/98], Loss: 5.3286, LR: 0.517882\nEpoch [194/800], Batch [10/98], Loss: 5.3309, LR: 0.517882\nEpoch [194/800], Batch [20/98], Loss: 5.3209, LR: 0.517882\nEpoch [194/800], Batch [30/98], Loss: 5.3627, LR: 0.517882\nEpoch [194/800], Batch [40/98], Loss: 5.3414, LR: 0.517882\nEpoch [194/800], Batch [50/98], Loss: 5.3396, LR: 0.517882\nEpoch [194/800], Batch [60/98], Loss: 5.3358, LR: 0.517882\nEpoch [194/800], Batch [70/98], Loss: 5.3302, LR: 0.517882\nEpoch [194/800], Batch [80/98], Loss: 5.3466, LR: 0.517882\nEpoch [194/800], Batch [90/98], Loss: 5.3320, LR: 0.517882\nEpoch [194/800] Average Loss: 5.3347\n\nEpoch [195/800], Batch [0/98], Loss: 5.3394, LR: 0.517071\nEpoch [195/800], Batch [10/98], Loss: 5.3590, LR: 0.517071\nEpoch [195/800], Batch [20/98], Loss: 5.3339, LR: 0.517071\nEpoch [195/800], Batch [30/98], Loss: 5.3347, LR: 0.517071\nEpoch [195/800], Batch [40/98], Loss: 5.3447, LR: 0.517071\nEpoch [195/800], Batch [50/98], Loss: 5.3495, LR: 0.517071\nEpoch [195/800], Batch [60/98], Loss: 5.3394, LR: 0.517071\nEpoch [195/800], Batch [70/98], Loss: 5.3569, LR: 0.517071\nEpoch [195/800], Batch [80/98], Loss: 5.3149, LR: 0.517071\nEpoch [195/800], Batch [90/98], Loss: 5.3505, LR: 0.517071\nEpoch [195/800] Average Loss: 5.3387\n\nEpoch [196/800], Batch [0/98], Loss: 5.3311, LR: 0.516256\nEpoch [196/800], Batch [10/98], Loss: 5.3126, LR: 0.516256\nEpoch [196/800], Batch [20/98], Loss: 5.3262, LR: 0.516256\nEpoch [196/800], Batch [30/98], Loss: 5.3477, LR: 0.516256\nEpoch [196/800], Batch [40/98], Loss: 5.3645, LR: 0.516256\nEpoch [196/800], Batch [50/98], Loss: 5.3369, LR: 0.516256\nEpoch [196/800], Batch [60/98], Loss: 5.3242, LR: 0.516256\nEpoch [196/800], Batch [70/98], Loss: 5.3543, LR: 0.516256\nEpoch [196/800], Batch [80/98], Loss: 5.3521, LR: 0.516256\nEpoch [196/800], Batch [90/98], Loss: 5.3062, LR: 0.516256\nEpoch [196/800] Average Loss: 5.3364\n\nEpoch [197/800], Batch [0/98], Loss: 5.3365, LR: 0.515438\nEpoch [197/800], Batch [10/98], Loss: 5.3626, LR: 0.515438\nEpoch [197/800], Batch [20/98], Loss: 5.3705, LR: 0.515438\nEpoch [197/800], Batch [30/98], Loss: 5.3458, LR: 0.515438\nEpoch [197/800], Batch [40/98], Loss: 5.3192, LR: 0.515438\nEpoch [197/800], Batch [50/98], Loss: 5.3086, LR: 0.515438\nEpoch [197/800], Batch [60/98], Loss: 5.3097, LR: 0.515438\nEpoch [197/800], Batch [70/98], Loss: 5.3551, LR: 0.515438\nEpoch [197/800], Batch [80/98], Loss: 5.3509, LR: 0.515438\nEpoch [197/800], Batch [90/98], Loss: 5.3314, LR: 0.515438\nEpoch [197/800] Average Loss: 5.3347\n\nEpoch [198/800], Batch [0/98], Loss: 5.3337, LR: 0.514616\nEpoch [198/800], Batch [10/98], Loss: 5.3167, LR: 0.514616\nEpoch [198/800], Batch [20/98], Loss: 5.3179, LR: 0.514616\nEpoch [198/800], Batch [30/98], Loss: 5.3356, LR: 0.514616\nEpoch [198/800], Batch [40/98], Loss: 5.3251, LR: 0.514616\nEpoch [198/800], Batch [50/98], Loss: 5.3381, LR: 0.514616\nEpoch [198/800], Batch [60/98], Loss: 5.3404, LR: 0.514616\nEpoch [198/800], Batch [70/98], Loss: 5.3240, LR: 0.514616\nEpoch [198/800], Batch [80/98], Loss: 5.3656, LR: 0.514616\nEpoch [198/800], Batch [90/98], Loss: 5.3439, LR: 0.514616\nEpoch [198/800] Average Loss: 5.3349\n\nEpoch [199/800], Batch [0/98], Loss: 5.3452, LR: 0.513792\nEpoch [199/800], Batch [10/98], Loss: 5.3363, LR: 0.513792\nEpoch [199/800], Batch [20/98], Loss: 5.3494, LR: 0.513792\nEpoch [199/800], Batch [30/98], Loss: 5.3532, LR: 0.513792\nEpoch [199/800], Batch [40/98], Loss: 5.3483, LR: 0.513792\nEpoch [199/800], Batch [50/98], Loss: 5.3533, LR: 0.513792\nEpoch [199/800], Batch [60/98], Loss: 5.3353, LR: 0.513792\nEpoch [199/800], Batch [70/98], Loss: 5.3388, LR: 0.513792\nEpoch [199/800], Batch [80/98], Loss: 5.3177, LR: 0.513792\nEpoch [199/800], Batch [90/98], Loss: 5.3110, LR: 0.513792\nEpoch [199/800] Average Loss: 5.3353\n\nEpoch [200/800], Batch [0/98], Loss: 5.3291, LR: 0.512963\nEpoch [200/800], Batch [10/98], Loss: 5.3240, LR: 0.512963\nEpoch [200/800], Batch [20/98], Loss: 5.3258, LR: 0.512963\nEpoch [200/800], Batch [30/98], Loss: 5.3642, LR: 0.512963\nEpoch [200/800], Batch [40/98], Loss: 5.3317, LR: 0.512963\nEpoch [200/800], Batch [50/98], Loss: 5.3246, LR: 0.512963\nEpoch [200/800], Batch [60/98], Loss: 5.3520, LR: 0.512963\nEpoch [200/800], Batch [70/98], Loss: 5.3544, LR: 0.512963\nEpoch [200/800], Batch [80/98], Loss: 5.3135, LR: 0.512963\nEpoch [200/800], Batch [90/98], Loss: 5.3387, LR: 0.512963\nEpoch [200/800] Average Loss: 5.3336\n\nEpoch [201/800], Batch [0/98], Loss: 5.3263, LR: 0.512132\nEpoch [201/800], Batch [10/98], Loss: 5.3359, LR: 0.512132\nEpoch [201/800], Batch [20/98], Loss: 5.3327, LR: 0.512132\nEpoch [201/800], Batch [30/98], Loss: 5.3492, LR: 0.512132\nEpoch [201/800], Batch [40/98], Loss: 5.3215, LR: 0.512132\nEpoch [201/800], Batch [50/98], Loss: 5.3440, LR: 0.512132\nEpoch [201/800], Batch [60/98], Loss: 5.3429, LR: 0.512132\nEpoch [201/800], Batch [70/98], Loss: 5.3322, LR: 0.512132\nEpoch [201/800], Batch [80/98], Loss: 5.3523, LR: 0.512132\nEpoch [201/800], Batch [90/98], Loss: 5.3423, LR: 0.512132\nEpoch [201/800] Average Loss: 5.3359\n\nEpoch [202/800], Batch [0/98], Loss: 5.3374, LR: 0.511297\nEpoch [202/800], Batch [10/98], Loss: 5.3153, LR: 0.511297\nEpoch [202/800], Batch [20/98], Loss: 5.3314, LR: 0.511297\nEpoch [202/800], Batch [30/98], Loss: 5.3504, LR: 0.511297\nEpoch [202/800], Batch [40/98], Loss: 5.3171, LR: 0.511297\nEpoch [202/800], Batch [50/98], Loss: 5.3202, LR: 0.511297\nEpoch [202/800], Batch [60/98], Loss: 5.3385, LR: 0.511297\nEpoch [202/800], Batch [70/98], Loss: 5.3314, LR: 0.511297\nEpoch [202/800], Batch [80/98], Loss: 5.3413, LR: 0.511297\nEpoch [202/800], Batch [90/98], Loss: 5.3445, LR: 0.511297\nEpoch [202/800] Average Loss: 5.3346\n\nEpoch [203/800], Batch [0/98], Loss: 5.3295, LR: 0.510459\nEpoch [203/800], Batch [10/98], Loss: 5.3402, LR: 0.510459\nEpoch [203/800], Batch [20/98], Loss: 5.3594, LR: 0.510459\nEpoch [203/800], Batch [30/98], Loss: 5.3255, LR: 0.510459\nEpoch [203/800], Batch [40/98], Loss: 5.3474, LR: 0.510459\nEpoch [203/800], Batch [50/98], Loss: 5.3198, LR: 0.510459\nEpoch [203/800], Batch [60/98], Loss: 5.3527, LR: 0.510459\nEpoch [203/800], Batch [70/98], Loss: 5.3463, LR: 0.510459\nEpoch [203/800], Batch [80/98], Loss: 5.3566, LR: 0.510459\nEpoch [203/800], Batch [90/98], Loss: 5.3459, LR: 0.510459\nEpoch [203/800] Average Loss: 5.3328\n\nEpoch [204/800], Batch [0/98], Loss: 5.3541, LR: 0.509618\nEpoch [204/800], Batch [10/98], Loss: 5.3572, LR: 0.509618\nEpoch [204/800], Batch [20/98], Loss: 5.3396, LR: 0.509618\nEpoch [204/800], Batch [30/98], Loss: 5.3547, LR: 0.509618\nEpoch [204/800], Batch [40/98], Loss: 5.3342, LR: 0.509618\nEpoch [204/800], Batch [50/98], Loss: 5.3495, LR: 0.509618\nEpoch [204/800], Batch [60/98], Loss: 5.3225, LR: 0.509618\nEpoch [204/800], Batch [70/98], Loss: 5.3245, LR: 0.509618\nEpoch [204/800], Batch [80/98], Loss: 5.3152, LR: 0.509618\nEpoch [204/800], Batch [90/98], Loss: 5.3381, LR: 0.509618\nEpoch [204/800] Average Loss: 5.3337\n\nEpoch [205/800], Batch [0/98], Loss: 5.3314, LR: 0.508774\nEpoch [205/800], Batch [10/98], Loss: 5.3096, LR: 0.508774\nEpoch [205/800], Batch [20/98], Loss: 5.3435, LR: 0.508774\nEpoch [205/800], Batch [30/98], Loss: 5.3589, LR: 0.508774\nEpoch [205/800], Batch [40/98], Loss: 5.3253, LR: 0.508774\nEpoch [205/800], Batch [50/98], Loss: 5.3375, LR: 0.508774\nEpoch [205/800], Batch [60/98], Loss: 5.3465, LR: 0.508774\nEpoch [205/800], Batch [70/98], Loss: 5.3246, LR: 0.508774\nEpoch [205/800], Batch [80/98], Loss: 5.3506, LR: 0.508774\nEpoch [205/800], Batch [90/98], Loss: 5.3270, LR: 0.508774\nEpoch [205/800] Average Loss: 5.3318\n\nEpoch [206/800], Batch [0/98], Loss: 5.3298, LR: 0.507926\nEpoch [206/800], Batch [10/98], Loss: 5.3531, LR: 0.507926\nEpoch [206/800], Batch [20/98], Loss: 5.3330, LR: 0.507926\nEpoch [206/800], Batch [30/98], Loss: 5.2993, LR: 0.507926\nEpoch [206/800], Batch [40/98], Loss: 5.3306, LR: 0.507926\nEpoch [206/800], Batch [50/98], Loss: 5.3349, LR: 0.507926\nEpoch [206/800], Batch [60/98], Loss: 5.3197, LR: 0.507926\nEpoch [206/800], Batch [70/98], Loss: 5.3258, LR: 0.507926\nEpoch [206/800], Batch [80/98], Loss: 5.3259, LR: 0.507926\nEpoch [206/800], Batch [90/98], Loss: 5.3296, LR: 0.507926\nEpoch [206/800] Average Loss: 5.3311\n\nEpoch [207/800], Batch [0/98], Loss: 5.3415, LR: 0.507075\nEpoch [207/800], Batch [10/98], Loss: 5.3235, LR: 0.507075\nEpoch [207/800], Batch [20/98], Loss: 5.3161, LR: 0.507075\nEpoch [207/800], Batch [30/98], Loss: 5.3262, LR: 0.507075\nEpoch [207/800], Batch [40/98], Loss: 5.3226, LR: 0.507075\nEpoch [207/800], Batch [50/98], Loss: 5.3263, LR: 0.507075\nEpoch [207/800], Batch [60/98], Loss: 5.3070, LR: 0.507075\nEpoch [207/800], Batch [70/98], Loss: 5.3439, LR: 0.507075\nEpoch [207/800], Batch [80/98], Loss: 5.3201, LR: 0.507075\nEpoch [207/800], Batch [90/98], Loss: 5.3265, LR: 0.507075\nEpoch [207/800] Average Loss: 5.3291\n\nEpoch [208/800], Batch [0/98], Loss: 5.3176, LR: 0.506221\nEpoch [208/800], Batch [10/98], Loss: 5.3394, LR: 0.506221\nEpoch [208/800], Batch [20/98], Loss: 5.3206, LR: 0.506221\nEpoch [208/800], Batch [30/98], Loss: 5.3400, LR: 0.506221\nEpoch [208/800], Batch [40/98], Loss: 5.3289, LR: 0.506221\nEpoch [208/800], Batch [50/98], Loss: 5.3295, LR: 0.506221\nEpoch [208/800], Batch [60/98], Loss: 5.3516, LR: 0.506221\nEpoch [208/800], Batch [70/98], Loss: 5.3074, LR: 0.506221\nEpoch [208/800], Batch [80/98], Loss: 5.3349, LR: 0.506221\nEpoch [208/800], Batch [90/98], Loss: 5.3107, LR: 0.506221\nEpoch [208/800] Average Loss: 5.3309\n\nEpoch [209/800], Batch [0/98], Loss: 5.3310, LR: 0.505364\nEpoch [209/800], Batch [10/98], Loss: 5.3176, LR: 0.505364\nEpoch [209/800], Batch [20/98], Loss: 5.3233, LR: 0.505364\nEpoch [209/800], Batch [30/98], Loss: 5.3474, LR: 0.505364\nEpoch [209/800], Batch [40/98], Loss: 5.3229, LR: 0.505364\nEpoch [209/800], Batch [50/98], Loss: 5.3411, LR: 0.505364\nEpoch [209/800], Batch [60/98], Loss: 5.2947, LR: 0.505364\nEpoch [209/800], Batch [70/98], Loss: 5.3287, LR: 0.505364\nEpoch [209/800], Batch [80/98], Loss: 5.3362, LR: 0.505364\nEpoch [209/800], Batch [90/98], Loss: 5.3211, LR: 0.505364\nEpoch [209/800] Average Loss: 5.3310\n\nEpoch [210/800], Batch [0/98], Loss: 5.3382, LR: 0.504504\nEpoch [210/800], Batch [10/98], Loss: 5.3285, LR: 0.504504\nEpoch [210/800], Batch [20/98], Loss: 5.3393, LR: 0.504504\nEpoch [210/800], Batch [30/98], Loss: 5.3557, LR: 0.504504\nEpoch [210/800], Batch [40/98], Loss: 5.3506, LR: 0.504504\nEpoch [210/800], Batch [50/98], Loss: 5.3735, LR: 0.504504\nEpoch [210/800], Batch [60/98], Loss: 5.3474, LR: 0.504504\nEpoch [210/800], Batch [70/98], Loss: 5.3547, LR: 0.504504\nEpoch [210/800], Batch [80/98], Loss: 5.3415, LR: 0.504504\nEpoch [210/800], Batch [90/98], Loss: 5.3295, LR: 0.504504\nEpoch [210/800] Average Loss: 5.3316\n\nEpoch [211/800], Batch [0/98], Loss: 5.3410, LR: 0.503640\nEpoch [211/800], Batch [10/98], Loss: 5.3509, LR: 0.503640\nEpoch [211/800], Batch [20/98], Loss: 5.3372, LR: 0.503640\nEpoch [211/800], Batch [30/98], Loss: 5.3516, LR: 0.503640\nEpoch [211/800], Batch [40/98], Loss: 5.3189, LR: 0.503640\nEpoch [211/800], Batch [50/98], Loss: 5.3339, LR: 0.503640\nEpoch [211/800], Batch [60/98], Loss: 5.3160, LR: 0.503640\nEpoch [211/800], Batch [70/98], Loss: 5.3459, LR: 0.503640\nEpoch [211/800], Batch [80/98], Loss: 5.3502, LR: 0.503640\nEpoch [211/800], Batch [90/98], Loss: 5.3336, LR: 0.503640\nEpoch [211/800] Average Loss: 5.3284\n\nEpoch [212/800], Batch [0/98], Loss: 5.3450, LR: 0.502774\nEpoch [212/800], Batch [10/98], Loss: 5.3212, LR: 0.502774\nEpoch [212/800], Batch [20/98], Loss: 5.3407, LR: 0.502774\nEpoch [212/800], Batch [30/98], Loss: 5.3322, LR: 0.502774\nEpoch [212/800], Batch [40/98], Loss: 5.3340, LR: 0.502774\nEpoch [212/800], Batch [50/98], Loss: 5.3329, LR: 0.502774\nEpoch [212/800], Batch [60/98], Loss: 5.3053, LR: 0.502774\nEpoch [212/800], Batch [70/98], Loss: 5.3298, LR: 0.502774\nEpoch [212/800], Batch [80/98], Loss: 5.3179, LR: 0.502774\nEpoch [212/800], Batch [90/98], Loss: 5.3186, LR: 0.502774\nEpoch [212/800] Average Loss: 5.3292\n\nEpoch [213/800], Batch [0/98], Loss: 5.3406, LR: 0.501904\nEpoch [213/800], Batch [10/98], Loss: 5.3414, LR: 0.501904\nEpoch [213/800], Batch [20/98], Loss: 5.3465, LR: 0.501904\nEpoch [213/800], Batch [30/98], Loss: 5.3342, LR: 0.501904\nEpoch [213/800], Batch [40/98], Loss: 5.3285, LR: 0.501904\nEpoch [213/800], Batch [50/98], Loss: 5.3434, LR: 0.501904\nEpoch [213/800], Batch [60/98], Loss: 5.3288, LR: 0.501904\nEpoch [213/800], Batch [70/98], Loss: 5.3589, LR: 0.501904\nEpoch [213/800], Batch [80/98], Loss: 5.3324, LR: 0.501904\nEpoch [213/800], Batch [90/98], Loss: 5.3000, LR: 0.501904\nEpoch [213/800] Average Loss: 5.3312\n\nEpoch [214/800], Batch [0/98], Loss: 5.3221, LR: 0.501031\nEpoch [214/800], Batch [10/98], Loss: 5.3100, LR: 0.501031\nEpoch [214/800], Batch [20/98], Loss: 5.3442, LR: 0.501031\nEpoch [214/800], Batch [30/98], Loss: 5.3474, LR: 0.501031\nEpoch [214/800], Batch [40/98], Loss: 5.3340, LR: 0.501031\nEpoch [214/800], Batch [50/98], Loss: 5.3296, LR: 0.501031\nEpoch [214/800], Batch [60/98], Loss: 5.3578, LR: 0.501031\nEpoch [214/800], Batch [70/98], Loss: 5.3539, LR: 0.501031\nEpoch [214/800], Batch [80/98], Loss: 5.3231, LR: 0.501031\nEpoch [214/800], Batch [90/98], Loss: 5.3335, LR: 0.501031\nEpoch [214/800] Average Loss: 5.3310\n\nEpoch [215/800], Batch [0/98], Loss: 5.3595, LR: 0.500155\nEpoch [215/800], Batch [10/98], Loss: 5.3498, LR: 0.500155\nEpoch [215/800], Batch [20/98], Loss: 5.3407, LR: 0.500155\nEpoch [215/800], Batch [30/98], Loss: 5.3447, LR: 0.500155\nEpoch [215/800], Batch [40/98], Loss: 5.3180, LR: 0.500155\nEpoch [215/800], Batch [50/98], Loss: 5.3224, LR: 0.500155\nEpoch [215/800], Batch [60/98], Loss: 5.3043, LR: 0.500155\nEpoch [215/800], Batch [70/98], Loss: 5.3392, LR: 0.500155\nEpoch [215/800], Batch [80/98], Loss: 5.3077, LR: 0.500155\nEpoch [215/800], Batch [90/98], Loss: 5.3357, LR: 0.500155\nEpoch [215/800] Average Loss: 5.3300\n\nEpoch [216/800], Batch [0/98], Loss: 5.3194, LR: 0.499276\nEpoch [216/800], Batch [10/98], Loss: 5.3421, LR: 0.499276\nEpoch [216/800], Batch [20/98], Loss: 5.3130, LR: 0.499276\nEpoch [216/800], Batch [30/98], Loss: 5.3560, LR: 0.499276\nEpoch [216/800], Batch [40/98], Loss: 5.3309, LR: 0.499276\nEpoch [216/800], Batch [50/98], Loss: 5.3462, LR: 0.499276\nEpoch [216/800], Batch [60/98], Loss: 5.3197, LR: 0.499276\nEpoch [216/800], Batch [70/98], Loss: 5.3460, LR: 0.499276\nEpoch [216/800], Batch [80/98], Loss: 5.3303, LR: 0.499276\nEpoch [216/800], Batch [90/98], Loss: 5.3272, LR: 0.499276\nEpoch [216/800] Average Loss: 5.3281\n\nEpoch [217/800], Batch [0/98], Loss: 5.3305, LR: 0.498394\nEpoch [217/800], Batch [10/98], Loss: 5.3286, LR: 0.498394\nEpoch [217/800], Batch [20/98], Loss: 5.3281, LR: 0.498394\nEpoch [217/800], Batch [30/98], Loss: 5.3226, LR: 0.498394\nEpoch [217/800], Batch [40/98], Loss: 5.3456, LR: 0.498394\nEpoch [217/800], Batch [50/98], Loss: 5.3288, LR: 0.498394\nEpoch [217/800], Batch [60/98], Loss: 5.3367, LR: 0.498394\nEpoch [217/800], Batch [70/98], Loss: 5.3166, LR: 0.498394\nEpoch [217/800], Batch [80/98], Loss: 5.3430, LR: 0.498394\nEpoch [217/800], Batch [90/98], Loss: 5.3239, LR: 0.498394\nEpoch [217/800] Average Loss: 5.3286\n\nEpoch [218/800], Batch [0/98], Loss: 5.3315, LR: 0.497508\nEpoch [218/800], Batch [10/98], Loss: 5.3222, LR: 0.497508\nEpoch [218/800], Batch [20/98], Loss: 5.3434, LR: 0.497508\nEpoch [218/800], Batch [30/98], Loss: 5.3111, LR: 0.497508\nEpoch [218/800], Batch [40/98], Loss: 5.3151, LR: 0.497508\nEpoch [218/800], Batch [50/98], Loss: 5.3408, LR: 0.497508\nEpoch [218/800], Batch [60/98], Loss: 5.3180, LR: 0.497508\nEpoch [218/800], Batch [70/98], Loss: 5.3351, LR: 0.497508\nEpoch [218/800], Batch [80/98], Loss: 5.3456, LR: 0.497508\nEpoch [218/800], Batch [90/98], Loss: 5.3346, LR: 0.497508\nEpoch [218/800] Average Loss: 5.3272\n\nEpoch [219/800], Batch [0/98], Loss: 5.3147, LR: 0.496620\nEpoch [219/800], Batch [10/98], Loss: 5.3847, LR: 0.496620\nEpoch [219/800], Batch [20/98], Loss: 5.3255, LR: 0.496620\nEpoch [219/800], Batch [30/98], Loss: 5.3428, LR: 0.496620\nEpoch [219/800], Batch [40/98], Loss: 5.3490, LR: 0.496620\nEpoch [219/800], Batch [50/98], Loss: 5.3326, LR: 0.496620\nEpoch [219/800], Batch [60/98], Loss: 5.3645, LR: 0.496620\nEpoch [219/800], Batch [70/98], Loss: 5.3131, LR: 0.496620\nEpoch [219/800], Batch [80/98], Loss: 5.3387, LR: 0.496620\nEpoch [219/800], Batch [90/98], Loss: 5.3042, LR: 0.496620\nEpoch [219/800] Average Loss: 5.3275\n\nEpoch [220/800], Batch [0/98], Loss: 5.3232, LR: 0.495729\nEpoch [220/800], Batch [10/98], Loss: 5.3399, LR: 0.495729\nEpoch [220/800], Batch [20/98], Loss: 5.3358, LR: 0.495729\nEpoch [220/800], Batch [30/98], Loss: 5.3213, LR: 0.495729\nEpoch [220/800], Batch [40/98], Loss: 5.3307, LR: 0.495729\nEpoch [220/800], Batch [50/98], Loss: 5.3402, LR: 0.495729\nEpoch [220/800], Batch [60/98], Loss: 5.3505, LR: 0.495729\nEpoch [220/800], Batch [70/98], Loss: 5.3167, LR: 0.495729\nEpoch [220/800], Batch [80/98], Loss: 5.3059, LR: 0.495729\nEpoch [220/800], Batch [90/98], Loss: 5.3346, LR: 0.495729\nEpoch [220/800] Average Loss: 5.3250\n\nEpoch [221/800], Batch [0/98], Loss: 5.3377, LR: 0.494834\nEpoch [221/800], Batch [10/98], Loss: 5.2956, LR: 0.494834\nEpoch [221/800], Batch [20/98], Loss: 5.3321, LR: 0.494834\nEpoch [221/800], Batch [30/98], Loss: 5.3241, LR: 0.494834\nEpoch [221/800], Batch [40/98], Loss: 5.3278, LR: 0.494834\nEpoch [221/800], Batch [50/98], Loss: 5.3046, LR: 0.494834\nEpoch [221/800], Batch [60/98], Loss: 5.3175, LR: 0.494834\nEpoch [221/800], Batch [70/98], Loss: 5.3140, LR: 0.494834\nEpoch [221/800], Batch [80/98], Loss: 5.3514, LR: 0.494834\nEpoch [221/800], Batch [90/98], Loss: 5.3176, LR: 0.494834\nEpoch [221/800] Average Loss: 5.3241\n\nEpoch [222/800], Batch [0/98], Loss: 5.3311, LR: 0.493937\nEpoch [222/800], Batch [10/98], Loss: 5.3210, LR: 0.493937\nEpoch [222/800], Batch [20/98], Loss: 5.3551, LR: 0.493937\nEpoch [222/800], Batch [30/98], Loss: 5.3309, LR: 0.493937\nEpoch [222/800], Batch [40/98], Loss: 5.3303, LR: 0.493937\nEpoch [222/800], Batch [50/98], Loss: 5.3072, LR: 0.493937\nEpoch [222/800], Batch [60/98], Loss: 5.3381, LR: 0.493937\nEpoch [222/800], Batch [70/98], Loss: 5.3202, LR: 0.493937\nEpoch [222/800], Batch [80/98], Loss: 5.2988, LR: 0.493937\nEpoch [222/800], Batch [90/98], Loss: 5.3057, LR: 0.493937\nEpoch [222/800] Average Loss: 5.3276\n\nEpoch [223/800], Batch [0/98], Loss: 5.3189, LR: 0.493037\nEpoch [223/800], Batch [10/98], Loss: 5.3098, LR: 0.493037\nEpoch [223/800], Batch [20/98], Loss: 5.3421, LR: 0.493037\nEpoch [223/800], Batch [30/98], Loss: 5.3057, LR: 0.493037\nEpoch [223/800], Batch [40/98], Loss: 5.3529, LR: 0.493037\nEpoch [223/800], Batch [50/98], Loss: 5.3362, LR: 0.493037\nEpoch [223/800], Batch [60/98], Loss: 5.3371, LR: 0.493037\nEpoch [223/800], Batch [70/98], Loss: 5.3193, LR: 0.493037\nEpoch [223/800], Batch [80/98], Loss: 5.3150, LR: 0.493037\nEpoch [223/800], Batch [90/98], Loss: 5.3470, LR: 0.493037\nEpoch [223/800] Average Loss: 5.3289\n\nEpoch [224/800], Batch [0/98], Loss: 5.3163, LR: 0.492133\nEpoch [224/800], Batch [10/98], Loss: 5.3159, LR: 0.492133\nEpoch [224/800], Batch [20/98], Loss: 5.3464, LR: 0.492133\nEpoch [224/800], Batch [30/98], Loss: 5.3659, LR: 0.492133\nEpoch [224/800], Batch [40/98], Loss: 5.3289, LR: 0.492133\nEpoch [224/800], Batch [50/98], Loss: 5.3151, LR: 0.492133\nEpoch [224/800], Batch [60/98], Loss: 5.3244, LR: 0.492133\nEpoch [224/800], Batch [70/98], Loss: 5.3221, LR: 0.492133\nEpoch [224/800], Batch [80/98], Loss: 5.3662, LR: 0.492133\nEpoch [224/800], Batch [90/98], Loss: 5.3292, LR: 0.492133\nEpoch [224/800] Average Loss: 5.3283\n\nEpoch [225/800], Batch [0/98], Loss: 5.3158, LR: 0.491227\nEpoch [225/800], Batch [10/98], Loss: 5.3269, LR: 0.491227\nEpoch [225/800], Batch [20/98], Loss: 5.3454, LR: 0.491227\nEpoch [225/800], Batch [30/98], Loss: 5.3352, LR: 0.491227\nEpoch [225/800], Batch [40/98], Loss: 5.3458, LR: 0.491227\nEpoch [225/800], Batch [50/98], Loss: 5.3502, LR: 0.491227\nEpoch [225/800], Batch [60/98], Loss: 5.2962, LR: 0.491227\nEpoch [225/800], Batch [70/98], Loss: 5.3356, LR: 0.491227\nEpoch [225/800], Batch [80/98], Loss: 5.3159, LR: 0.491227\nEpoch [225/800], Batch [90/98], Loss: 5.3442, LR: 0.491227\nEpoch [225/800] Average Loss: 5.3270\n\nEpoch [226/800], Batch [0/98], Loss: 5.3220, LR: 0.490318\nEpoch [226/800], Batch [10/98], Loss: 5.3071, LR: 0.490318\nEpoch [226/800], Batch [20/98], Loss: 5.3223, LR: 0.490318\nEpoch [226/800], Batch [30/98], Loss: 5.3404, LR: 0.490318\nEpoch [226/800], Batch [40/98], Loss: 5.3359, LR: 0.490318\nEpoch [226/800], Batch [50/98], Loss: 5.3849, LR: 0.490318\nEpoch [226/800], Batch [60/98], Loss: 5.3378, LR: 0.490318\nEpoch [226/800], Batch [70/98], Loss: 5.3386, LR: 0.490318\nEpoch [226/800], Batch [80/98], Loss: 5.3270, LR: 0.490318\nEpoch [226/800], Batch [90/98], Loss: 5.3463, LR: 0.490318\nEpoch [226/800] Average Loss: 5.3258\n\nEpoch [227/800], Batch [0/98], Loss: 5.3224, LR: 0.489406\nEpoch [227/800], Batch [10/98], Loss: 5.3528, LR: 0.489406\nEpoch [227/800], Batch [20/98], Loss: 5.3148, LR: 0.489406\nEpoch [227/800], Batch [30/98], Loss: 5.3472, LR: 0.489406\nEpoch [227/800], Batch [40/98], Loss: 5.3245, LR: 0.489406\nEpoch [227/800], Batch [50/98], Loss: 5.3264, LR: 0.489406\nEpoch [227/800], Batch [60/98], Loss: 5.3369, LR: 0.489406\nEpoch [227/800], Batch [70/98], Loss: 5.3525, LR: 0.489406\nEpoch [227/800], Batch [80/98], Loss: 5.3480, LR: 0.489406\nEpoch [227/800], Batch [90/98], Loss: 5.3206, LR: 0.489406\nEpoch [227/800] Average Loss: 5.3250\n\nEpoch [228/800], Batch [0/98], Loss: 5.3276, LR: 0.488491\nEpoch [228/800], Batch [10/98], Loss: 5.3196, LR: 0.488491\nEpoch [228/800], Batch [20/98], Loss: 5.3442, LR: 0.488491\nEpoch [228/800], Batch [30/98], Loss: 5.3532, LR: 0.488491\nEpoch [228/800], Batch [40/98], Loss: 5.3524, LR: 0.488491\nEpoch [228/800], Batch [50/98], Loss: 5.3270, LR: 0.488491\nEpoch [228/800], Batch [60/98], Loss: 5.3406, LR: 0.488491\nEpoch [228/800], Batch [70/98], Loss: 5.3337, LR: 0.488491\nEpoch [228/800], Batch [80/98], Loss: 5.3117, LR: 0.488491\nEpoch [228/800], Batch [90/98], Loss: 5.3583, LR: 0.488491\nEpoch [228/800] Average Loss: 5.3262\n\nEpoch [229/800], Batch [0/98], Loss: 5.3112, LR: 0.487573\nEpoch [229/800], Batch [10/98], Loss: 5.3299, LR: 0.487573\nEpoch [229/800], Batch [20/98], Loss: 5.3169, LR: 0.487573\nEpoch [229/800], Batch [30/98], Loss: 5.3381, LR: 0.487573\nEpoch [229/800], Batch [40/98], Loss: 5.3069, LR: 0.487573\nEpoch [229/800], Batch [50/98], Loss: 5.2981, LR: 0.487573\nEpoch [229/800], Batch [60/98], Loss: 5.3227, LR: 0.487573\nEpoch [229/800], Batch [70/98], Loss: 5.3474, LR: 0.487573\nEpoch [229/800], Batch [80/98], Loss: 5.3283, LR: 0.487573\nEpoch [229/800], Batch [90/98], Loss: 5.3267, LR: 0.487573\nEpoch [229/800] Average Loss: 5.3261\n\nEpoch [230/800], Batch [0/98], Loss: 5.3372, LR: 0.486652\nEpoch [230/800], Batch [10/98], Loss: 5.3232, LR: 0.486652\nEpoch [230/800], Batch [20/98], Loss: 5.3396, LR: 0.486652\nEpoch [230/800], Batch [30/98], Loss: 5.3229, LR: 0.486652\nEpoch [230/800], Batch [40/98], Loss: 5.3523, LR: 0.486652\nEpoch [230/800], Batch [50/98], Loss: 5.3249, LR: 0.486652\nEpoch [230/800], Batch [60/98], Loss: 5.3159, LR: 0.486652\nEpoch [230/800], Batch [70/98], Loss: 5.3227, LR: 0.486652\nEpoch [230/800], Batch [80/98], Loss: 5.3452, LR: 0.486652\nEpoch [230/800], Batch [90/98], Loss: 5.3096, LR: 0.486652\nEpoch [230/800] Average Loss: 5.3221\n\nEpoch [231/800], Batch [0/98], Loss: 5.3351, LR: 0.485728\nEpoch [231/800], Batch [10/98], Loss: 5.3617, LR: 0.485728\nEpoch [231/800], Batch [20/98], Loss: 5.3112, LR: 0.485728\nEpoch [231/800], Batch [30/98], Loss: 5.3240, LR: 0.485728\nEpoch [231/800], Batch [40/98], Loss: 5.3078, LR: 0.485728\nEpoch [231/800], Batch [50/98], Loss: 5.3329, LR: 0.485728\nEpoch [231/800], Batch [60/98], Loss: 5.3362, LR: 0.485728\nEpoch [231/800], Batch [70/98], Loss: 5.3414, LR: 0.485728\nEpoch [231/800], Batch [80/98], Loss: 5.3308, LR: 0.485728\nEpoch [231/800], Batch [90/98], Loss: 5.3144, LR: 0.485728\nEpoch [231/800] Average Loss: 5.3221\n\nEpoch [232/800], Batch [0/98], Loss: 5.3490, LR: 0.484802\nEpoch [232/800], Batch [10/98], Loss: 5.3353, LR: 0.484802\nEpoch [232/800], Batch [20/98], Loss: 5.3392, LR: 0.484802\nEpoch [232/800], Batch [30/98], Loss: 5.3272, LR: 0.484802\nEpoch [232/800], Batch [40/98], Loss: 5.3163, LR: 0.484802\nEpoch [232/800], Batch [50/98], Loss: 5.3182, LR: 0.484802\nEpoch [232/800], Batch [60/98], Loss: 5.3314, LR: 0.484802\nEpoch [232/800], Batch [70/98], Loss: 5.3609, LR: 0.484802\nEpoch [232/800], Batch [80/98], Loss: 5.3481, LR: 0.484802\nEpoch [232/800], Batch [90/98], Loss: 5.3124, LR: 0.484802\nEpoch [232/800] Average Loss: 5.3244\n\nEpoch [233/800], Batch [0/98], Loss: 5.3356, LR: 0.483872\nEpoch [233/800], Batch [10/98], Loss: 5.3326, LR: 0.483872\nEpoch [233/800], Batch [20/98], Loss: 5.3221, LR: 0.483872\nEpoch [233/800], Batch [30/98], Loss: 5.3378, LR: 0.483872\nEpoch [233/800], Batch [40/98], Loss: 5.3272, LR: 0.483872\nEpoch [233/800], Batch [50/98], Loss: 5.3374, LR: 0.483872\nEpoch [233/800], Batch [60/98], Loss: 5.3433, LR: 0.483872\nEpoch [233/800], Batch [70/98], Loss: 5.3105, LR: 0.483872\nEpoch [233/800], Batch [80/98], Loss: 5.3246, LR: 0.483872\nEpoch [233/800], Batch [90/98], Loss: 5.3292, LR: 0.483872\nEpoch [233/800] Average Loss: 5.3241\n\nEpoch [234/800], Batch [0/98], Loss: 5.3507, LR: 0.482940\nEpoch [234/800], Batch [10/98], Loss: 5.3305, LR: 0.482940\nEpoch [234/800], Batch [20/98], Loss: 5.3190, LR: 0.482940\nEpoch [234/800], Batch [30/98], Loss: 5.3246, LR: 0.482940\nEpoch [234/800], Batch [40/98], Loss: 5.3256, LR: 0.482940\nEpoch [234/800], Batch [50/98], Loss: 5.3477, LR: 0.482940\nEpoch [234/800], Batch [60/98], Loss: 5.3089, LR: 0.482940\nEpoch [234/800], Batch [70/98], Loss: 5.3203, LR: 0.482940\nEpoch [234/800], Batch [80/98], Loss: 5.3421, LR: 0.482940\nEpoch [234/800], Batch [90/98], Loss: 5.3267, LR: 0.482940\nEpoch [234/800] Average Loss: 5.3212\n\nEpoch [235/800], Batch [0/98], Loss: 5.2945, LR: 0.482005\nEpoch [235/800], Batch [10/98], Loss: 5.3024, LR: 0.482005\nEpoch [235/800], Batch [20/98], Loss: 5.3416, LR: 0.482005\nEpoch [235/800], Batch [30/98], Loss: 5.3203, LR: 0.482005\nEpoch [235/800], Batch [40/98], Loss: 5.3147, LR: 0.482005\nEpoch [235/800], Batch [50/98], Loss: 5.3249, LR: 0.482005\nEpoch [235/800], Batch [60/98], Loss: 5.3194, LR: 0.482005\nEpoch [235/800], Batch [70/98], Loss: 5.3190, LR: 0.482005\nEpoch [235/800], Batch [80/98], Loss: 5.3179, LR: 0.482005\nEpoch [235/800], Batch [90/98], Loss: 5.3356, LR: 0.482005\nEpoch [235/800] Average Loss: 5.3225\n\nEpoch [236/800], Batch [0/98], Loss: 5.3251, LR: 0.481067\nEpoch [236/800], Batch [10/98], Loss: 5.3495, LR: 0.481067\nEpoch [236/800], Batch [20/98], Loss: 5.3236, LR: 0.481067\nEpoch [236/800], Batch [30/98], Loss: 5.3204, LR: 0.481067\nEpoch [236/800], Batch [40/98], Loss: 5.3260, LR: 0.481067\nEpoch [236/800], Batch [50/98], Loss: 5.3187, LR: 0.481067\nEpoch [236/800], Batch [60/98], Loss: 5.3148, LR: 0.481067\nEpoch [236/800], Batch [70/98], Loss: 5.3141, LR: 0.481067\nEpoch [236/800], Batch [80/98], Loss: 5.3576, LR: 0.481067\nEpoch [236/800], Batch [90/98], Loss: 5.3181, LR: 0.481067\nEpoch [236/800] Average Loss: 5.3221\n\nEpoch [237/800], Batch [0/98], Loss: 5.3053, LR: 0.480126\nEpoch [237/800], Batch [10/98], Loss: 5.3380, LR: 0.480126\nEpoch [237/800], Batch [20/98], Loss: 5.3143, LR: 0.480126\nEpoch [237/800], Batch [30/98], Loss: 5.3083, LR: 0.480126\nEpoch [237/800], Batch [40/98], Loss: 5.3304, LR: 0.480126\nEpoch [237/800], Batch [50/98], Loss: 5.3242, LR: 0.480126\nEpoch [237/800], Batch [60/98], Loss: 5.3253, LR: 0.480126\nEpoch [237/800], Batch [70/98], Loss: 5.3363, LR: 0.480126\nEpoch [237/800], Batch [80/98], Loss: 5.3305, LR: 0.480126\nEpoch [237/800], Batch [90/98], Loss: 5.3327, LR: 0.480126\nEpoch [237/800] Average Loss: 5.3213\n\nEpoch [238/800], Batch [0/98], Loss: 5.2916, LR: 0.479183\nEpoch [238/800], Batch [10/98], Loss: 5.3256, LR: 0.479183\nEpoch [238/800], Batch [20/98], Loss: 5.3316, LR: 0.479183\nEpoch [238/800], Batch [30/98], Loss: 5.3250, LR: 0.479183\nEpoch [238/800], Batch [40/98], Loss: 5.3128, LR: 0.479183\nEpoch [238/800], Batch [50/98], Loss: 5.3126, LR: 0.479183\nEpoch [238/800], Batch [60/98], Loss: 5.3220, LR: 0.479183\nEpoch [238/800], Batch [70/98], Loss: 5.3096, LR: 0.479183\nEpoch [238/800], Batch [80/98], Loss: 5.3600, LR: 0.479183\nEpoch [238/800], Batch [90/98], Loss: 5.3320, LR: 0.479183\nEpoch [238/800] Average Loss: 5.3220\n\nEpoch [239/800], Batch [0/98], Loss: 5.3299, LR: 0.478236\nEpoch [239/800], Batch [10/98], Loss: 5.3401, LR: 0.478236\nEpoch [239/800], Batch [20/98], Loss: 5.3204, LR: 0.478236\nEpoch [239/800], Batch [30/98], Loss: 5.3453, LR: 0.478236\nEpoch [239/800], Batch [40/98], Loss: 5.3250, LR: 0.478236\nEpoch [239/800], Batch [50/98], Loss: 5.3282, LR: 0.478236\nEpoch [239/800], Batch [60/98], Loss: 5.3409, LR: 0.478236\nEpoch [239/800], Batch [70/98], Loss: 5.3197, LR: 0.478236\nEpoch [239/800], Batch [80/98], Loss: 5.3475, LR: 0.478236\nEpoch [239/800], Batch [90/98], Loss: 5.3211, LR: 0.478236\nEpoch [239/800] Average Loss: 5.3197\n\nEpoch [240/800], Batch [0/98], Loss: 5.3221, LR: 0.477287\nEpoch [240/800], Batch [10/98], Loss: 5.3137, LR: 0.477287\nEpoch [240/800], Batch [20/98], Loss: 5.3405, LR: 0.477287\nEpoch [240/800], Batch [30/98], Loss: 5.3088, LR: 0.477287\nEpoch [240/800], Batch [40/98], Loss: 5.3146, LR: 0.477287\nEpoch [240/800], Batch [50/98], Loss: 5.3128, LR: 0.477287\nEpoch [240/800], Batch [60/98], Loss: 5.3253, LR: 0.477287\nEpoch [240/800], Batch [70/98], Loss: 5.3304, LR: 0.477287\nEpoch [240/800], Batch [80/98], Loss: 5.3293, LR: 0.477287\nEpoch [240/800], Batch [90/98], Loss: 5.3270, LR: 0.477287\nEpoch [240/800] Average Loss: 5.3215\n\nEpoch [241/800], Batch [0/98], Loss: 5.3064, LR: 0.476336\nEpoch [241/800], Batch [10/98], Loss: 5.3225, LR: 0.476336\nEpoch [241/800], Batch [20/98], Loss: 5.3238, LR: 0.476336\nEpoch [241/800], Batch [30/98], Loss: 5.3338, LR: 0.476336\nEpoch [241/800], Batch [40/98], Loss: 5.3396, LR: 0.476336\nEpoch [241/800], Batch [50/98], Loss: 5.3274, LR: 0.476336\nEpoch [241/800], Batch [60/98], Loss: 5.3443, LR: 0.476336\nEpoch [241/800], Batch [70/98], Loss: 5.2918, LR: 0.476336\nEpoch [241/800], Batch [80/98], Loss: 5.3123, LR: 0.476336\nEpoch [241/800], Batch [90/98], Loss: 5.3169, LR: 0.476336\nEpoch [241/800] Average Loss: 5.3211\n\nEpoch [242/800], Batch [0/98], Loss: 5.3220, LR: 0.475381\nEpoch [242/800], Batch [10/98], Loss: 5.3426, LR: 0.475381\nEpoch [242/800], Batch [20/98], Loss: 5.3319, LR: 0.475381\nEpoch [242/800], Batch [30/98], Loss: 5.3065, LR: 0.475381\nEpoch [242/800], Batch [40/98], Loss: 5.3343, LR: 0.475381\nEpoch [242/800], Batch [50/98], Loss: 5.3025, LR: 0.475381\nEpoch [242/800], Batch [60/98], Loss: 5.3108, LR: 0.475381\nEpoch [242/800], Batch [70/98], Loss: 5.3075, LR: 0.475381\nEpoch [242/800], Batch [80/98], Loss: 5.3238, LR: 0.475381\nEpoch [242/800], Batch [90/98], Loss: 5.3080, LR: 0.475381\nEpoch [242/800] Average Loss: 5.3193\n\nEpoch [243/800], Batch [0/98], Loss: 5.3397, LR: 0.474424\nEpoch [243/800], Batch [10/98], Loss: 5.3504, LR: 0.474424\nEpoch [243/800], Batch [20/98], Loss: 5.3168, LR: 0.474424\nEpoch [243/800], Batch [30/98], Loss: 5.3052, LR: 0.474424\nEpoch [243/800], Batch [40/98], Loss: 5.3481, LR: 0.474424\nEpoch [243/800], Batch [50/98], Loss: 5.3247, LR: 0.474424\nEpoch [243/800], Batch [60/98], Loss: 5.3362, LR: 0.474424\nEpoch [243/800], Batch [70/98], Loss: 5.3132, LR: 0.474424\nEpoch [243/800], Batch [80/98], Loss: 5.3076, LR: 0.474424\nEpoch [243/800], Batch [90/98], Loss: 5.3109, LR: 0.474424\nEpoch [243/800] Average Loss: 5.3190\n\nEpoch [244/800], Batch [0/98], Loss: 5.3086, LR: 0.473464\nEpoch [244/800], Batch [10/98], Loss: 5.3305, LR: 0.473464\nEpoch [244/800], Batch [20/98], Loss: 5.2950, LR: 0.473464\nEpoch [244/800], Batch [30/98], Loss: 5.3348, LR: 0.473464\nEpoch [244/800], Batch [40/98], Loss: 5.3022, LR: 0.473464\nEpoch [244/800], Batch [50/98], Loss: 5.3260, LR: 0.473464\nEpoch [244/800], Batch [60/98], Loss: 5.3123, LR: 0.473464\nEpoch [244/800], Batch [70/98], Loss: 5.3305, LR: 0.473464\nEpoch [244/800], Batch [80/98], Loss: 5.3402, LR: 0.473464\nEpoch [244/800], Batch [90/98], Loss: 5.3226, LR: 0.473464\nEpoch [244/800] Average Loss: 5.3188\n\nEpoch [245/800], Batch [0/98], Loss: 5.3113, LR: 0.472502\nEpoch [245/800], Batch [10/98], Loss: 5.3250, LR: 0.472502\nEpoch [245/800], Batch [20/98], Loss: 5.3119, LR: 0.472502\nEpoch [245/800], Batch [30/98], Loss: 5.3358, LR: 0.472502\nEpoch [245/800], Batch [40/98], Loss: 5.3024, LR: 0.472502\nEpoch [245/800], Batch [50/98], Loss: 5.3133, LR: 0.472502\nEpoch [245/800], Batch [60/98], Loss: 5.3102, LR: 0.472502\nEpoch [245/800], Batch [70/98], Loss: 5.3348, LR: 0.472502\nEpoch [245/800], Batch [80/98], Loss: 5.3375, LR: 0.472502\nEpoch [245/800], Batch [90/98], Loss: 5.3290, LR: 0.472502\nEpoch [245/800] Average Loss: 5.3189\n\nEpoch [246/800], Batch [0/98], Loss: 5.3195, LR: 0.471536\nEpoch [246/800], Batch [10/98], Loss: 5.3149, LR: 0.471536\nEpoch [246/800], Batch [20/98], Loss: 5.3487, LR: 0.471536\nEpoch [246/800], Batch [30/98], Loss: 5.3357, LR: 0.471536\nEpoch [246/800], Batch [40/98], Loss: 5.3285, LR: 0.471536\nEpoch [246/800], Batch [50/98], Loss: 5.3025, LR: 0.471536\nEpoch [246/800], Batch [60/98], Loss: 5.3502, LR: 0.471536\nEpoch [246/800], Batch [70/98], Loss: 5.3335, LR: 0.471536\nEpoch [246/800], Batch [80/98], Loss: 5.3287, LR: 0.471536\nEpoch [246/800], Batch [90/98], Loss: 5.3237, LR: 0.471536\nEpoch [246/800] Average Loss: 5.3203\n\nEpoch [247/800], Batch [0/98], Loss: 5.3592, LR: 0.470569\nEpoch [247/800], Batch [10/98], Loss: 5.3322, LR: 0.470569\nEpoch [247/800], Batch [20/98], Loss: 5.3234, LR: 0.470569\nEpoch [247/800], Batch [30/98], Loss: 5.3273, LR: 0.470569\nEpoch [247/800], Batch [40/98], Loss: 5.3061, LR: 0.470569\nEpoch [247/800], Batch [50/98], Loss: 5.3336, LR: 0.470569\nEpoch [247/800], Batch [60/98], Loss: 5.3336, LR: 0.470569\nEpoch [247/800], Batch [70/98], Loss: 5.3053, LR: 0.470569\nEpoch [247/800], Batch [80/98], Loss: 5.3102, LR: 0.470569\nEpoch [247/800], Batch [90/98], Loss: 5.2946, LR: 0.470569\nEpoch [247/800] Average Loss: 5.3205\n\nEpoch [248/800], Batch [0/98], Loss: 5.3111, LR: 0.469598\nEpoch [248/800], Batch [10/98], Loss: 5.3284, LR: 0.469598\nEpoch [248/800], Batch [20/98], Loss: 5.3465, LR: 0.469598\nEpoch [248/800], Batch [30/98], Loss: 5.3113, LR: 0.469598\nEpoch [248/800], Batch [40/98], Loss: 5.3228, LR: 0.469598\nEpoch [248/800], Batch [50/98], Loss: 5.3217, LR: 0.469598\nEpoch [248/800], Batch [60/98], Loss: 5.3128, LR: 0.469598\nEpoch [248/800], Batch [70/98], Loss: 5.3285, LR: 0.469598\nEpoch [248/800], Batch [80/98], Loss: 5.3352, LR: 0.469598\nEpoch [248/800], Batch [90/98], Loss: 5.3060, LR: 0.469598\nEpoch [248/800] Average Loss: 5.3180\n\nEpoch [249/800], Batch [0/98], Loss: 5.3016, LR: 0.468625\nEpoch [249/800], Batch [10/98], Loss: 5.3240, LR: 0.468625\nEpoch [249/800], Batch [20/98], Loss: 5.3624, LR: 0.468625\nEpoch [249/800], Batch [30/98], Loss: 5.3080, LR: 0.468625\nEpoch [249/800], Batch [40/98], Loss: 5.3209, LR: 0.468625\nEpoch [249/800], Batch [50/98], Loss: 5.2987, LR: 0.468625\nEpoch [249/800], Batch [60/98], Loss: 5.3193, LR: 0.468625\nEpoch [249/800], Batch [70/98], Loss: 5.3623, LR: 0.468625\nEpoch [249/800], Batch [80/98], Loss: 5.3113, LR: 0.468625\nEpoch [249/800], Batch [90/98], Loss: 5.3348, LR: 0.468625\nEpoch [249/800] Average Loss: 5.3194\n\nEpoch [250/800], Batch [0/98], Loss: 5.2970, LR: 0.467649\nEpoch [250/800], Batch [10/98], Loss: 5.3323, LR: 0.467649\nEpoch [250/800], Batch [20/98], Loss: 5.3216, LR: 0.467649\nEpoch [250/800], Batch [30/98], Loss: 5.3249, LR: 0.467649\nEpoch [250/800], Batch [40/98], Loss: 5.3246, LR: 0.467649\nEpoch [250/800], Batch [50/98], Loss: 5.3139, LR: 0.467649\nEpoch [250/800], Batch [60/98], Loss: 5.3353, LR: 0.467649\nEpoch [250/800], Batch [70/98], Loss: 5.3133, LR: 0.467649\nEpoch [250/800], Batch [80/98], Loss: 5.3278, LR: 0.467649\nEpoch [250/800], Batch [90/98], Loss: 5.3368, LR: 0.467649\nEpoch [250/800] Average Loss: 5.3149\n\nEpoch [251/800], Batch [0/98], Loss: 5.3253, LR: 0.466671\nEpoch [251/800], Batch [10/98], Loss: 5.3316, LR: 0.466671\nEpoch [251/800], Batch [20/98], Loss: 5.3364, LR: 0.466671\nEpoch [251/800], Batch [30/98], Loss: 5.3202, LR: 0.466671\nEpoch [251/800], Batch [40/98], Loss: 5.3014, LR: 0.466671\nEpoch [251/800], Batch [50/98], Loss: 5.3252, LR: 0.466671\nEpoch [251/800], Batch [60/98], Loss: 5.3631, LR: 0.466671\nEpoch [251/800], Batch [70/98], Loss: 5.3118, LR: 0.466671\nEpoch [251/800], Batch [80/98], Loss: 5.3072, LR: 0.466671\nEpoch [251/800], Batch [90/98], Loss: 5.3447, LR: 0.466671\nEpoch [251/800] Average Loss: 5.3188\n\nEpoch [252/800], Batch [0/98], Loss: 5.2875, LR: 0.465690\nEpoch [252/800], Batch [10/98], Loss: 5.3274, LR: 0.465690\nEpoch [252/800], Batch [20/98], Loss: 5.3110, LR: 0.465690\nEpoch [252/800], Batch [30/98], Loss: 5.3267, LR: 0.465690\nEpoch [252/800], Batch [40/98], Loss: 5.3468, LR: 0.465690\nEpoch [252/800], Batch [50/98], Loss: 5.3009, LR: 0.465690\nEpoch [252/800], Batch [60/98], Loss: 5.3029, LR: 0.465690\nEpoch [252/800], Batch [70/98], Loss: 5.3255, LR: 0.465690\nEpoch [252/800], Batch [80/98], Loss: 5.3194, LR: 0.465690\nEpoch [252/800], Batch [90/98], Loss: 5.3135, LR: 0.465690\nEpoch [252/800] Average Loss: 5.3189\n\nEpoch [253/800], Batch [0/98], Loss: 5.3083, LR: 0.464707\nEpoch [253/800], Batch [10/98], Loss: 5.3177, LR: 0.464707\nEpoch [253/800], Batch [20/98], Loss: 5.3256, LR: 0.464707\nEpoch [253/800], Batch [30/98], Loss: 5.3211, LR: 0.464707\nEpoch [253/800], Batch [40/98], Loss: 5.3295, LR: 0.464707\nEpoch [253/800], Batch [50/98], Loss: 5.3334, LR: 0.464707\nEpoch [253/800], Batch [60/98], Loss: 5.3231, LR: 0.464707\nEpoch [253/800], Batch [70/98], Loss: 5.3074, LR: 0.464707\nEpoch [253/800], Batch [80/98], Loss: 5.3169, LR: 0.464707\nEpoch [253/800], Batch [90/98], Loss: 5.3631, LR: 0.464707\nEpoch [253/800] Average Loss: 5.3185\n\nEpoch [254/800], Batch [0/98], Loss: 5.3187, LR: 0.463721\nEpoch [254/800], Batch [10/98], Loss: 5.3129, LR: 0.463721\nEpoch [254/800], Batch [20/98], Loss: 5.3078, LR: 0.463721\nEpoch [254/800], Batch [30/98], Loss: 5.3517, LR: 0.463721\nEpoch [254/800], Batch [40/98], Loss: 5.3344, LR: 0.463721\nEpoch [254/800], Batch [50/98], Loss: 5.3233, LR: 0.463721\nEpoch [254/800], Batch [60/98], Loss: 5.3236, LR: 0.463721\nEpoch [254/800], Batch [70/98], Loss: 5.3375, LR: 0.463721\nEpoch [254/800], Batch [80/98], Loss: 5.3341, LR: 0.463721\nEpoch [254/800], Batch [90/98], Loss: 5.3214, LR: 0.463721\nEpoch [254/800] Average Loss: 5.3166\n\nEpoch [255/800], Batch [0/98], Loss: 5.3300, LR: 0.462732\nEpoch [255/800], Batch [10/98], Loss: 5.3388, LR: 0.462732\nEpoch [255/800], Batch [20/98], Loss: 5.2975, LR: 0.462732\nEpoch [255/800], Batch [30/98], Loss: 5.3198, LR: 0.462732\nEpoch [255/800], Batch [40/98], Loss: 5.3186, LR: 0.462732\nEpoch [255/800], Batch [50/98], Loss: 5.3238, LR: 0.462732\nEpoch [255/800], Batch [60/98], Loss: 5.3232, LR: 0.462732\nEpoch [255/800], Batch [70/98], Loss: 5.3147, LR: 0.462732\nEpoch [255/800], Batch [80/98], Loss: 5.3182, LR: 0.462732\nEpoch [255/800], Batch [90/98], Loss: 5.3057, LR: 0.462732\nEpoch [255/800] Average Loss: 5.3165\n\nEpoch [256/800], Batch [0/98], Loss: 5.3152, LR: 0.461741\nEpoch [256/800], Batch [10/98], Loss: 5.3338, LR: 0.461741\nEpoch [256/800], Batch [20/98], Loss: 5.3043, LR: 0.461741\nEpoch [256/800], Batch [30/98], Loss: 5.3156, LR: 0.461741\nEpoch [256/800], Batch [40/98], Loss: 5.3323, LR: 0.461741\nEpoch [256/800], Batch [50/98], Loss: 5.3122, LR: 0.461741\nEpoch [256/800], Batch [60/98], Loss: 5.3344, LR: 0.461741\nEpoch [256/800], Batch [70/98], Loss: 5.3219, LR: 0.461741\nEpoch [256/800], Batch [80/98], Loss: 5.3027, LR: 0.461741\nEpoch [256/800], Batch [90/98], Loss: 5.3021, LR: 0.461741\nEpoch [256/800] Average Loss: 5.3144\n\nEpoch [257/800], Batch [0/98], Loss: 5.2955, LR: 0.460748\nEpoch [257/800], Batch [10/98], Loss: 5.3304, LR: 0.460748\nEpoch [257/800], Batch [20/98], Loss: 5.3088, LR: 0.460748\nEpoch [257/800], Batch [30/98], Loss: 5.3264, LR: 0.460748\nEpoch [257/800], Batch [40/98], Loss: 5.3183, LR: 0.460748\nEpoch [257/800], Batch [50/98], Loss: 5.3013, LR: 0.460748\nEpoch [257/800], Batch [60/98], Loss: 5.3460, LR: 0.460748\nEpoch [257/800], Batch [70/98], Loss: 5.2956, LR: 0.460748\nEpoch [257/800], Batch [80/98], Loss: 5.2936, LR: 0.460748\nEpoch [257/800], Batch [90/98], Loss: 5.3445, LR: 0.460748\nEpoch [257/800] Average Loss: 5.3159\n\nEpoch [258/800], Batch [0/98], Loss: 5.3148, LR: 0.459752\nEpoch [258/800], Batch [10/98], Loss: 5.3163, LR: 0.459752\nEpoch [258/800], Batch [20/98], Loss: 5.3411, LR: 0.459752\nEpoch [258/800], Batch [30/98], Loss: 5.3039, LR: 0.459752\nEpoch [258/800], Batch [40/98], Loss: 5.3200, LR: 0.459752\nEpoch [258/800], Batch [50/98], Loss: 5.3013, LR: 0.459752\nEpoch [258/800], Batch [60/98], Loss: 5.3058, LR: 0.459752\nEpoch [258/800], Batch [70/98], Loss: 5.3011, LR: 0.459752\nEpoch [258/800], Batch [80/98], Loss: 5.3233, LR: 0.459752\nEpoch [258/800], Batch [90/98], Loss: 5.3120, LR: 0.459752\nEpoch [258/800] Average Loss: 5.3150\n\nEpoch [259/800], Batch [0/98], Loss: 5.3042, LR: 0.458754\nEpoch [259/800], Batch [10/98], Loss: 5.3196, LR: 0.458754\nEpoch [259/800], Batch [20/98], Loss: 5.3215, LR: 0.458754\nEpoch [259/800], Batch [30/98], Loss: 5.3082, LR: 0.458754\nEpoch [259/800], Batch [40/98], Loss: 5.3415, LR: 0.458754\nEpoch [259/800], Batch [50/98], Loss: 5.3174, LR: 0.458754\nEpoch [259/800], Batch [60/98], Loss: 5.3031, LR: 0.458754\nEpoch [259/800], Batch [70/98], Loss: 5.3217, LR: 0.458754\nEpoch [259/800], Batch [80/98], Loss: 5.3369, LR: 0.458754\nEpoch [259/800], Batch [90/98], Loss: 5.3132, LR: 0.458754\nEpoch [259/800] Average Loss: 5.3189\n\nEpoch [260/800], Batch [0/98], Loss: 5.3187, LR: 0.457753\nEpoch [260/800], Batch [10/98], Loss: 5.2894, LR: 0.457753\nEpoch [260/800], Batch [20/98], Loss: 5.3192, LR: 0.457753\nEpoch [260/800], Batch [30/98], Loss: 5.3003, LR: 0.457753\nEpoch [260/800], Batch [40/98], Loss: 5.3080, LR: 0.457753\nEpoch [260/800], Batch [50/98], Loss: 5.3039, LR: 0.457753\nEpoch [260/800], Batch [60/98], Loss: 5.3118, LR: 0.457753\nEpoch [260/800], Batch [70/98], Loss: 5.3128, LR: 0.457753\nEpoch [260/800], Batch [80/98], Loss: 5.3034, LR: 0.457753\nEpoch [260/800], Batch [90/98], Loss: 5.3203, LR: 0.457753\nEpoch [260/800] Average Loss: 5.3142\n\nEpoch [261/800], Batch [0/98], Loss: 5.3144, LR: 0.456750\nEpoch [261/800], Batch [10/98], Loss: 5.3224, LR: 0.456750\nEpoch [261/800], Batch [20/98], Loss: 5.3077, LR: 0.456750\nEpoch [261/800], Batch [30/98], Loss: 5.2924, LR: 0.456750\nEpoch [261/800], Batch [40/98], Loss: 5.3100, LR: 0.456750\nEpoch [261/800], Batch [50/98], Loss: 5.3440, LR: 0.456750\nEpoch [261/800], Batch [60/98], Loss: 5.3412, LR: 0.456750\nEpoch [261/800], Batch [70/98], Loss: 5.3171, LR: 0.456750\nEpoch [261/800], Batch [80/98], Loss: 5.3050, LR: 0.456750\nEpoch [261/800], Batch [90/98], Loss: 5.3201, LR: 0.456750\nEpoch [261/800] Average Loss: 5.3129\n\nEpoch [262/800], Batch [0/98], Loss: 5.3059, LR: 0.455744\nEpoch [262/800], Batch [10/98], Loss: 5.3282, LR: 0.455744\nEpoch [262/800], Batch [20/98], Loss: 5.3438, LR: 0.455744\nEpoch [262/800], Batch [30/98], Loss: 5.3531, LR: 0.455744\nEpoch [262/800], Batch [40/98], Loss: 5.3278, LR: 0.455744\nEpoch [262/800], Batch [50/98], Loss: 5.3200, LR: 0.455744\nEpoch [262/800], Batch [60/98], Loss: 5.3085, LR: 0.455744\nEpoch [262/800], Batch [70/98], Loss: 5.3179, LR: 0.455744\nEpoch [262/800], Batch [80/98], Loss: 5.3552, LR: 0.455744\nEpoch [262/800], Batch [90/98], Loss: 5.3440, LR: 0.455744\nEpoch [262/800] Average Loss: 5.3169\n\nEpoch [263/800], Batch [0/98], Loss: 5.3039, LR: 0.454736\nEpoch [263/800], Batch [10/98], Loss: 5.3195, LR: 0.454736\nEpoch [263/800], Batch [20/98], Loss: 5.3021, LR: 0.454736\nEpoch [263/800], Batch [30/98], Loss: 5.3215, LR: 0.454736\nEpoch [263/800], Batch [40/98], Loss: 5.3206, LR: 0.454736\nEpoch [263/800], Batch [50/98], Loss: 5.3433, LR: 0.454736\nEpoch [263/800], Batch [60/98], Loss: 5.3032, LR: 0.454736\nEpoch [263/800], Batch [70/98], Loss: 5.3046, LR: 0.454736\nEpoch [263/800], Batch [80/98], Loss: 5.3136, LR: 0.454736\nEpoch [263/800], Batch [90/98], Loss: 5.3080, LR: 0.454736\nEpoch [263/800] Average Loss: 5.3155\n\nEpoch [264/800], Batch [0/98], Loss: 5.2922, LR: 0.453725\nEpoch [264/800], Batch [10/98], Loss: 5.3120, LR: 0.453725\nEpoch [264/800], Batch [20/98], Loss: 5.3282, LR: 0.453725\nEpoch [264/800], Batch [30/98], Loss: 5.3248, LR: 0.453725\nEpoch [264/800], Batch [40/98], Loss: 5.3226, LR: 0.453725\nEpoch [264/800], Batch [50/98], Loss: 5.3024, LR: 0.453725\nEpoch [264/800], Batch [60/98], Loss: 5.3058, LR: 0.453725\nEpoch [264/800], Batch [70/98], Loss: 5.3197, LR: 0.453725\nEpoch [264/800], Batch [80/98], Loss: 5.3062, LR: 0.453725\nEpoch [264/800], Batch [90/98], Loss: 5.3251, LR: 0.453725\nEpoch [264/800] Average Loss: 5.3143\n\nEpoch [265/800], Batch [0/98], Loss: 5.3236, LR: 0.452712\nEpoch [265/800], Batch [10/98], Loss: 5.3075, LR: 0.452712\nEpoch [265/800], Batch [20/98], Loss: 5.3215, LR: 0.452712\nEpoch [265/800], Batch [30/98], Loss: 5.3177, LR: 0.452712\nEpoch [265/800], Batch [40/98], Loss: 5.3159, LR: 0.452712\nEpoch [265/800], Batch [50/98], Loss: 5.3041, LR: 0.452712\nEpoch [265/800], Batch [60/98], Loss: 5.2942, LR: 0.452712\nEpoch [265/800], Batch [70/98], Loss: 5.3246, LR: 0.452712\nEpoch [265/800], Batch [80/98], Loss: 5.3039, LR: 0.452712\nEpoch [265/800], Batch [90/98], Loss: 5.2976, LR: 0.452712\nEpoch [265/800] Average Loss: 5.3115\n\nEpoch [266/800], Batch [0/98], Loss: 5.3097, LR: 0.451697\nEpoch [266/800], Batch [10/98], Loss: 5.3354, LR: 0.451697\nEpoch [266/800], Batch [20/98], Loss: 5.3022, LR: 0.451697\nEpoch [266/800], Batch [30/98], Loss: 5.2946, LR: 0.451697\nEpoch [266/800], Batch [40/98], Loss: 5.3150, LR: 0.451697\nEpoch [266/800], Batch [50/98], Loss: 5.3073, LR: 0.451697\nEpoch [266/800], Batch [60/98], Loss: 5.2983, LR: 0.451697\nEpoch [266/800], Batch [70/98], Loss: 5.3280, LR: 0.451697\nEpoch [266/800], Batch [80/98], Loss: 5.3497, LR: 0.451697\nEpoch [266/800], Batch [90/98], Loss: 5.3302, LR: 0.451697\nEpoch [266/800] Average Loss: 5.3128\n\nEpoch [267/800], Batch [0/98], Loss: 5.3100, LR: 0.450680\nEpoch [267/800], Batch [10/98], Loss: 5.3101, LR: 0.450680\nEpoch [267/800], Batch [20/98], Loss: 5.3074, LR: 0.450680\nEpoch [267/800], Batch [30/98], Loss: 5.3081, LR: 0.450680\nEpoch [267/800], Batch [40/98], Loss: 5.3242, LR: 0.450680\nEpoch [267/800], Batch [50/98], Loss: 5.3095, LR: 0.450680\nEpoch [267/800], Batch [60/98], Loss: 5.3250, LR: 0.450680\nEpoch [267/800], Batch [70/98], Loss: 5.2899, LR: 0.450680\nEpoch [267/800], Batch [80/98], Loss: 5.3044, LR: 0.450680\nEpoch [267/800], Batch [90/98], Loss: 5.3210, LR: 0.450680\nEpoch [267/800] Average Loss: 5.3119\n\nEpoch [268/800], Batch [0/98], Loss: 5.3263, LR: 0.449660\nEpoch [268/800], Batch [10/98], Loss: 5.3102, LR: 0.449660\nEpoch [268/800], Batch [20/98], Loss: 5.3387, LR: 0.449660\nEpoch [268/800], Batch [30/98], Loss: 5.2996, LR: 0.449660\nEpoch [268/800], Batch [40/98], Loss: 5.3128, LR: 0.449660\nEpoch [268/800], Batch [50/98], Loss: 5.3163, LR: 0.449660\nEpoch [268/800], Batch [60/98], Loss: 5.3510, LR: 0.449660\nEpoch [268/800], Batch [70/98], Loss: 5.3391, LR: 0.449660\nEpoch [268/800], Batch [80/98], Loss: 5.2901, LR: 0.449660\nEpoch [268/800], Batch [90/98], Loss: 5.3151, LR: 0.449660\nEpoch [268/800] Average Loss: 5.3125\n\nEpoch [269/800], Batch [0/98], Loss: 5.3099, LR: 0.448638\nEpoch [269/800], Batch [10/98], Loss: 5.3145, LR: 0.448638\nEpoch [269/800], Batch [20/98], Loss: 5.3157, LR: 0.448638\nEpoch [269/800], Batch [30/98], Loss: 5.3153, LR: 0.448638\nEpoch [269/800], Batch [40/98], Loss: 5.3218, LR: 0.448638\nEpoch [269/800], Batch [50/98], Loss: 5.3107, LR: 0.448638\nEpoch [269/800], Batch [60/98], Loss: 5.3194, LR: 0.448638\nEpoch [269/800], Batch [70/98], Loss: 5.2956, LR: 0.448638\nEpoch [269/800], Batch [80/98], Loss: 5.3334, LR: 0.448638\nEpoch [269/800], Batch [90/98], Loss: 5.2874, LR: 0.448638\nEpoch [269/800] Average Loss: 5.3128\n\nEpoch [270/800], Batch [0/98], Loss: 5.3134, LR: 0.447613\nEpoch [270/800], Batch [10/98], Loss: 5.3236, LR: 0.447613\nEpoch [270/800], Batch [20/98], Loss: 5.3114, LR: 0.447613\nEpoch [270/800], Batch [30/98], Loss: 5.3408, LR: 0.447613\nEpoch [270/800], Batch [40/98], Loss: 5.3109, LR: 0.447613\nEpoch [270/800], Batch [50/98], Loss: 5.3227, LR: 0.447613\nEpoch [270/800], Batch [60/98], Loss: 5.3325, LR: 0.447613\nEpoch [270/800], Batch [70/98], Loss: 5.3315, LR: 0.447613\nEpoch [270/800], Batch [80/98], Loss: 5.3293, LR: 0.447613\nEpoch [270/800], Batch [90/98], Loss: 5.3256, LR: 0.447613\nEpoch [270/800] Average Loss: 5.3131\n\nEpoch [271/800], Batch [0/98], Loss: 5.3235, LR: 0.446586\nEpoch [271/800], Batch [10/98], Loss: 5.3238, LR: 0.446586\nEpoch [271/800], Batch [20/98], Loss: 5.3055, LR: 0.446586\nEpoch [271/800], Batch [30/98], Loss: 5.3226, LR: 0.446586\nEpoch [271/800], Batch [40/98], Loss: 5.3183, LR: 0.446586\nEpoch [271/800], Batch [50/98], Loss: 5.3473, LR: 0.446586\nEpoch [271/800], Batch [60/98], Loss: 5.3187, LR: 0.446586\nEpoch [271/800], Batch [70/98], Loss: 5.3156, LR: 0.446586\nEpoch [271/800], Batch [80/98], Loss: 5.3272, LR: 0.446586\nEpoch [271/800], Batch [90/98], Loss: 5.3086, LR: 0.446586\nEpoch [271/800] Average Loss: 5.3123\n\nEpoch [272/800], Batch [0/98], Loss: 5.3268, LR: 0.445557\nEpoch [272/800], Batch [10/98], Loss: 5.3403, LR: 0.445557\nEpoch [272/800], Batch [20/98], Loss: 5.3069, LR: 0.445557\nEpoch [272/800], Batch [30/98], Loss: 5.3107, LR: 0.445557\nEpoch [272/800], Batch [40/98], Loss: 5.3257, LR: 0.445557\nEpoch [272/800], Batch [50/98], Loss: 5.3374, LR: 0.445557\nEpoch [272/800], Batch [60/98], Loss: 5.3185, LR: 0.445557\nEpoch [272/800], Batch [70/98], Loss: 5.3321, LR: 0.445557\nEpoch [272/800], Batch [80/98], Loss: 5.3100, LR: 0.445557\nEpoch [272/800], Batch [90/98], Loss: 5.3407, LR: 0.445557\nEpoch [272/800] Average Loss: 5.3106\n\nEpoch [273/800], Batch [0/98], Loss: 5.3329, LR: 0.444526\nEpoch [273/800], Batch [10/98], Loss: 5.2949, LR: 0.444526\nEpoch [273/800], Batch [20/98], Loss: 5.3435, LR: 0.444526\nEpoch [273/800], Batch [30/98], Loss: 5.3192, LR: 0.444526\nEpoch [273/800], Batch [40/98], Loss: 5.3153, LR: 0.444526\nEpoch [273/800], Batch [50/98], Loss: 5.2933, LR: 0.444526\nEpoch [273/800], Batch [60/98], Loss: 5.3136, LR: 0.444526\nEpoch [273/800], Batch [70/98], Loss: 5.3157, LR: 0.444526\nEpoch [273/800], Batch [80/98], Loss: 5.3174, LR: 0.444526\nEpoch [273/800], Batch [90/98], Loss: 5.3003, LR: 0.444526\nEpoch [273/800] Average Loss: 5.3117\n\nEpoch [274/800], Batch [0/98], Loss: 5.3274, LR: 0.443493\nEpoch [274/800], Batch [10/98], Loss: 5.3082, LR: 0.443493\nEpoch [274/800], Batch [20/98], Loss: 5.3138, LR: 0.443493\nEpoch [274/800], Batch [30/98], Loss: 5.3260, LR: 0.443493\nEpoch [274/800], Batch [40/98], Loss: 5.3151, LR: 0.443493\nEpoch [274/800], Batch [50/98], Loss: 5.3009, LR: 0.443493\nEpoch [274/800], Batch [60/98], Loss: 5.3436, LR: 0.443493\nEpoch [274/800], Batch [70/98], Loss: 5.3097, LR: 0.443493\nEpoch [274/800], Batch [80/98], Loss: 5.2944, LR: 0.443493\nEpoch [274/800], Batch [90/98], Loss: 5.2894, LR: 0.443493\nEpoch [274/800] Average Loss: 5.3111\n\nEpoch [275/800], Batch [0/98], Loss: 5.3196, LR: 0.442457\nEpoch [275/800], Batch [10/98], Loss: 5.3145, LR: 0.442457\nEpoch [275/800], Batch [20/98], Loss: 5.3040, LR: 0.442457\nEpoch [275/800], Batch [30/98], Loss: 5.2875, LR: 0.442457\nEpoch [275/800], Batch [40/98], Loss: 5.3321, LR: 0.442457\nEpoch [275/800], Batch [50/98], Loss: 5.3070, LR: 0.442457\nEpoch [275/800], Batch [60/98], Loss: 5.3172, LR: 0.442457\nEpoch [275/800], Batch [70/98], Loss: 5.3007, LR: 0.442457\nEpoch [275/800], Batch [80/98], Loss: 5.3165, LR: 0.442457\nEpoch [275/800], Batch [90/98], Loss: 5.3260, LR: 0.442457\nEpoch [275/800] Average Loss: 5.3103\n\nEpoch [276/800], Batch [0/98], Loss: 5.3086, LR: 0.441419\nEpoch [276/800], Batch [10/98], Loss: 5.2820, LR: 0.441419\nEpoch [276/800], Batch [20/98], Loss: 5.3279, LR: 0.441419\nEpoch [276/800], Batch [30/98], Loss: 5.2950, LR: 0.441419\nEpoch [276/800], Batch [40/98], Loss: 5.3196, LR: 0.441419\nEpoch [276/800], Batch [50/98], Loss: 5.3206, LR: 0.441419\nEpoch [276/800], Batch [60/98], Loss: 5.3008, LR: 0.441419\nEpoch [276/800], Batch [70/98], Loss: 5.3118, LR: 0.441419\nEpoch [276/800], Batch [80/98], Loss: 5.3103, LR: 0.441419\nEpoch [276/800], Batch [90/98], Loss: 5.3132, LR: 0.441419\nEpoch [276/800] Average Loss: 5.3096\n\nEpoch [277/800], Batch [0/98], Loss: 5.3390, LR: 0.440379\nEpoch [277/800], Batch [10/98], Loss: 5.3177, LR: 0.440379\nEpoch [277/800], Batch [20/98], Loss: 5.2966, LR: 0.440379\nEpoch [277/800], Batch [30/98], Loss: 5.3082, LR: 0.440379\nEpoch [277/800], Batch [40/98], Loss: 5.3256, LR: 0.440379\nEpoch [277/800], Batch [50/98], Loss: 5.3315, LR: 0.440379\nEpoch [277/800], Batch [60/98], Loss: 5.3506, LR: 0.440379\nEpoch [277/800], Batch [70/98], Loss: 5.3257, LR: 0.440379\nEpoch [277/800], Batch [80/98], Loss: 5.3030, LR: 0.440379\nEpoch [277/800], Batch [90/98], Loss: 5.3068, LR: 0.440379\nEpoch [277/800] Average Loss: 5.3113\n\nEpoch [278/800], Batch [0/98], Loss: 5.3088, LR: 0.439337\nEpoch [278/800], Batch [10/98], Loss: 5.3114, LR: 0.439337\nEpoch [278/800], Batch [20/98], Loss: 5.2938, LR: 0.439337\nEpoch [278/800], Batch [30/98], Loss: 5.2926, LR: 0.439337\nEpoch [278/800], Batch [40/98], Loss: 5.3207, LR: 0.439337\nEpoch [278/800], Batch [50/98], Loss: 5.3004, LR: 0.439337\nEpoch [278/800], Batch [60/98], Loss: 5.3118, LR: 0.439337\nEpoch [278/800], Batch [70/98], Loss: 5.3279, LR: 0.439337\nEpoch [278/800], Batch [80/98], Loss: 5.3477, LR: 0.439337\nEpoch [278/800], Batch [90/98], Loss: 5.3387, LR: 0.439337\nEpoch [278/800] Average Loss: 5.3090\n\nEpoch [279/800], Batch [0/98], Loss: 5.2872, LR: 0.438292\nEpoch [279/800], Batch [10/98], Loss: 5.3030, LR: 0.438292\nEpoch [279/800], Batch [20/98], Loss: 5.2980, LR: 0.438292\nEpoch [279/800], Batch [30/98], Loss: 5.3278, LR: 0.438292\nEpoch [279/800], Batch [40/98], Loss: 5.2978, LR: 0.438292\nEpoch [279/800], Batch [50/98], Loss: 5.3349, LR: 0.438292\nEpoch [279/800], Batch [60/98], Loss: 5.3305, LR: 0.438292\nEpoch [279/800], Batch [70/98], Loss: 5.3231, LR: 0.438292\nEpoch [279/800], Batch [80/98], Loss: 5.3194, LR: 0.438292\nEpoch [279/800], Batch [90/98], Loss: 5.3013, LR: 0.438292\nEpoch [279/800] Average Loss: 5.3120\n\nEpoch [280/800], Batch [0/98], Loss: 5.2956, LR: 0.437246\nEpoch [280/800], Batch [10/98], Loss: 5.3043, LR: 0.437246\nEpoch [280/800], Batch [20/98], Loss: 5.3272, LR: 0.437246\nEpoch [280/800], Batch [30/98], Loss: 5.3252, LR: 0.437246\nEpoch [280/800], Batch [40/98], Loss: 5.3225, LR: 0.437246\nEpoch [280/800], Batch [50/98], Loss: 5.3282, LR: 0.437246\nEpoch [280/800], Batch [60/98], Loss: 5.2910, LR: 0.437246\nEpoch [280/800], Batch [70/98], Loss: 5.3055, LR: 0.437246\nEpoch [280/800], Batch [80/98], Loss: 5.2924, LR: 0.437246\nEpoch [280/800], Batch [90/98], Loss: 5.3183, LR: 0.437246\nEpoch [280/800] Average Loss: 5.3121\n\nEpoch [281/800], Batch [0/98], Loss: 5.3205, LR: 0.436197\nEpoch [281/800], Batch [10/98], Loss: 5.3295, LR: 0.436197\nEpoch [281/800], Batch [20/98], Loss: 5.3211, LR: 0.436197\nEpoch [281/800], Batch [30/98], Loss: 5.2792, LR: 0.436197\nEpoch [281/800], Batch [40/98], Loss: 5.2985, LR: 0.436197\nEpoch [281/800], Batch [50/98], Loss: 5.3086, LR: 0.436197\nEpoch [281/800], Batch [60/98], Loss: 5.2879, LR: 0.436197\nEpoch [281/800], Batch [70/98], Loss: 5.3241, LR: 0.436197\nEpoch [281/800], Batch [80/98], Loss: 5.3210, LR: 0.436197\nEpoch [281/800], Batch [90/98], Loss: 5.3446, LR: 0.436197\nEpoch [281/800] Average Loss: 5.3115\n\nEpoch [282/800], Batch [0/98], Loss: 5.3156, LR: 0.435146\nEpoch [282/800], Batch [10/98], Loss: 5.3217, LR: 0.435146\nEpoch [282/800], Batch [20/98], Loss: 5.2807, LR: 0.435146\nEpoch [282/800], Batch [30/98], Loss: 5.3308, LR: 0.435146\nEpoch [282/800], Batch [40/98], Loss: 5.3063, LR: 0.435146\nEpoch [282/800], Batch [50/98], Loss: 5.3303, LR: 0.435146\nEpoch [282/800], Batch [60/98], Loss: 5.3170, LR: 0.435146\nEpoch [282/800], Batch [70/98], Loss: 5.3261, LR: 0.435146\nEpoch [282/800], Batch [80/98], Loss: 5.2982, LR: 0.435146\nEpoch [282/800], Batch [90/98], Loss: 5.2958, LR: 0.435146\nEpoch [282/800] Average Loss: 5.3091\n\nEpoch [283/800], Batch [0/98], Loss: 5.3122, LR: 0.434094\nEpoch [283/800], Batch [10/98], Loss: 5.3232, LR: 0.434094\nEpoch [283/800], Batch [20/98], Loss: 5.3397, LR: 0.434094\nEpoch [283/800], Batch [30/98], Loss: 5.3391, LR: 0.434094\nEpoch [283/800], Batch [40/98], Loss: 5.3098, LR: 0.434094\nEpoch [283/800], Batch [50/98], Loss: 5.2980, LR: 0.434094\nEpoch [283/800], Batch [60/98], Loss: 5.3137, LR: 0.434094\nEpoch [283/800], Batch [70/98], Loss: 5.2961, LR: 0.434094\nEpoch [283/800], Batch [80/98], Loss: 5.3109, LR: 0.434094\nEpoch [283/800], Batch [90/98], Loss: 5.3082, LR: 0.434094\nEpoch [283/800] Average Loss: 5.3088\n\nEpoch [284/800], Batch [0/98], Loss: 5.3232, LR: 0.433039\nEpoch [284/800], Batch [10/98], Loss: 5.3313, LR: 0.433039\nEpoch [284/800], Batch [20/98], Loss: 5.3012, LR: 0.433039\nEpoch [284/800], Batch [30/98], Loss: 5.3052, LR: 0.433039\nEpoch [284/800], Batch [40/98], Loss: 5.3029, LR: 0.433039\nEpoch [284/800], Batch [50/98], Loss: 5.3178, LR: 0.433039\nEpoch [284/800], Batch [60/98], Loss: 5.3183, LR: 0.433039\nEpoch [284/800], Batch [70/98], Loss: 5.2957, LR: 0.433039\nEpoch [284/800], Batch [80/98], Loss: 5.3262, LR: 0.433039\nEpoch [284/800], Batch [90/98], Loss: 5.2832, LR: 0.433039\nEpoch [284/800] Average Loss: 5.3070\n\nEpoch [285/800], Batch [0/98], Loss: 5.2985, LR: 0.431982\nEpoch [285/800], Batch [10/98], Loss: 5.3184, LR: 0.431982\nEpoch [285/800], Batch [20/98], Loss: 5.2886, LR: 0.431982\nEpoch [285/800], Batch [30/98], Loss: 5.3362, LR: 0.431982\nEpoch [285/800], Batch [40/98], Loss: 5.3177, LR: 0.431982\nEpoch [285/800], Batch [50/98], Loss: 5.3009, LR: 0.431982\nEpoch [285/800], Batch [60/98], Loss: 5.3089, LR: 0.431982\nEpoch [285/800], Batch [70/98], Loss: 5.3103, LR: 0.431982\nEpoch [285/800], Batch [80/98], Loss: 5.3127, LR: 0.431982\nEpoch [285/800], Batch [90/98], Loss: 5.2969, LR: 0.431982\nEpoch [285/800] Average Loss: 5.3086\n\nEpoch [286/800], Batch [0/98], Loss: 5.3348, LR: 0.430923\nEpoch [286/800], Batch [10/98], Loss: 5.2969, LR: 0.430923\nEpoch [286/800], Batch [20/98], Loss: 5.3351, LR: 0.430923\nEpoch [286/800], Batch [30/98], Loss: 5.3408, LR: 0.430923\nEpoch [286/800], Batch [40/98], Loss: 5.3151, LR: 0.430923\nEpoch [286/800], Batch [50/98], Loss: 5.3207, LR: 0.430923\nEpoch [286/800], Batch [60/98], Loss: 5.3093, LR: 0.430923\nEpoch [286/800], Batch [70/98], Loss: 5.2919, LR: 0.430923\nEpoch [286/800], Batch [80/98], Loss: 5.3123, LR: 0.430923\nEpoch [286/800], Batch [90/98], Loss: 5.2969, LR: 0.430923\nEpoch [286/800] Average Loss: 5.3088\n\nEpoch [287/800], Batch [0/98], Loss: 5.3266, LR: 0.429862\nEpoch [287/800], Batch [10/98], Loss: 5.3135, LR: 0.429862\nEpoch [287/800], Batch [20/98], Loss: 5.3087, LR: 0.429862\nEpoch [287/800], Batch [30/98], Loss: 5.3298, LR: 0.429862\nEpoch [287/800], Batch [40/98], Loss: 5.3169, LR: 0.429862\nEpoch [287/800], Batch [50/98], Loss: 5.2974, LR: 0.429862\nEpoch [287/800], Batch [60/98], Loss: 5.3426, LR: 0.429862\nEpoch [287/800], Batch [70/98], Loss: 5.3139, LR: 0.429862\nEpoch [287/800], Batch [80/98], Loss: 5.3065, LR: 0.429862\nEpoch [287/800], Batch [90/98], Loss: 5.2752, LR: 0.429862\nEpoch [287/800] Average Loss: 5.3064\n\nEpoch [288/800], Batch [0/98], Loss: 5.3108, LR: 0.428799\nEpoch [288/800], Batch [10/98], Loss: 5.3363, LR: 0.428799\nEpoch [288/800], Batch [20/98], Loss: 5.3313, LR: 0.428799\nEpoch [288/800], Batch [30/98], Loss: 5.3068, LR: 0.428799\nEpoch [288/800], Batch [40/98], Loss: 5.3077, LR: 0.428799\nEpoch [288/800], Batch [50/98], Loss: 5.3275, LR: 0.428799\nEpoch [288/800], Batch [60/98], Loss: 5.3172, LR: 0.428799\nEpoch [288/800], Batch [70/98], Loss: 5.3101, LR: 0.428799\nEpoch [288/800], Batch [80/98], Loss: 5.2796, LR: 0.428799\nEpoch [288/800], Batch [90/98], Loss: 5.2959, LR: 0.428799\nEpoch [288/800] Average Loss: 5.3076\n\nEpoch [289/800], Batch [0/98], Loss: 5.3144, LR: 0.427734\nEpoch [289/800], Batch [10/98], Loss: 5.3081, LR: 0.427734\nEpoch [289/800], Batch [20/98], Loss: 5.3299, LR: 0.427734\nEpoch [289/800], Batch [30/98], Loss: 5.3072, LR: 0.427734\nEpoch [289/800], Batch [40/98], Loss: 5.2700, LR: 0.427734\nEpoch [289/800], Batch [50/98], Loss: 5.3081, LR: 0.427734\nEpoch [289/800], Batch [60/98], Loss: 5.2914, LR: 0.427734\nEpoch [289/800], Batch [70/98], Loss: 5.2908, LR: 0.427734\nEpoch [289/800], Batch [80/98], Loss: 5.3146, LR: 0.427734\nEpoch [289/800], Batch [90/98], Loss: 5.3080, LR: 0.427734\nEpoch [289/800] Average Loss: 5.3058\n\nEpoch [290/800], Batch [0/98], Loss: 5.3231, LR: 0.426667\nEpoch [290/800], Batch [10/98], Loss: 5.3134, LR: 0.426667\nEpoch [290/800], Batch [20/98], Loss: 5.3039, LR: 0.426667\nEpoch [290/800], Batch [30/98], Loss: 5.3278, LR: 0.426667\nEpoch [290/800], Batch [40/98], Loss: 5.2962, LR: 0.426667\nEpoch [290/800], Batch [50/98], Loss: 5.3252, LR: 0.426667\nEpoch [290/800], Batch [60/98], Loss: 5.3135, LR: 0.426667\nEpoch [290/800], Batch [70/98], Loss: 5.3040, LR: 0.426667\nEpoch [290/800], Batch [80/98], Loss: 5.2979, LR: 0.426667\nEpoch [290/800], Batch [90/98], Loss: 5.3111, LR: 0.426667\nEpoch [290/800] Average Loss: 5.3064\n\nEpoch [291/800], Batch [0/98], Loss: 5.3279, LR: 0.425598\nEpoch [291/800], Batch [10/98], Loss: 5.2921, LR: 0.425598\nEpoch [291/800], Batch [20/98], Loss: 5.2982, LR: 0.425598\nEpoch [291/800], Batch [30/98], Loss: 5.3034, LR: 0.425598\nEpoch [291/800], Batch [40/98], Loss: 5.3037, LR: 0.425598\nEpoch [291/800], Batch [50/98], Loss: 5.3203, LR: 0.425598\nEpoch [291/800], Batch [60/98], Loss: 5.3175, LR: 0.425598\nEpoch [291/800], Batch [70/98], Loss: 5.3194, LR: 0.425598\nEpoch [291/800], Batch [80/98], Loss: 5.3052, LR: 0.425598\nEpoch [291/800], Batch [90/98], Loss: 5.3109, LR: 0.425598\nEpoch [291/800] Average Loss: 5.3077\n\nEpoch [292/800], Batch [0/98], Loss: 5.3220, LR: 0.424527\nEpoch [292/800], Batch [10/98], Loss: 5.3088, LR: 0.424527\nEpoch [292/800], Batch [20/98], Loss: 5.3072, LR: 0.424527\nEpoch [292/800], Batch [30/98], Loss: 5.3234, LR: 0.424527\nEpoch [292/800], Batch [40/98], Loss: 5.2957, LR: 0.424527\nEpoch [292/800], Batch [50/98], Loss: 5.2998, LR: 0.424527\nEpoch [292/800], Batch [60/98], Loss: 5.2989, LR: 0.424527\nEpoch [292/800], Batch [70/98], Loss: 5.3161, LR: 0.424527\nEpoch [292/800], Batch [80/98], Loss: 5.3168, LR: 0.424527\nEpoch [292/800], Batch [90/98], Loss: 5.3369, LR: 0.424527\nEpoch [292/800] Average Loss: 5.3065\n\nEpoch [293/800], Batch [0/98], Loss: 5.3073, LR: 0.423454\nEpoch [293/800], Batch [10/98], Loss: 5.3085, LR: 0.423454\nEpoch [293/800], Batch [20/98], Loss: 5.3076, LR: 0.423454\nEpoch [293/800], Batch [30/98], Loss: 5.2652, LR: 0.423454\nEpoch [293/800], Batch [40/98], Loss: 5.3087, LR: 0.423454\nEpoch [293/800], Batch [50/98], Loss: 5.2945, LR: 0.423454\nEpoch [293/800], Batch [60/98], Loss: 5.3115, LR: 0.423454\nEpoch [293/800], Batch [70/98], Loss: 5.3097, LR: 0.423454\nEpoch [293/800], Batch [80/98], Loss: 5.3134, LR: 0.423454\nEpoch [293/800], Batch [90/98], Loss: 5.3020, LR: 0.423454\nEpoch [293/800] Average Loss: 5.3072\n\nEpoch [294/800], Batch [0/98], Loss: 5.3273, LR: 0.422380\nEpoch [294/800], Batch [10/98], Loss: 5.3171, LR: 0.422380\nEpoch [294/800], Batch [20/98], Loss: 5.2900, LR: 0.422380\nEpoch [294/800], Batch [30/98], Loss: 5.2915, LR: 0.422380\nEpoch [294/800], Batch [40/98], Loss: 5.3290, LR: 0.422380\nEpoch [294/800], Batch [50/98], Loss: 5.3137, LR: 0.422380\nEpoch [294/800], Batch [60/98], Loss: 5.3033, LR: 0.422380\nEpoch [294/800], Batch [70/98], Loss: 5.3036, LR: 0.422380\nEpoch [294/800], Batch [80/98], Loss: 5.3195, LR: 0.422380\nEpoch [294/800], Batch [90/98], Loss: 5.3071, LR: 0.422380\nEpoch [294/800] Average Loss: 5.3050\n\nEpoch [295/800], Batch [0/98], Loss: 5.2966, LR: 0.421303\nEpoch [295/800], Batch [10/98], Loss: 5.3309, LR: 0.421303\nEpoch [295/800], Batch [20/98], Loss: 5.2799, LR: 0.421303\nEpoch [295/800], Batch [30/98], Loss: 5.3193, LR: 0.421303\nEpoch [295/800], Batch [40/98], Loss: 5.3389, LR: 0.421303\nEpoch [295/800], Batch [50/98], Loss: 5.2965, LR: 0.421303\nEpoch [295/800], Batch [60/98], Loss: 5.2886, LR: 0.421303\nEpoch [295/800], Batch [70/98], Loss: 5.3242, LR: 0.421303\nEpoch [295/800], Batch [80/98], Loss: 5.3279, LR: 0.421303\nEpoch [295/800], Batch [90/98], Loss: 5.3072, LR: 0.421303\nEpoch [295/800] Average Loss: 5.3040\n\nEpoch [296/800], Batch [0/98], Loss: 5.3043, LR: 0.420225\nEpoch [296/800], Batch [10/98], Loss: 5.2985, LR: 0.420225\nEpoch [296/800], Batch [20/98], Loss: 5.3041, LR: 0.420225\nEpoch [296/800], Batch [30/98], Loss: 5.2965, LR: 0.420225\nEpoch [296/800], Batch [40/98], Loss: 5.3069, LR: 0.420225\nEpoch [296/800], Batch [50/98], Loss: 5.2964, LR: 0.420225\nEpoch [296/800], Batch [60/98], Loss: 5.3030, LR: 0.420225\nEpoch [296/800], Batch [70/98], Loss: 5.3178, LR: 0.420225\nEpoch [296/800], Batch [80/98], Loss: 5.3062, LR: 0.420225\nEpoch [296/800], Batch [90/98], Loss: 5.3010, LR: 0.420225\nEpoch [296/800] Average Loss: 5.3061\n\nEpoch [297/800], Batch [0/98], Loss: 5.2997, LR: 0.419144\nEpoch [297/800], Batch [10/98], Loss: 5.2874, LR: 0.419144\nEpoch [297/800], Batch [20/98], Loss: 5.3030, LR: 0.419144\nEpoch [297/800], Batch [30/98], Loss: 5.3064, LR: 0.419144\nEpoch [297/800], Batch [40/98], Loss: 5.3114, LR: 0.419144\nEpoch [297/800], Batch [50/98], Loss: 5.3121, LR: 0.419144\nEpoch [297/800], Batch [60/98], Loss: 5.2978, LR: 0.419144\nEpoch [297/800], Batch [70/98], Loss: 5.2705, LR: 0.419144\nEpoch [297/800], Batch [80/98], Loss: 5.3168, LR: 0.419144\nEpoch [297/800], Batch [90/98], Loss: 5.2965, LR: 0.419144\nEpoch [297/800] Average Loss: 5.3044\n\nEpoch [298/800], Batch [0/98], Loss: 5.3117, LR: 0.418062\nEpoch [298/800], Batch [10/98], Loss: 5.3345, LR: 0.418062\nEpoch [298/800], Batch [20/98], Loss: 5.2879, LR: 0.418062\nEpoch [298/800], Batch [30/98], Loss: 5.3077, LR: 0.418062\nEpoch [298/800], Batch [40/98], Loss: 5.3310, LR: 0.418062\nEpoch [298/800], Batch [50/98], Loss: 5.3221, LR: 0.418062\nEpoch [298/800], Batch [60/98], Loss: 5.3280, LR: 0.418062\nEpoch [298/800], Batch [70/98], Loss: 5.2865, LR: 0.418062\nEpoch [298/800], Batch [80/98], Loss: 5.3184, LR: 0.418062\nEpoch [298/800], Batch [90/98], Loss: 5.3144, LR: 0.418062\nEpoch [298/800] Average Loss: 5.3062\n\nEpoch [299/800], Batch [0/98], Loss: 5.3119, LR: 0.416978\nEpoch [299/800], Batch [10/98], Loss: 5.2957, LR: 0.416978\nEpoch [299/800], Batch [20/98], Loss: 5.3150, LR: 0.416978\nEpoch [299/800], Batch [30/98], Loss: 5.3287, LR: 0.416978\nEpoch [299/800], Batch [40/98], Loss: 5.3077, LR: 0.416978\nEpoch [299/800], Batch [50/98], Loss: 5.3192, LR: 0.416978\nEpoch [299/800], Batch [60/98], Loss: 5.3051, LR: 0.416978\nEpoch [299/800], Batch [70/98], Loss: 5.3176, LR: 0.416978\nEpoch [299/800], Batch [80/98], Loss: 5.3249, LR: 0.416978\nEpoch [299/800], Batch [90/98], Loss: 5.3167, LR: 0.416978\nEpoch [299/800] Average Loss: 5.3083\n\nEpoch [300/800], Batch [0/98], Loss: 5.2991, LR: 0.415893\nEpoch [300/800], Batch [10/98], Loss: 5.3214, LR: 0.415893\nEpoch [300/800], Batch [20/98], Loss: 5.2860, LR: 0.415893\nEpoch [300/800], Batch [30/98], Loss: 5.3124, LR: 0.415893\nEpoch [300/800], Batch [40/98], Loss: 5.2938, LR: 0.415893\nEpoch [300/800], Batch [50/98], Loss: 5.2908, LR: 0.415893\nEpoch [300/800], Batch [60/98], Loss: 5.2999, LR: 0.415893\nEpoch [300/800], Batch [70/98], Loss: 5.3041, LR: 0.415893\nEpoch [300/800], Batch [80/98], Loss: 5.3222, LR: 0.415893\nEpoch [300/800], Batch [90/98], Loss: 5.3007, LR: 0.415893\nEpoch [300/800] Average Loss: 5.3042\n\nEpoch [301/800], Batch [0/98], Loss: 5.3181, LR: 0.414805\nEpoch [301/800], Batch [10/98], Loss: 5.3080, LR: 0.414805\nEpoch [301/800], Batch [20/98], Loss: 5.3117, LR: 0.414805\nEpoch [301/800], Batch [30/98], Loss: 5.3194, LR: 0.414805\nEpoch [301/800], Batch [40/98], Loss: 5.3453, LR: 0.414805\nEpoch [301/800], Batch [50/98], Loss: 5.3330, LR: 0.414805\nEpoch [301/800], Batch [60/98], Loss: 5.3243, LR: 0.414805\nEpoch [301/800], Batch [70/98], Loss: 5.3015, LR: 0.414805\nEpoch [301/800], Batch [80/98], Loss: 5.2991, LR: 0.414805\nEpoch [301/800], Batch [90/98], Loss: 5.3158, LR: 0.414805\nEpoch [301/800] Average Loss: 5.3055\n\nEpoch [302/800], Batch [0/98], Loss: 5.2966, LR: 0.413716\nEpoch [302/800], Batch [10/98], Loss: 5.3179, LR: 0.413716\nEpoch [302/800], Batch [20/98], Loss: 5.3291, LR: 0.413716\nEpoch [302/800], Batch [30/98], Loss: 5.3064, LR: 0.413716\nEpoch [302/800], Batch [40/98], Loss: 5.2879, LR: 0.413716\nEpoch [302/800], Batch [50/98], Loss: 5.3169, LR: 0.413716\nEpoch [302/800], Batch [60/98], Loss: 5.3041, LR: 0.413716\nEpoch [302/800], Batch [70/98], Loss: 5.3028, LR: 0.413716\nEpoch [302/800], Batch [80/98], Loss: 5.3144, LR: 0.413716\nEpoch [302/800], Batch [90/98], Loss: 5.3017, LR: 0.413716\nEpoch [302/800] Average Loss: 5.3043\n\nEpoch [303/800], Batch [0/98], Loss: 5.2904, LR: 0.412625\nEpoch [303/800], Batch [10/98], Loss: 5.3422, LR: 0.412625\nEpoch [303/800], Batch [20/98], Loss: 5.3206, LR: 0.412625\nEpoch [303/800], Batch [30/98], Loss: 5.3008, LR: 0.412625\nEpoch [303/800], Batch [40/98], Loss: 5.3078, LR: 0.412625\nEpoch [303/800], Batch [50/98], Loss: 5.3082, LR: 0.412625\nEpoch [303/800], Batch [60/98], Loss: 5.3189, LR: 0.412625\nEpoch [303/800], Batch [70/98], Loss: 5.2837, LR: 0.412625\nEpoch [303/800], Batch [80/98], Loss: 5.2910, LR: 0.412625\nEpoch [303/800], Batch [90/98], Loss: 5.3106, LR: 0.412625\nEpoch [303/800] Average Loss: 5.3030\n\nEpoch [304/800], Batch [0/98], Loss: 5.3077, LR: 0.411532\nEpoch [304/800], Batch [10/98], Loss: 5.3022, LR: 0.411532\nEpoch [304/800], Batch [20/98], Loss: 5.3263, LR: 0.411532\nEpoch [304/800], Batch [30/98], Loss: 5.3244, LR: 0.411532\nEpoch [304/800], Batch [40/98], Loss: 5.3421, LR: 0.411532\nEpoch [304/800], Batch [50/98], Loss: 5.3207, LR: 0.411532\nEpoch [304/800], Batch [60/98], Loss: 5.3062, LR: 0.411532\nEpoch [304/800], Batch [70/98], Loss: 5.3097, LR: 0.411532\nEpoch [304/800], Batch [80/98], Loss: 5.3119, LR: 0.411532\nEpoch [304/800], Batch [90/98], Loss: 5.3129, LR: 0.411532\nEpoch [304/800] Average Loss: 5.3029\n\nEpoch [305/800], Batch [0/98], Loss: 5.2927, LR: 0.410437\nEpoch [305/800], Batch [10/98], Loss: 5.2975, LR: 0.410437\nEpoch [305/800], Batch [20/98], Loss: 5.3161, LR: 0.410437\nEpoch [305/800], Batch [30/98], Loss: 5.2766, LR: 0.410437\nEpoch [305/800], Batch [40/98], Loss: 5.3032, LR: 0.410437\nEpoch [305/800], Batch [50/98], Loss: 5.2931, LR: 0.410437\nEpoch [305/800], Batch [60/98], Loss: 5.2925, LR: 0.410437\nEpoch [305/800], Batch [70/98], Loss: 5.2884, LR: 0.410437\nEpoch [305/800], Batch [80/98], Loss: 5.2907, LR: 0.410437\nEpoch [305/800], Batch [90/98], Loss: 5.3416, LR: 0.410437\nEpoch [305/800] Average Loss: 5.2994\n\nEpoch [306/800], Batch [0/98], Loss: 5.3231, LR: 0.409341\nEpoch [306/800], Batch [10/98], Loss: 5.3406, LR: 0.409341\nEpoch [306/800], Batch [20/98], Loss: 5.3162, LR: 0.409341\nEpoch [306/800], Batch [30/98], Loss: 5.2873, LR: 0.409341\nEpoch [306/800], Batch [40/98], Loss: 5.3355, LR: 0.409341\nEpoch [306/800], Batch [50/98], Loss: 5.3091, LR: 0.409341\nEpoch [306/800], Batch [60/98], Loss: 5.2936, LR: 0.409341\nEpoch [306/800], Batch [70/98], Loss: 5.3096, LR: 0.409341\nEpoch [306/800], Batch [80/98], Loss: 5.3165, LR: 0.409341\nEpoch [306/800], Batch [90/98], Loss: 5.3109, LR: 0.409341\nEpoch [306/800] Average Loss: 5.3021\n\nEpoch [307/800], Batch [0/98], Loss: 5.3051, LR: 0.408243\nEpoch [307/800], Batch [10/98], Loss: 5.2921, LR: 0.408243\nEpoch [307/800], Batch [20/98], Loss: 5.3084, LR: 0.408243\nEpoch [307/800], Batch [30/98], Loss: 5.3382, LR: 0.408243\nEpoch [307/800], Batch [40/98], Loss: 5.2920, LR: 0.408243\nEpoch [307/800], Batch [50/98], Loss: 5.3066, LR: 0.408243\nEpoch [307/800], Batch [60/98], Loss: 5.3062, LR: 0.408243\nEpoch [307/800], Batch [70/98], Loss: 5.3037, LR: 0.408243\nEpoch [307/800], Batch [80/98], Loss: 5.2911, LR: 0.408243\nEpoch [307/800], Batch [90/98], Loss: 5.3005, LR: 0.408243\nEpoch [307/800] Average Loss: 5.3050\n\nEpoch [308/800], Batch [0/98], Loss: 5.3012, LR: 0.407144\nEpoch [308/800], Batch [10/98], Loss: 5.3133, LR: 0.407144\nEpoch [308/800], Batch [20/98], Loss: 5.3102, LR: 0.407144\nEpoch [308/800], Batch [30/98], Loss: 5.3013, LR: 0.407144\nEpoch [308/800], Batch [40/98], Loss: 5.2853, LR: 0.407144\nEpoch [308/800], Batch [50/98], Loss: 5.3075, LR: 0.407144\nEpoch [308/800], Batch [60/98], Loss: 5.3037, LR: 0.407144\nEpoch [308/800], Batch [70/98], Loss: 5.3128, LR: 0.407144\nEpoch [308/800], Batch [80/98], Loss: 5.3063, LR: 0.407144\nEpoch [308/800], Batch [90/98], Loss: 5.3358, LR: 0.407144\nEpoch [308/800] Average Loss: 5.3017\n\nEpoch [309/800], Batch [0/98], Loss: 5.3203, LR: 0.406042\nEpoch [309/800], Batch [10/98], Loss: 5.2836, LR: 0.406042\nEpoch [309/800], Batch [20/98], Loss: 5.3122, LR: 0.406042\nEpoch [309/800], Batch [30/98], Loss: 5.2765, LR: 0.406042\nEpoch [309/800], Batch [40/98], Loss: 5.3290, LR: 0.406042\nEpoch [309/800], Batch [50/98], Loss: 5.3036, LR: 0.406042\nEpoch [309/800], Batch [60/98], Loss: 5.3065, LR: 0.406042\nEpoch [309/800], Batch [70/98], Loss: 5.3054, LR: 0.406042\nEpoch [309/800], Batch [80/98], Loss: 5.3077, LR: 0.406042\nEpoch [309/800], Batch [90/98], Loss: 5.3092, LR: 0.406042\nEpoch [309/800] Average Loss: 5.3024\n\nEpoch [310/800], Batch [0/98], Loss: 5.3090, LR: 0.404940\nEpoch [310/800], Batch [10/98], Loss: 5.2903, LR: 0.404940\nEpoch [310/800], Batch [20/98], Loss: 5.2984, LR: 0.404940\nEpoch [310/800], Batch [30/98], Loss: 5.3075, LR: 0.404940\nEpoch [310/800], Batch [40/98], Loss: 5.3115, LR: 0.404940\nEpoch [310/800], Batch [50/98], Loss: 5.3267, LR: 0.404940\nEpoch [310/800], Batch [60/98], Loss: 5.3482, LR: 0.404940\nEpoch [310/800], Batch [70/98], Loss: 5.2898, LR: 0.404940\nEpoch [310/800], Batch [80/98], Loss: 5.2914, LR: 0.404940\nEpoch [310/800], Batch [90/98], Loss: 5.3300, LR: 0.404940\nEpoch [310/800] Average Loss: 5.3032\n\nEpoch [311/800], Batch [0/98], Loss: 5.3116, LR: 0.403835\nEpoch [311/800], Batch [10/98], Loss: 5.3046, LR: 0.403835\nEpoch [311/800], Batch [20/98], Loss: 5.2920, LR: 0.403835\nEpoch [311/800], Batch [30/98], Loss: 5.3106, LR: 0.403835\nEpoch [311/800], Batch [40/98], Loss: 5.2938, LR: 0.403835\nEpoch [311/800], Batch [50/98], Loss: 5.3204, LR: 0.403835\nEpoch [311/800], Batch [60/98], Loss: 5.2950, LR: 0.403835\nEpoch [311/800], Batch [70/98], Loss: 5.3015, LR: 0.403835\nEpoch [311/800], Batch [80/98], Loss: 5.2919, LR: 0.403835\nEpoch [311/800], Batch [90/98], Loss: 5.3137, LR: 0.403835\nEpoch [311/800] Average Loss: 5.3015\n\nEpoch [312/800], Batch [0/98], Loss: 5.2941, LR: 0.402729\nEpoch [312/800], Batch [10/98], Loss: 5.3122, LR: 0.402729\nEpoch [312/800], Batch [20/98], Loss: 5.3038, LR: 0.402729\nEpoch [312/800], Batch [30/98], Loss: 5.3153, LR: 0.402729\nEpoch [312/800], Batch [40/98], Loss: 5.3305, LR: 0.402729\nEpoch [312/800], Batch [50/98], Loss: 5.2804, LR: 0.402729\nEpoch [312/800], Batch [60/98], Loss: 5.3042, LR: 0.402729\nEpoch [312/800], Batch [70/98], Loss: 5.3049, LR: 0.402729\nEpoch [312/800], Batch [80/98], Loss: 5.2963, LR: 0.402729\nEpoch [312/800], Batch [90/98], Loss: 5.3121, LR: 0.402729\nEpoch [312/800] Average Loss: 5.3004\n\nEpoch [313/800], Batch [0/98], Loss: 5.2915, LR: 0.401621\nEpoch [313/800], Batch [10/98], Loss: 5.2916, LR: 0.401621\nEpoch [313/800], Batch [20/98], Loss: 5.2926, LR: 0.401621\nEpoch [313/800], Batch [30/98], Loss: 5.3174, LR: 0.401621\nEpoch [313/800], Batch [40/98], Loss: 5.3102, LR: 0.401621\nEpoch [313/800], Batch [50/98], Loss: 5.2932, LR: 0.401621\nEpoch [313/800], Batch [60/98], Loss: 5.3021, LR: 0.401621\nEpoch [313/800], Batch [70/98], Loss: 5.2918, LR: 0.401621\nEpoch [313/800], Batch [80/98], Loss: 5.2887, LR: 0.401621\nEpoch [313/800], Batch [90/98], Loss: 5.3098, LR: 0.401621\nEpoch [313/800] Average Loss: 5.3001\n\nEpoch [314/800], Batch [0/98], Loss: 5.3051, LR: 0.400512\nEpoch [314/800], Batch [10/98], Loss: 5.3020, LR: 0.400512\nEpoch [314/800], Batch [20/98], Loss: 5.2912, LR: 0.400512\nEpoch [314/800], Batch [30/98], Loss: 5.3117, LR: 0.400512\nEpoch [314/800], Batch [40/98], Loss: 5.2999, LR: 0.400512\nEpoch [314/800], Batch [50/98], Loss: 5.2917, LR: 0.400512\nEpoch [314/800], Batch [60/98], Loss: 5.2960, LR: 0.400512\nEpoch [314/800], Batch [70/98], Loss: 5.3006, LR: 0.400512\nEpoch [314/800], Batch [80/98], Loss: 5.3117, LR: 0.400512\nEpoch [314/800], Batch [90/98], Loss: 5.2951, LR: 0.400512\nEpoch [314/800] Average Loss: 5.3005\n\nEpoch [315/800], Batch [0/98], Loss: 5.3374, LR: 0.399401\nEpoch [315/800], Batch [10/98], Loss: 5.2994, LR: 0.399401\nEpoch [315/800], Batch [20/98], Loss: 5.2929, LR: 0.399401\nEpoch [315/800], Batch [30/98], Loss: 5.2953, LR: 0.399401\nEpoch [315/800], Batch [40/98], Loss: 5.3067, LR: 0.399401\nEpoch [315/800], Batch [50/98], Loss: 5.3084, LR: 0.399401\nEpoch [315/800], Batch [60/98], Loss: 5.3152, LR: 0.399401\nEpoch [315/800], Batch [70/98], Loss: 5.2900, LR: 0.399401\nEpoch [315/800], Batch [80/98], Loss: 5.3039, LR: 0.399401\nEpoch [315/800], Batch [90/98], Loss: 5.2935, LR: 0.399401\nEpoch [315/800] Average Loss: 5.3037\n\nEpoch [316/800], Batch [0/98], Loss: 5.2853, LR: 0.398289\nEpoch [316/800], Batch [10/98], Loss: 5.3096, LR: 0.398289\nEpoch [316/800], Batch [20/98], Loss: 5.2883, LR: 0.398289\nEpoch [316/800], Batch [30/98], Loss: 5.2975, LR: 0.398289\nEpoch [316/800], Batch [40/98], Loss: 5.3057, LR: 0.398289\nEpoch [316/800], Batch [50/98], Loss: 5.3216, LR: 0.398289\nEpoch [316/800], Batch [60/98], Loss: 5.2920, LR: 0.398289\nEpoch [316/800], Batch [70/98], Loss: 5.3058, LR: 0.398289\nEpoch [316/800], Batch [80/98], Loss: 5.3137, LR: 0.398289\nEpoch [316/800], Batch [90/98], Loss: 5.3127, LR: 0.398289\nEpoch [316/800] Average Loss: 5.3037\n\nEpoch [317/800], Batch [0/98], Loss: 5.3135, LR: 0.397175\nEpoch [317/800], Batch [10/98], Loss: 5.2998, LR: 0.397175\nEpoch [317/800], Batch [20/98], Loss: 5.3110, LR: 0.397175\nEpoch [317/800], Batch [30/98], Loss: 5.3037, LR: 0.397175\nEpoch [317/800], Batch [40/98], Loss: 5.3204, LR: 0.397175\nEpoch [317/800], Batch [50/98], Loss: 5.2962, LR: 0.397175\nEpoch [317/800], Batch [60/98], Loss: 5.3155, LR: 0.397175\nEpoch [317/800], Batch [70/98], Loss: 5.2793, LR: 0.397175\nEpoch [317/800], Batch [80/98], Loss: 5.2931, LR: 0.397175\nEpoch [317/800], Batch [90/98], Loss: 5.3202, LR: 0.397175\nEpoch [317/800] Average Loss: 5.3018\n\nEpoch [318/800], Batch [0/98], Loss: 5.3147, LR: 0.396060\nEpoch [318/800], Batch [10/98], Loss: 5.2860, LR: 0.396060\nEpoch [318/800], Batch [20/98], Loss: 5.2965, LR: 0.396060\nEpoch [318/800], Batch [30/98], Loss: 5.2833, LR: 0.396060\nEpoch [318/800], Batch [40/98], Loss: 5.3129, LR: 0.396060\nEpoch [318/800], Batch [50/98], Loss: 5.3073, LR: 0.396060\nEpoch [318/800], Batch [60/98], Loss: 5.2874, LR: 0.396060\nEpoch [318/800], Batch [70/98], Loss: 5.3137, LR: 0.396060\nEpoch [318/800], Batch [80/98], Loss: 5.2957, LR: 0.396060\nEpoch [318/800], Batch [90/98], Loss: 5.3111, LR: 0.396060\nEpoch [318/800] Average Loss: 5.2986\n\nEpoch [319/800], Batch [0/98], Loss: 5.2880, LR: 0.394943\nEpoch [319/800], Batch [10/98], Loss: 5.3155, LR: 0.394943\nEpoch [319/800], Batch [20/98], Loss: 5.3177, LR: 0.394943\nEpoch [319/800], Batch [30/98], Loss: 5.3001, LR: 0.394943\nEpoch [319/800], Batch [40/98], Loss: 5.3248, LR: 0.394943\nEpoch [319/800], Batch [50/98], Loss: 5.3099, LR: 0.394943\nEpoch [319/800], Batch [60/98], Loss: 5.3018, LR: 0.394943\nEpoch [319/800], Batch [70/98], Loss: 5.2755, LR: 0.394943\nEpoch [319/800], Batch [80/98], Loss: 5.3076, LR: 0.394943\nEpoch [319/800], Batch [90/98], Loss: 5.3147, LR: 0.394943\nEpoch [319/800] Average Loss: 5.3005\n\nEpoch [320/800], Batch [0/98], Loss: 5.3074, LR: 0.393825\nEpoch [320/800], Batch [10/98], Loss: 5.3126, LR: 0.393825\nEpoch [320/800], Batch [20/98], Loss: 5.3131, LR: 0.393825\nEpoch [320/800], Batch [30/98], Loss: 5.3042, LR: 0.393825\nEpoch [320/800], Batch [40/98], Loss: 5.2873, LR: 0.393825\nEpoch [320/800], Batch [50/98], Loss: 5.2987, LR: 0.393825\nEpoch [320/800], Batch [60/98], Loss: 5.3011, LR: 0.393825\nEpoch [320/800], Batch [70/98], Loss: 5.3044, LR: 0.393825\nEpoch [320/800], Batch [80/98], Loss: 5.3196, LR: 0.393825\nEpoch [320/800], Batch [90/98], Loss: 5.2853, LR: 0.393825\nEpoch [320/800] Average Loss: 5.3002\n\nEpoch [321/800], Batch [0/98], Loss: 5.3127, LR: 0.392705\nEpoch [321/800], Batch [10/98], Loss: 5.3192, LR: 0.392705\nEpoch [321/800], Batch [20/98], Loss: 5.3001, LR: 0.392705\nEpoch [321/800], Batch [30/98], Loss: 5.2989, LR: 0.392705\nEpoch [321/800], Batch [40/98], Loss: 5.2899, LR: 0.392705\nEpoch [321/800], Batch [50/98], Loss: 5.2947, LR: 0.392705\nEpoch [321/800], Batch [60/98], Loss: 5.2971, LR: 0.392705\nEpoch [321/800], Batch [70/98], Loss: 5.2957, LR: 0.392705\nEpoch [321/800], Batch [80/98], Loss: 5.3214, LR: 0.392705\nEpoch [321/800], Batch [90/98], Loss: 5.3393, LR: 0.392705\nEpoch [321/800] Average Loss: 5.3014\n\nEpoch [322/800], Batch [0/98], Loss: 5.3008, LR: 0.391584\nEpoch [322/800], Batch [10/98], Loss: 5.2912, LR: 0.391584\nEpoch [322/800], Batch [20/98], Loss: 5.3100, LR: 0.391584\nEpoch [322/800], Batch [30/98], Loss: 5.3110, LR: 0.391584\nEpoch [322/800], Batch [40/98], Loss: 5.2814, LR: 0.391584\nEpoch [322/800], Batch [50/98], Loss: 5.2993, LR: 0.391584\nEpoch [322/800], Batch [60/98], Loss: 5.3182, LR: 0.391584\nEpoch [322/800], Batch [70/98], Loss: 5.3091, LR: 0.391584\nEpoch [322/800], Batch [80/98], Loss: 5.3316, LR: 0.391584\nEpoch [322/800], Batch [90/98], Loss: 5.2970, LR: 0.391584\nEpoch [322/800] Average Loss: 5.2978\n\nEpoch [323/800], Batch [0/98], Loss: 5.3139, LR: 0.390461\nEpoch [323/800], Batch [10/98], Loss: 5.2835, LR: 0.390461\nEpoch [323/800], Batch [20/98], Loss: 5.2860, LR: 0.390461\nEpoch [323/800], Batch [30/98], Loss: 5.3078, LR: 0.390461\nEpoch [323/800], Batch [40/98], Loss: 5.3224, LR: 0.390461\nEpoch [323/800], Batch [50/98], Loss: 5.2944, LR: 0.390461\nEpoch [323/800], Batch [60/98], Loss: 5.3087, LR: 0.390461\nEpoch [323/800], Batch [70/98], Loss: 5.3074, LR: 0.390461\nEpoch [323/800], Batch [80/98], Loss: 5.3024, LR: 0.390461\nEpoch [323/800], Batch [90/98], Loss: 5.2995, LR: 0.390461\nEpoch [323/800] Average Loss: 5.2985\n\nEpoch [324/800], Batch [0/98], Loss: 5.2929, LR: 0.389337\nEpoch [324/800], Batch [10/98], Loss: 5.3177, LR: 0.389337\nEpoch [324/800], Batch [20/98], Loss: 5.3037, LR: 0.389337\nEpoch [324/800], Batch [30/98], Loss: 5.2906, LR: 0.389337\nEpoch [324/800], Batch [40/98], Loss: 5.3039, LR: 0.389337\nEpoch [324/800], Batch [50/98], Loss: 5.3029, LR: 0.389337\nEpoch [324/800], Batch [60/98], Loss: 5.3124, LR: 0.389337\nEpoch [324/800], Batch [70/98], Loss: 5.3085, LR: 0.389337\nEpoch [324/800], Batch [80/98], Loss: 5.3025, LR: 0.389337\nEpoch [324/800], Batch [90/98], Loss: 5.2911, LR: 0.389337\nEpoch [324/800] Average Loss: 5.2982\n\nEpoch [325/800], Batch [0/98], Loss: 5.2836, LR: 0.388212\nEpoch [325/800], Batch [10/98], Loss: 5.2998, LR: 0.388212\nEpoch [325/800], Batch [20/98], Loss: 5.3098, LR: 0.388212\nEpoch [325/800], Batch [30/98], Loss: 5.3101, LR: 0.388212\nEpoch [325/800], Batch [40/98], Loss: 5.2892, LR: 0.388212\nEpoch [325/800], Batch [50/98], Loss: 5.2922, LR: 0.388212\nEpoch [325/800], Batch [60/98], Loss: 5.2945, LR: 0.388212\nEpoch [325/800], Batch [70/98], Loss: 5.3256, LR: 0.388212\nEpoch [325/800], Batch [80/98], Loss: 5.3207, LR: 0.388212\nEpoch [325/800], Batch [90/98], Loss: 5.3065, LR: 0.388212\nEpoch [325/800] Average Loss: 5.3005\n\nEpoch [326/800], Batch [0/98], Loss: 5.3019, LR: 0.387085\nEpoch [326/800], Batch [10/98], Loss: 5.3158, LR: 0.387085\nEpoch [326/800], Batch [20/98], Loss: 5.2891, LR: 0.387085\nEpoch [326/800], Batch [30/98], Loss: 5.3127, LR: 0.387085\nEpoch [326/800], Batch [40/98], Loss: 5.3210, LR: 0.387085\nEpoch [326/800], Batch [50/98], Loss: 5.3020, LR: 0.387085\nEpoch [326/800], Batch [60/98], Loss: 5.2912, LR: 0.387085\nEpoch [326/800], Batch [70/98], Loss: 5.3195, LR: 0.387085\nEpoch [326/800], Batch [80/98], Loss: 5.3114, LR: 0.387085\nEpoch [326/800], Batch [90/98], Loss: 5.3101, LR: 0.387085\nEpoch [326/800] Average Loss: 5.2971\n\nEpoch [327/800], Batch [0/98], Loss: 5.2834, LR: 0.385957\nEpoch [327/800], Batch [10/98], Loss: 5.3073, LR: 0.385957\nEpoch [327/800], Batch [20/98], Loss: 5.3058, LR: 0.385957\nEpoch [327/800], Batch [30/98], Loss: 5.2851, LR: 0.385957\nEpoch [327/800], Batch [40/98], Loss: 5.3200, LR: 0.385957\nEpoch [327/800], Batch [50/98], Loss: 5.2766, LR: 0.385957\nEpoch [327/800], Batch [60/98], Loss: 5.2838, LR: 0.385957\nEpoch [327/800], Batch [70/98], Loss: 5.3208, LR: 0.385957\nEpoch [327/800], Batch [80/98], Loss: 5.2846, LR: 0.385957\nEpoch [327/800], Batch [90/98], Loss: 5.2797, LR: 0.385957\nEpoch [327/800] Average Loss: 5.2978\n\nEpoch [328/800], Batch [0/98], Loss: 5.2988, LR: 0.384828\nEpoch [328/800], Batch [10/98], Loss: 5.3009, LR: 0.384828\nEpoch [328/800], Batch [20/98], Loss: 5.3022, LR: 0.384828\nEpoch [328/800], Batch [30/98], Loss: 5.2992, LR: 0.384828\nEpoch [328/800], Batch [40/98], Loss: 5.2743, LR: 0.384828\nEpoch [328/800], Batch [50/98], Loss: 5.2766, LR: 0.384828\nEpoch [328/800], Batch [60/98], Loss: 5.3075, LR: 0.384828\nEpoch [328/800], Batch [70/98], Loss: 5.3220, LR: 0.384828\nEpoch [328/800], Batch [80/98], Loss: 5.3295, LR: 0.384828\nEpoch [328/800], Batch [90/98], Loss: 5.3168, LR: 0.384828\nEpoch [328/800] Average Loss: 5.2967\n\nEpoch [329/800], Batch [0/98], Loss: 5.3059, LR: 0.383697\nEpoch [329/800], Batch [10/98], Loss: 5.2857, LR: 0.383697\nEpoch [329/800], Batch [20/98], Loss: 5.3010, LR: 0.383697\nEpoch [329/800], Batch [30/98], Loss: 5.2954, LR: 0.383697\nEpoch [329/800], Batch [40/98], Loss: 5.2953, LR: 0.383697\nEpoch [329/800], Batch [50/98], Loss: 5.3251, LR: 0.383697\nEpoch [329/800], Batch [60/98], Loss: 5.2963, LR: 0.383697\nEpoch [329/800], Batch [70/98], Loss: 5.2859, LR: 0.383697\nEpoch [329/800], Batch [80/98], Loss: 5.3330, LR: 0.383697\nEpoch [329/800], Batch [90/98], Loss: 5.2947, LR: 0.383697\nEpoch [329/800] Average Loss: 5.2971\n\nEpoch [330/800], Batch [0/98], Loss: 5.3120, LR: 0.382565\nEpoch [330/800], Batch [10/98], Loss: 5.2859, LR: 0.382565\nEpoch [330/800], Batch [20/98], Loss: 5.3225, LR: 0.382565\nEpoch [330/800], Batch [30/98], Loss: 5.3208, LR: 0.382565\nEpoch [330/800], Batch [40/98], Loss: 5.2909, LR: 0.382565\nEpoch [330/800], Batch [50/98], Loss: 5.3022, LR: 0.382565\nEpoch [330/800], Batch [60/98], Loss: 5.3047, LR: 0.382565\nEpoch [330/800], Batch [70/98], Loss: 5.3043, LR: 0.382565\nEpoch [330/800], Batch [80/98], Loss: 5.3099, LR: 0.382565\nEpoch [330/800], Batch [90/98], Loss: 5.2792, LR: 0.382565\nEpoch [330/800] Average Loss: 5.2976\n\nEpoch [331/800], Batch [0/98], Loss: 5.3156, LR: 0.381432\nEpoch [331/800], Batch [10/98], Loss: 5.3154, LR: 0.381432\nEpoch [331/800], Batch [20/98], Loss: 5.3108, LR: 0.381432\nEpoch [331/800], Batch [30/98], Loss: 5.2994, LR: 0.381432\nEpoch [331/800], Batch [40/98], Loss: 5.2818, LR: 0.381432\nEpoch [331/800], Batch [50/98], Loss: 5.2989, LR: 0.381432\nEpoch [331/800], Batch [60/98], Loss: 5.2860, LR: 0.381432\nEpoch [331/800], Batch [70/98], Loss: 5.2802, LR: 0.381432\nEpoch [331/800], Batch [80/98], Loss: 5.2976, LR: 0.381432\nEpoch [331/800], Batch [90/98], Loss: 5.2912, LR: 0.381432\nEpoch [331/800] Average Loss: 5.2967\n\nEpoch [332/800], Batch [0/98], Loss: 5.2985, LR: 0.380298\nEpoch [332/800], Batch [10/98], Loss: 5.3108, LR: 0.380298\nEpoch [332/800], Batch [20/98], Loss: 5.3249, LR: 0.380298\nEpoch [332/800], Batch [30/98], Loss: 5.2702, LR: 0.380298\nEpoch [332/800], Batch [40/98], Loss: 5.3190, LR: 0.380298\nEpoch [332/800], Batch [50/98], Loss: 5.3089, LR: 0.380298\nEpoch [332/800], Batch [60/98], Loss: 5.2999, LR: 0.380298\nEpoch [332/800], Batch [70/98], Loss: 5.2980, LR: 0.380298\nEpoch [332/800], Batch [80/98], Loss: 5.2974, LR: 0.380298\nEpoch [332/800], Batch [90/98], Loss: 5.3211, LR: 0.380298\nEpoch [332/800] Average Loss: 5.2961\n\nEpoch [333/800], Batch [0/98], Loss: 5.2975, LR: 0.379162\nEpoch [333/800], Batch [10/98], Loss: 5.3281, LR: 0.379162\nEpoch [333/800], Batch [20/98], Loss: 5.2699, LR: 0.379162\nEpoch [333/800], Batch [30/98], Loss: 5.2694, LR: 0.379162\nEpoch [333/800], Batch [40/98], Loss: 5.3075, LR: 0.379162\nEpoch [333/800], Batch [50/98], Loss: 5.2762, LR: 0.379162\nEpoch [333/800], Batch [60/98], Loss: 5.3076, LR: 0.379162\nEpoch [333/800], Batch [70/98], Loss: 5.3012, LR: 0.379162\nEpoch [333/800], Batch [80/98], Loss: 5.3009, LR: 0.379162\nEpoch [333/800], Batch [90/98], Loss: 5.2988, LR: 0.379162\nEpoch [333/800] Average Loss: 5.2965\n\nEpoch [334/800], Batch [0/98], Loss: 5.3214, LR: 0.378025\nEpoch [334/800], Batch [10/98], Loss: 5.3145, LR: 0.378025\nEpoch [334/800], Batch [20/98], Loss: 5.3125, LR: 0.378025\nEpoch [334/800], Batch [30/98], Loss: 5.3057, LR: 0.378025\nEpoch [334/800], Batch [40/98], Loss: 5.2745, LR: 0.378025\nEpoch [334/800], Batch [50/98], Loss: 5.2846, LR: 0.378025\nEpoch [334/800], Batch [60/98], Loss: 5.3183, LR: 0.378025\nEpoch [334/800], Batch [70/98], Loss: 5.3259, LR: 0.378025\nEpoch [334/800], Batch [80/98], Loss: 5.3026, LR: 0.378025\nEpoch [334/800], Batch [90/98], Loss: 5.3010, LR: 0.378025\nEpoch [334/800] Average Loss: 5.2979\n\nEpoch [335/800], Batch [0/98], Loss: 5.2887, LR: 0.376887\nEpoch [335/800], Batch [10/98], Loss: 5.3149, LR: 0.376887\nEpoch [335/800], Batch [20/98], Loss: 5.3103, LR: 0.376887\nEpoch [335/800], Batch [30/98], Loss: 5.3012, LR: 0.376887\nEpoch [335/800], Batch [40/98], Loss: 5.2591, LR: 0.376887\nEpoch [335/800], Batch [50/98], Loss: 5.3074, LR: 0.376887\nEpoch [335/800], Batch [60/98], Loss: 5.3130, LR: 0.376887\nEpoch [335/800], Batch [70/98], Loss: 5.3031, LR: 0.376887\nEpoch [335/800], Batch [80/98], Loss: 5.2906, LR: 0.376887\nEpoch [335/800], Batch [90/98], Loss: 5.3002, LR: 0.376887\nEpoch [335/800] Average Loss: 5.2958\n\nEpoch [336/800], Batch [0/98], Loss: 5.2783, LR: 0.375747\nEpoch [336/800], Batch [10/98], Loss: 5.2866, LR: 0.375747\nEpoch [336/800], Batch [20/98], Loss: 5.2689, LR: 0.375747\nEpoch [336/800], Batch [30/98], Loss: 5.3076, LR: 0.375747\nEpoch [336/800], Batch [40/98], Loss: 5.3080, LR: 0.375747\nEpoch [336/800], Batch [50/98], Loss: 5.3262, LR: 0.375747\nEpoch [336/800], Batch [60/98], Loss: 5.2860, LR: 0.375747\nEpoch [336/800], Batch [70/98], Loss: 5.2774, LR: 0.375747\nEpoch [336/800], Batch [80/98], Loss: 5.3036, LR: 0.375747\nEpoch [336/800], Batch [90/98], Loss: 5.3197, LR: 0.375747\nEpoch [336/800] Average Loss: 5.2973\n\nEpoch [337/800], Batch [0/98], Loss: 5.3042, LR: 0.374607\nEpoch [337/800], Batch [10/98], Loss: 5.3046, LR: 0.374607\nEpoch [337/800], Batch [20/98], Loss: 5.3195, LR: 0.374607\nEpoch [337/800], Batch [30/98], Loss: 5.2843, LR: 0.374607\nEpoch [337/800], Batch [40/98], Loss: 5.3209, LR: 0.374607\nEpoch [337/800], Batch [50/98], Loss: 5.3126, LR: 0.374607\nEpoch [337/800], Batch [60/98], Loss: 5.3050, LR: 0.374607\nEpoch [337/800], Batch [70/98], Loss: 5.3073, LR: 0.374607\nEpoch [337/800], Batch [80/98], Loss: 5.2935, LR: 0.374607\nEpoch [337/800], Batch [90/98], Loss: 5.2730, LR: 0.374607\nEpoch [337/800] Average Loss: 5.2979\n\nEpoch [338/800], Batch [0/98], Loss: 5.2806, LR: 0.373465\nEpoch [338/800], Batch [10/98], Loss: 5.3122, LR: 0.373465\nEpoch [338/800], Batch [20/98], Loss: 5.3150, LR: 0.373465\nEpoch [338/800], Batch [30/98], Loss: 5.3137, LR: 0.373465\nEpoch [338/800], Batch [40/98], Loss: 5.3040, LR: 0.373465\nEpoch [338/800], Batch [50/98], Loss: 5.3108, LR: 0.373465\nEpoch [338/800], Batch [60/98], Loss: 5.3201, LR: 0.373465\nEpoch [338/800], Batch [70/98], Loss: 5.2997, LR: 0.373465\nEpoch [338/800], Batch [80/98], Loss: 5.3121, LR: 0.373465\nEpoch [338/800], Batch [90/98], Loss: 5.3196, LR: 0.373465\nEpoch [338/800] Average Loss: 5.2955\n\nEpoch [339/800], Batch [0/98], Loss: 5.2882, LR: 0.372323\nEpoch [339/800], Batch [10/98], Loss: 5.2753, LR: 0.372323\nEpoch [339/800], Batch [20/98], Loss: 5.3045, LR: 0.372323\nEpoch [339/800], Batch [30/98], Loss: 5.3120, LR: 0.372323\nEpoch [339/800], Batch [40/98], Loss: 5.2794, LR: 0.372323\nEpoch [339/800], Batch [50/98], Loss: 5.3014, LR: 0.372323\nEpoch [339/800], Batch [60/98], Loss: 5.2820, LR: 0.372323\nEpoch [339/800], Batch [70/98], Loss: 5.2948, LR: 0.372323\nEpoch [339/800], Batch [80/98], Loss: 5.2986, LR: 0.372323\nEpoch [339/800], Batch [90/98], Loss: 5.3113, LR: 0.372323\nEpoch [339/800] Average Loss: 5.2956\n\nEpoch [340/800], Batch [0/98], Loss: 5.3007, LR: 0.371179\nEpoch [340/800], Batch [10/98], Loss: 5.2845, LR: 0.371179\nEpoch [340/800], Batch [20/98], Loss: 5.3053, LR: 0.371179\nEpoch [340/800], Batch [30/98], Loss: 5.3016, LR: 0.371179\nEpoch [340/800], Batch [40/98], Loss: 5.2770, LR: 0.371179\nEpoch [340/800], Batch [50/98], Loss: 5.2920, LR: 0.371179\nEpoch [340/800], Batch [60/98], Loss: 5.3120, LR: 0.371179\nEpoch [340/800], Batch [70/98], Loss: 5.3045, LR: 0.371179\nEpoch [340/800], Batch [80/98], Loss: 5.2817, LR: 0.371179\nEpoch [340/800], Batch [90/98], Loss: 5.3186, LR: 0.371179\nEpoch [340/800] Average Loss: 5.2974\n\nEpoch [341/800], Batch [0/98], Loss: 5.3003, LR: 0.370034\nEpoch [341/800], Batch [10/98], Loss: 5.3075, LR: 0.370034\nEpoch [341/800], Batch [20/98], Loss: 5.3084, LR: 0.370034\nEpoch [341/800], Batch [30/98], Loss: 5.2935, LR: 0.370034\nEpoch [341/800], Batch [40/98], Loss: 5.3113, LR: 0.370034\nEpoch [341/800], Batch [50/98], Loss: 5.2841, LR: 0.370034\nEpoch [341/800], Batch [60/98], Loss: 5.3153, LR: 0.370034\nEpoch [341/800], Batch [70/98], Loss: 5.2952, LR: 0.370034\nEpoch [341/800], Batch [80/98], Loss: 5.3068, LR: 0.370034\nEpoch [341/800], Batch [90/98], Loss: 5.2945, LR: 0.370034\nEpoch [341/800] Average Loss: 5.2954\n\nEpoch [342/800], Batch [0/98], Loss: 5.2987, LR: 0.368888\nEpoch [342/800], Batch [10/98], Loss: 5.2823, LR: 0.368888\nEpoch [342/800], Batch [20/98], Loss: 5.2993, LR: 0.368888\nEpoch [342/800], Batch [30/98], Loss: 5.3365, LR: 0.368888\nEpoch [342/800], Batch [40/98], Loss: 5.3124, LR: 0.368888\nEpoch [342/800], Batch [50/98], Loss: 5.2841, LR: 0.368888\nEpoch [342/800], Batch [60/98], Loss: 5.3003, LR: 0.368888\nEpoch [342/800], Batch [70/98], Loss: 5.2818, LR: 0.368888\nEpoch [342/800], Batch [80/98], Loss: 5.2893, LR: 0.368888\nEpoch [342/800], Batch [90/98], Loss: 5.2761, LR: 0.368888\nEpoch [342/800] Average Loss: 5.2940\n\nEpoch [343/800], Batch [0/98], Loss: 5.3071, LR: 0.367740\nEpoch [343/800], Batch [10/98], Loss: 5.3020, LR: 0.367740\nEpoch [343/800], Batch [20/98], Loss: 5.3042, LR: 0.367740\nEpoch [343/800], Batch [30/98], Loss: 5.3036, LR: 0.367740\nEpoch [343/800], Batch [40/98], Loss: 5.2914, LR: 0.367740\nEpoch [343/800], Batch [50/98], Loss: 5.2979, LR: 0.367740\nEpoch [343/800], Batch [60/98], Loss: 5.3227, LR: 0.367740\nEpoch [343/800], Batch [70/98], Loss: 5.2953, LR: 0.367740\nEpoch [343/800], Batch [80/98], Loss: 5.3075, LR: 0.367740\nEpoch [343/800], Batch [90/98], Loss: 5.2676, LR: 0.367740\nEpoch [343/800] Average Loss: 5.2939\n\nEpoch [344/800], Batch [0/98], Loss: 5.3184, LR: 0.366592\nEpoch [344/800], Batch [10/98], Loss: 5.2770, LR: 0.366592\nEpoch [344/800], Batch [20/98], Loss: 5.2946, LR: 0.366592\nEpoch [344/800], Batch [30/98], Loss: 5.2972, LR: 0.366592\nEpoch [344/800], Batch [40/98], Loss: 5.2960, LR: 0.366592\nEpoch [344/800], Batch [50/98], Loss: 5.3104, LR: 0.366592\nEpoch [344/800], Batch [60/98], Loss: 5.2872, LR: 0.366592\nEpoch [344/800], Batch [70/98], Loss: 5.2949, LR: 0.366592\nEpoch [344/800], Batch [80/98], Loss: 5.3089, LR: 0.366592\nEpoch [344/800], Batch [90/98], Loss: 5.2994, LR: 0.366592\nEpoch [344/800] Average Loss: 5.2936\n\nEpoch [345/800], Batch [0/98], Loss: 5.3016, LR: 0.365443\nEpoch [345/800], Batch [10/98], Loss: 5.3051, LR: 0.365443\nEpoch [345/800], Batch [20/98], Loss: 5.3087, LR: 0.365443\nEpoch [345/800], Batch [30/98], Loss: 5.2994, LR: 0.365443\nEpoch [345/800], Batch [40/98], Loss: 5.3289, LR: 0.365443\nEpoch [345/800], Batch [50/98], Loss: 5.2991, LR: 0.365443\nEpoch [345/800], Batch [60/98], Loss: 5.2877, LR: 0.365443\nEpoch [345/800], Batch [70/98], Loss: 5.3060, LR: 0.365443\nEpoch [345/800], Batch [80/98], Loss: 5.2764, LR: 0.365443\nEpoch [345/800], Batch [90/98], Loss: 5.2777, LR: 0.365443\nEpoch [345/800] Average Loss: 5.2945\n\nEpoch [346/800], Batch [0/98], Loss: 5.3036, LR: 0.364293\nEpoch [346/800], Batch [10/98], Loss: 5.3149, LR: 0.364293\nEpoch [346/800], Batch [20/98], Loss: 5.3064, LR: 0.364293\nEpoch [346/800], Batch [30/98], Loss: 5.3215, LR: 0.364293\nEpoch [346/800], Batch [40/98], Loss: 5.2961, LR: 0.364293\nEpoch [346/800], Batch [50/98], Loss: 5.2841, LR: 0.364293\nEpoch [346/800], Batch [60/98], Loss: 5.2889, LR: 0.364293\nEpoch [346/800], Batch [70/98], Loss: 5.2884, LR: 0.364293\nEpoch [346/800], Batch [80/98], Loss: 5.2953, LR: 0.364293\nEpoch [346/800], Batch [90/98], Loss: 5.3195, LR: 0.364293\nEpoch [346/800] Average Loss: 5.2941\n\nEpoch [347/800], Batch [0/98], Loss: 5.3085, LR: 0.363142\nEpoch [347/800], Batch [10/98], Loss: 5.2985, LR: 0.363142\nEpoch [347/800], Batch [20/98], Loss: 5.2934, LR: 0.363142\nEpoch [347/800], Batch [30/98], Loss: 5.2913, LR: 0.363142\nEpoch [347/800], Batch [40/98], Loss: 5.2985, LR: 0.363142\nEpoch [347/800], Batch [50/98], Loss: 5.2849, LR: 0.363142\nEpoch [347/800], Batch [60/98], Loss: 5.2968, LR: 0.363142\nEpoch [347/800], Batch [70/98], Loss: 5.3204, LR: 0.363142\nEpoch [347/800], Batch [80/98], Loss: 5.2783, LR: 0.363142\nEpoch [347/800], Batch [90/98], Loss: 5.2777, LR: 0.363142\nEpoch [347/800] Average Loss: 5.2927\n\nEpoch [348/800], Batch [0/98], Loss: 5.3086, LR: 0.361989\nEpoch [348/800], Batch [10/98], Loss: 5.2802, LR: 0.361989\nEpoch [348/800], Batch [20/98], Loss: 5.3169, LR: 0.361989\nEpoch [348/800], Batch [30/98], Loss: 5.3136, LR: 0.361989\nEpoch [348/800], Batch [40/98], Loss: 5.3076, LR: 0.361989\nEpoch [348/800], Batch [50/98], Loss: 5.3087, LR: 0.361989\nEpoch [348/800], Batch [60/98], Loss: 5.2942, LR: 0.361989\nEpoch [348/800], Batch [70/98], Loss: 5.2880, LR: 0.361989\nEpoch [348/800], Batch [80/98], Loss: 5.3124, LR: 0.361989\nEpoch [348/800], Batch [90/98], Loss: 5.2746, LR: 0.361989\nEpoch [348/800] Average Loss: 5.2946\n\nEpoch [349/800], Batch [0/98], Loss: 5.3203, LR: 0.360836\nEpoch [349/800], Batch [10/98], Loss: 5.3123, LR: 0.360836\nEpoch [349/800], Batch [20/98], Loss: 5.3151, LR: 0.360836\nEpoch [349/800], Batch [30/98], Loss: 5.3290, LR: 0.360836\nEpoch [349/800], Batch [40/98], Loss: 5.2948, LR: 0.360836\nEpoch [349/800], Batch [50/98], Loss: 5.2969, LR: 0.360836\nEpoch [349/800], Batch [60/98], Loss: 5.2930, LR: 0.360836\nEpoch [349/800], Batch [70/98], Loss: 5.3176, LR: 0.360836\nEpoch [349/800], Batch [80/98], Loss: 5.2977, LR: 0.360836\nEpoch [349/800], Batch [90/98], Loss: 5.3397, LR: 0.360836\nEpoch [349/800] Average Loss: 5.2946\n\nEpoch [350/800], Batch [0/98], Loss: 5.2962, LR: 0.359682\nEpoch [350/800], Batch [10/98], Loss: 5.3135, LR: 0.359682\nEpoch [350/800], Batch [20/98], Loss: 5.2768, LR: 0.359682\nEpoch [350/800], Batch [30/98], Loss: 5.3092, LR: 0.359682\nEpoch [350/800], Batch [40/98], Loss: 5.3009, LR: 0.359682\nEpoch [350/800], Batch [50/98], Loss: 5.2890, LR: 0.359682\nEpoch [350/800], Batch [60/98], Loss: 5.2885, LR: 0.359682\nEpoch [350/800], Batch [70/98], Loss: 5.2856, LR: 0.359682\nEpoch [350/800], Batch [80/98], Loss: 5.3174, LR: 0.359682\nEpoch [350/800], Batch [90/98], Loss: 5.2900, LR: 0.359682\nEpoch [350/800] Average Loss: 5.2923\n\nEpoch [351/800], Batch [0/98], Loss: 5.2872, LR: 0.358527\nEpoch [351/800], Batch [10/98], Loss: 5.2785, LR: 0.358527\nEpoch [351/800], Batch [20/98], Loss: 5.2964, LR: 0.358527\nEpoch [351/800], Batch [30/98], Loss: 5.2873, LR: 0.358527\nEpoch [351/800], Batch [40/98], Loss: 5.3142, LR: 0.358527\nEpoch [351/800], Batch [50/98], Loss: 5.2930, LR: 0.358527\nEpoch [351/800], Batch [60/98], Loss: 5.3143, LR: 0.358527\nEpoch [351/800], Batch [70/98], Loss: 5.2632, LR: 0.358527\nEpoch [351/800], Batch [80/98], Loss: 5.3088, LR: 0.358527\nEpoch [351/800], Batch [90/98], Loss: 5.2821, LR: 0.358527\nEpoch [351/800] Average Loss: 5.2931\n\nEpoch [352/800], Batch [0/98], Loss: 5.3073, LR: 0.357371\nEpoch [352/800], Batch [10/98], Loss: 5.3017, LR: 0.357371\nEpoch [352/800], Batch [20/98], Loss: 5.2656, LR: 0.357371\nEpoch [352/800], Batch [30/98], Loss: 5.3014, LR: 0.357371\nEpoch [352/800], Batch [40/98], Loss: 5.2775, LR: 0.357371\nEpoch [352/800], Batch [50/98], Loss: 5.3234, LR: 0.357371\nEpoch [352/800], Batch [60/98], Loss: 5.2672, LR: 0.357371\nEpoch [352/800], Batch [70/98], Loss: 5.3115, LR: 0.357371\nEpoch [352/800], Batch [80/98], Loss: 5.2924, LR: 0.357371\nEpoch [352/800], Batch [90/98], Loss: 5.2855, LR: 0.357371\nEpoch [352/800] Average Loss: 5.2907\n\nEpoch [353/800], Batch [0/98], Loss: 5.3241, LR: 0.356214\nEpoch [353/800], Batch [10/98], Loss: 5.2944, LR: 0.356214\nEpoch [353/800], Batch [20/98], Loss: 5.2864, LR: 0.356214\nEpoch [353/800], Batch [30/98], Loss: 5.2885, LR: 0.356214\nEpoch [353/800], Batch [40/98], Loss: 5.2920, LR: 0.356214\nEpoch [353/800], Batch [50/98], Loss: 5.2904, LR: 0.356214\nEpoch [353/800], Batch [60/98], Loss: 5.2812, LR: 0.356214\nEpoch [353/800], Batch [70/98], Loss: 5.3138, LR: 0.356214\nEpoch [353/800], Batch [80/98], Loss: 5.3036, LR: 0.356214\nEpoch [353/800], Batch [90/98], Loss: 5.2911, LR: 0.356214\nEpoch [353/800] Average Loss: 5.2948\n\nEpoch [354/800], Batch [0/98], Loss: 5.3130, LR: 0.355057\nEpoch [354/800], Batch [10/98], Loss: 5.2713, LR: 0.355057\nEpoch [354/800], Batch [20/98], Loss: 5.2941, LR: 0.355057\nEpoch [354/800], Batch [30/98], Loss: 5.2937, LR: 0.355057\nEpoch [354/800], Batch [40/98], Loss: 5.2977, LR: 0.355057\nEpoch [354/800], Batch [50/98], Loss: 5.3074, LR: 0.355057\nEpoch [354/800], Batch [60/98], Loss: 5.2721, LR: 0.355057\nEpoch [354/800], Batch [70/98], Loss: 5.2821, LR: 0.355057\nEpoch [354/800], Batch [80/98], Loss: 5.3182, LR: 0.355057\nEpoch [354/800], Batch [90/98], Loss: 5.2821, LR: 0.355057\nEpoch [354/800] Average Loss: 5.2900\n\nEpoch [355/800], Batch [0/98], Loss: 5.2724, LR: 0.353898\nEpoch [355/800], Batch [10/98], Loss: 5.2778, LR: 0.353898\nEpoch [355/800], Batch [20/98], Loss: 5.3061, LR: 0.353898\nEpoch [355/800], Batch [30/98], Loss: 5.2889, LR: 0.353898\nEpoch [355/800], Batch [40/98], Loss: 5.2855, LR: 0.353898\nEpoch [355/800], Batch [50/98], Loss: 5.2924, LR: 0.353898\nEpoch [355/800], Batch [60/98], Loss: 5.2865, LR: 0.353898\nEpoch [355/800], Batch [70/98], Loss: 5.2799, LR: 0.353898\nEpoch [355/800], Batch [80/98], Loss: 5.3201, LR: 0.353898\nEpoch [355/800], Batch [90/98], Loss: 5.2997, LR: 0.353898\nEpoch [355/800] Average Loss: 5.2909\n\nEpoch [356/800], Batch [0/98], Loss: 5.3005, LR: 0.352739\nEpoch [356/800], Batch [10/98], Loss: 5.2919, LR: 0.352739\nEpoch [356/800], Batch [20/98], Loss: 5.2925, LR: 0.352739\nEpoch [356/800], Batch [30/98], Loss: 5.3079, LR: 0.352739\nEpoch [356/800], Batch [40/98], Loss: 5.2697, LR: 0.352739\nEpoch [356/800], Batch [50/98], Loss: 5.3140, LR: 0.352739\nEpoch [356/800], Batch [60/98], Loss: 5.3001, LR: 0.352739\nEpoch [356/800], Batch [70/98], Loss: 5.2898, LR: 0.352739\nEpoch [356/800], Batch [80/98], Loss: 5.2881, LR: 0.352739\nEpoch [356/800], Batch [90/98], Loss: 5.2946, LR: 0.352739\nEpoch [356/800] Average Loss: 5.2917\n\nEpoch [357/800], Batch [0/98], Loss: 5.3006, LR: 0.351579\nEpoch [357/800], Batch [10/98], Loss: 5.2767, LR: 0.351579\nEpoch [357/800], Batch [20/98], Loss: 5.2823, LR: 0.351579\nEpoch [357/800], Batch [30/98], Loss: 5.2637, LR: 0.351579\nEpoch [357/800], Batch [40/98], Loss: 5.2761, LR: 0.351579\nEpoch [357/800], Batch [50/98], Loss: 5.2810, LR: 0.351579\nEpoch [357/800], Batch [60/98], Loss: 5.2988, LR: 0.351579\nEpoch [357/800], Batch [70/98], Loss: 5.2875, LR: 0.351579\nEpoch [357/800], Batch [80/98], Loss: 5.2913, LR: 0.351579\nEpoch [357/800], Batch [90/98], Loss: 5.2868, LR: 0.351579\nEpoch [357/800] Average Loss: 5.2896\n\nEpoch [358/800], Batch [0/98], Loss: 5.2949, LR: 0.350418\nEpoch [358/800], Batch [10/98], Loss: 5.2804, LR: 0.350418\nEpoch [358/800], Batch [20/98], Loss: 5.2765, LR: 0.350418\nEpoch [358/800], Batch [30/98], Loss: 5.3090, LR: 0.350418\nEpoch [358/800], Batch [40/98], Loss: 5.3248, LR: 0.350418\nEpoch [358/800], Batch [50/98], Loss: 5.3129, LR: 0.350418\nEpoch [358/800], Batch [60/98], Loss: 5.3232, LR: 0.350418\nEpoch [358/800], Batch [70/98], Loss: 5.3014, LR: 0.350418\nEpoch [358/800], Batch [80/98], Loss: 5.2955, LR: 0.350418\nEpoch [358/800], Batch [90/98], Loss: 5.2749, LR: 0.350418\nEpoch [358/800] Average Loss: 5.2903\n\nEpoch [359/800], Batch [0/98], Loss: 5.3036, LR: 0.349256\nEpoch [359/800], Batch [10/98], Loss: 5.2840, LR: 0.349256\nEpoch [359/800], Batch [20/98], Loss: 5.2806, LR: 0.349256\nEpoch [359/800], Batch [30/98], Loss: 5.2697, LR: 0.349256\nEpoch [359/800], Batch [40/98], Loss: 5.3107, LR: 0.349256\nEpoch [359/800], Batch [50/98], Loss: 5.3126, LR: 0.349256\nEpoch [359/800], Batch [60/98], Loss: 5.2777, LR: 0.349256\nEpoch [359/800], Batch [70/98], Loss: 5.2712, LR: 0.349256\nEpoch [359/800], Batch [80/98], Loss: 5.2956, LR: 0.349256\nEpoch [359/800], Batch [90/98], Loss: 5.2748, LR: 0.349256\nEpoch [359/800] Average Loss: 5.2894\n\nEpoch [360/800], Batch [0/98], Loss: 5.3016, LR: 0.348094\nEpoch [360/800], Batch [10/98], Loss: 5.2697, LR: 0.348094\nEpoch [360/800], Batch [20/98], Loss: 5.3005, LR: 0.348094\nEpoch [360/800], Batch [30/98], Loss: 5.2551, LR: 0.348094\nEpoch [360/800], Batch [40/98], Loss: 5.3018, LR: 0.348094\nEpoch [360/800], Batch [50/98], Loss: 5.2911, LR: 0.348094\nEpoch [360/800], Batch [60/98], Loss: 5.3153, LR: 0.348094\nEpoch [360/800], Batch [70/98], Loss: 5.2836, LR: 0.348094\nEpoch [360/800], Batch [80/98], Loss: 5.2770, LR: 0.348094\nEpoch [360/800], Batch [90/98], Loss: 5.3100, LR: 0.348094\nEpoch [360/800] Average Loss: 5.2906\n\nEpoch [361/800], Batch [0/98], Loss: 5.3133, LR: 0.346930\nEpoch [361/800], Batch [10/98], Loss: 5.2937, LR: 0.346930\nEpoch [361/800], Batch [20/98], Loss: 5.2976, LR: 0.346930\nEpoch [361/800], Batch [30/98], Loss: 5.2912, LR: 0.346930\nEpoch [361/800], Batch [40/98], Loss: 5.2995, LR: 0.346930\nEpoch [361/800], Batch [50/98], Loss: 5.2817, LR: 0.346930\nEpoch [361/800], Batch [60/98], Loss: 5.2804, LR: 0.346930\nEpoch [361/800], Batch [70/98], Loss: 5.3027, LR: 0.346930\nEpoch [361/800], Batch [80/98], Loss: 5.3065, LR: 0.346930\nEpoch [361/800], Batch [90/98], Loss: 5.2968, LR: 0.346930\nEpoch [361/800] Average Loss: 5.2876\n\nEpoch [362/800], Batch [0/98], Loss: 5.3155, LR: 0.345766\nEpoch [362/800], Batch [10/98], Loss: 5.2736, LR: 0.345766\nEpoch [362/800], Batch [20/98], Loss: 5.2934, LR: 0.345766\nEpoch [362/800], Batch [30/98], Loss: 5.3206, LR: 0.345766\nEpoch [362/800], Batch [40/98], Loss: 5.3063, LR: 0.345766\nEpoch [362/800], Batch [50/98], Loss: 5.2876, LR: 0.345766\nEpoch [362/800], Batch [60/98], Loss: 5.2798, LR: 0.345766\nEpoch [362/800], Batch [70/98], Loss: 5.2783, LR: 0.345766\nEpoch [362/800], Batch [80/98], Loss: 5.3131, LR: 0.345766\nEpoch [362/800], Batch [90/98], Loss: 5.3181, LR: 0.345766\nEpoch [362/800] Average Loss: 5.2913\n\nEpoch [363/800], Batch [0/98], Loss: 5.2712, LR: 0.344602\nEpoch [363/800], Batch [10/98], Loss: 5.2903, LR: 0.344602\nEpoch [363/800], Batch [20/98], Loss: 5.2914, LR: 0.344602\nEpoch [363/800], Batch [30/98], Loss: 5.2972, LR: 0.344602\nEpoch [363/800], Batch [40/98], Loss: 5.3279, LR: 0.344602\nEpoch [363/800], Batch [50/98], Loss: 5.2955, LR: 0.344602\nEpoch [363/800], Batch [60/98], Loss: 5.3196, LR: 0.344602\nEpoch [363/800], Batch [70/98], Loss: 5.2805, LR: 0.344602\nEpoch [363/800], Batch [80/98], Loss: 5.2592, LR: 0.344602\nEpoch [363/800], Batch [90/98], Loss: 5.2999, LR: 0.344602\nEpoch [363/800] Average Loss: 5.2905\n\nEpoch [364/800], Batch [0/98], Loss: 5.2706, LR: 0.343436\nEpoch [364/800], Batch [10/98], Loss: 5.2822, LR: 0.343436\nEpoch [364/800], Batch [20/98], Loss: 5.2966, LR: 0.343436\nEpoch [364/800], Batch [30/98], Loss: 5.3026, LR: 0.343436\nEpoch [364/800], Batch [40/98], Loss: 5.2906, LR: 0.343436\nEpoch [364/800], Batch [50/98], Loss: 5.3079, LR: 0.343436\nEpoch [364/800], Batch [60/98], Loss: 5.3093, LR: 0.343436\nEpoch [364/800], Batch [70/98], Loss: 5.2811, LR: 0.343436\nEpoch [364/800], Batch [80/98], Loss: 5.2993, LR: 0.343436\nEpoch [364/800], Batch [90/98], Loss: 5.2820, LR: 0.343436\nEpoch [364/800] Average Loss: 5.2886\n\nEpoch [365/800], Batch [0/98], Loss: 5.3156, LR: 0.342270\nEpoch [365/800], Batch [10/98], Loss: 5.2987, LR: 0.342270\nEpoch [365/800], Batch [20/98], Loss: 5.2926, LR: 0.342270\nEpoch [365/800], Batch [30/98], Loss: 5.3024, LR: 0.342270\nEpoch [365/800], Batch [40/98], Loss: 5.2804, LR: 0.342270\nEpoch [365/800], Batch [50/98], Loss: 5.3027, LR: 0.342270\nEpoch [365/800], Batch [60/98], Loss: 5.3096, LR: 0.342270\nEpoch [365/800], Batch [70/98], Loss: 5.2996, LR: 0.342270\nEpoch [365/800], Batch [80/98], Loss: 5.3156, LR: 0.342270\nEpoch [365/800], Batch [90/98], Loss: 5.2927, LR: 0.342270\nEpoch [365/800] Average Loss: 5.2907\n\nEpoch [366/800], Batch [0/98], Loss: 5.3171, LR: 0.341104\nEpoch [366/800], Batch [10/98], Loss: 5.2799, LR: 0.341104\nEpoch [366/800], Batch [20/98], Loss: 5.2900, LR: 0.341104\nEpoch [366/800], Batch [30/98], Loss: 5.2842, LR: 0.341104\nEpoch [366/800], Batch [40/98], Loss: 5.3003, LR: 0.341104\nEpoch [366/800], Batch [50/98], Loss: 5.2971, LR: 0.341104\nEpoch [366/800], Batch [60/98], Loss: 5.2809, LR: 0.341104\nEpoch [366/800], Batch [70/98], Loss: 5.2870, LR: 0.341104\nEpoch [366/800], Batch [80/98], Loss: 5.2831, LR: 0.341104\nEpoch [366/800], Batch [90/98], Loss: 5.3125, LR: 0.341104\nEpoch [366/800] Average Loss: 5.2904\n\nEpoch [367/800], Batch [0/98], Loss: 5.2817, LR: 0.339936\nEpoch [367/800], Batch [10/98], Loss: 5.2991, LR: 0.339936\nEpoch [367/800], Batch [20/98], Loss: 5.3027, LR: 0.339936\nEpoch [367/800], Batch [30/98], Loss: 5.2702, LR: 0.339936\nEpoch [367/800], Batch [40/98], Loss: 5.3193, LR: 0.339936\nEpoch [367/800], Batch [50/98], Loss: 5.3003, LR: 0.339936\nEpoch [367/800], Batch [60/98], Loss: 5.2968, LR: 0.339936\nEpoch [367/800], Batch [70/98], Loss: 5.2959, LR: 0.339936\nEpoch [367/800], Batch [80/98], Loss: 5.3099, LR: 0.339936\nEpoch [367/800], Batch [90/98], Loss: 5.3145, LR: 0.339936\nEpoch [367/800] Average Loss: 5.2892\n\nEpoch [368/800], Batch [0/98], Loss: 5.2713, LR: 0.338768\nEpoch [368/800], Batch [10/98], Loss: 5.2719, LR: 0.338768\nEpoch [368/800], Batch [20/98], Loss: 5.3023, LR: 0.338768\nEpoch [368/800], Batch [30/98], Loss: 5.3155, LR: 0.338768\nEpoch [368/800], Batch [40/98], Loss: 5.2909, LR: 0.338768\nEpoch [368/800], Batch [50/98], Loss: 5.3263, LR: 0.338768\nEpoch [368/800], Batch [60/98], Loss: 5.2873, LR: 0.338768\nEpoch [368/800], Batch [70/98], Loss: 5.2980, LR: 0.338768\nEpoch [368/800], Batch [80/98], Loss: 5.2828, LR: 0.338768\nEpoch [368/800], Batch [90/98], Loss: 5.2985, LR: 0.338768\nEpoch [368/800] Average Loss: 5.2876\n\nEpoch [369/800], Batch [0/98], Loss: 5.2731, LR: 0.337600\nEpoch [369/800], Batch [10/98], Loss: 5.2827, LR: 0.337600\nEpoch [369/800], Batch [20/98], Loss: 5.2941, LR: 0.337600\nEpoch [369/800], Batch [30/98], Loss: 5.2976, LR: 0.337600\nEpoch [369/800], Batch [40/98], Loss: 5.2838, LR: 0.337600\nEpoch [369/800], Batch [50/98], Loss: 5.3245, LR: 0.337600\nEpoch [369/800], Batch [60/98], Loss: 5.2687, LR: 0.337600\nEpoch [369/800], Batch [70/98], Loss: 5.3013, LR: 0.337600\nEpoch [369/800], Batch [80/98], Loss: 5.3236, LR: 0.337600\nEpoch [369/800], Batch [90/98], Loss: 5.3011, LR: 0.337600\nEpoch [369/800] Average Loss: 5.2881\n\nEpoch [370/800], Batch [0/98], Loss: 5.3082, LR: 0.336431\nEpoch [370/800], Batch [10/98], Loss: 5.2941, LR: 0.336431\nEpoch [370/800], Batch [20/98], Loss: 5.2911, LR: 0.336431\nEpoch [370/800], Batch [30/98], Loss: 5.2980, LR: 0.336431\nEpoch [370/800], Batch [40/98], Loss: 5.3153, LR: 0.336431\nEpoch [370/800], Batch [50/98], Loss: 5.3011, LR: 0.336431\nEpoch [370/800], Batch [60/98], Loss: 5.3090, LR: 0.336431\nEpoch [370/800], Batch [70/98], Loss: 5.2800, LR: 0.336431\nEpoch [370/800], Batch [80/98], Loss: 5.3041, LR: 0.336431\nEpoch [370/800], Batch [90/98], Loss: 5.2667, LR: 0.336431\nEpoch [370/800] Average Loss: 5.2895\n\nEpoch [371/800], Batch [0/98], Loss: 5.2945, LR: 0.335261\nEpoch [371/800], Batch [10/98], Loss: 5.3020, LR: 0.335261\nEpoch [371/800], Batch [20/98], Loss: 5.3102, LR: 0.335261\nEpoch [371/800], Batch [30/98], Loss: 5.2584, LR: 0.335261\nEpoch [371/800], Batch [40/98], Loss: 5.3135, LR: 0.335261\nEpoch [371/800], Batch [50/98], Loss: 5.3037, LR: 0.335261\nEpoch [371/800], Batch [60/98], Loss: 5.2665, LR: 0.335261\nEpoch [371/800], Batch [70/98], Loss: 5.2928, LR: 0.335261\nEpoch [371/800], Batch [80/98], Loss: 5.3014, LR: 0.335261\nEpoch [371/800], Batch [90/98], Loss: 5.2984, LR: 0.335261\nEpoch [371/800] Average Loss: 5.2911\n\nEpoch [372/800], Batch [0/98], Loss: 5.3023, LR: 0.334091\nEpoch [372/800], Batch [10/98], Loss: 5.2900, LR: 0.334091\nEpoch [372/800], Batch [20/98], Loss: 5.2823, LR: 0.334091\nEpoch [372/800], Batch [30/98], Loss: 5.2997, LR: 0.334091\nEpoch [372/800], Batch [40/98], Loss: 5.2989, LR: 0.334091\nEpoch [372/800], Batch [50/98], Loss: 5.2910, LR: 0.334091\nEpoch [372/800], Batch [60/98], Loss: 5.2960, LR: 0.334091\nEpoch [372/800], Batch [70/98], Loss: 5.2845, LR: 0.334091\nEpoch [372/800], Batch [80/98], Loss: 5.3066, LR: 0.334091\nEpoch [372/800], Batch [90/98], Loss: 5.3010, LR: 0.334091\nEpoch [372/800] Average Loss: 5.2886\n\nEpoch [373/800], Batch [0/98], Loss: 5.2993, LR: 0.332920\nEpoch [373/800], Batch [10/98], Loss: 5.3097, LR: 0.332920\nEpoch [373/800], Batch [20/98], Loss: 5.3092, LR: 0.332920\nEpoch [373/800], Batch [30/98], Loss: 5.2997, LR: 0.332920\nEpoch [373/800], Batch [40/98], Loss: 5.2876, LR: 0.332920\nEpoch [373/800], Batch [50/98], Loss: 5.2867, LR: 0.332920\nEpoch [373/800], Batch [60/98], Loss: 5.3063, LR: 0.332920\nEpoch [373/800], Batch [70/98], Loss: 5.3130, LR: 0.332920\nEpoch [373/800], Batch [80/98], Loss: 5.2945, LR: 0.332920\nEpoch [373/800], Batch [90/98], Loss: 5.3167, LR: 0.332920\nEpoch [373/800] Average Loss: 5.2894\n\nEpoch [374/800], Batch [0/98], Loss: 5.2892, LR: 0.331749\nEpoch [374/800], Batch [10/98], Loss: 5.2946, LR: 0.331749\nEpoch [374/800], Batch [20/98], Loss: 5.2968, LR: 0.331749\nEpoch [374/800], Batch [30/98], Loss: 5.2884, LR: 0.331749\nEpoch [374/800], Batch [40/98], Loss: 5.2874, LR: 0.331749\nEpoch [374/800], Batch [50/98], Loss: 5.2998, LR: 0.331749\nEpoch [374/800], Batch [60/98], Loss: 5.2848, LR: 0.331749\nEpoch [374/800], Batch [70/98], Loss: 5.2893, LR: 0.331749\nEpoch [374/800], Batch [80/98], Loss: 5.2863, LR: 0.331749\nEpoch [374/800], Batch [90/98], Loss: 5.2857, LR: 0.331749\nEpoch [374/800] Average Loss: 5.2885\n\nEpoch [375/800], Batch [0/98], Loss: 5.2881, LR: 0.330577\nEpoch [375/800], Batch [10/98], Loss: 5.2757, LR: 0.330577\nEpoch [375/800], Batch [20/98], Loss: 5.2939, LR: 0.330577\nEpoch [375/800], Batch [30/98], Loss: 5.2845, LR: 0.330577\nEpoch [375/800], Batch [40/98], Loss: 5.2759, LR: 0.330577\nEpoch [375/800], Batch [50/98], Loss: 5.2879, LR: 0.330577\nEpoch [375/800], Batch [60/98], Loss: 5.2846, LR: 0.330577\nEpoch [375/800], Batch [70/98], Loss: 5.2773, LR: 0.330577\nEpoch [375/800], Batch [80/98], Loss: 5.2942, LR: 0.330577\nEpoch [375/800], Batch [90/98], Loss: 5.2815, LR: 0.330577\nEpoch [375/800] Average Loss: 5.2863\n\nEpoch [376/800], Batch [0/98], Loss: 5.2909, LR: 0.329405\nEpoch [376/800], Batch [10/98], Loss: 5.2937, LR: 0.329405\nEpoch [376/800], Batch [20/98], Loss: 5.2969, LR: 0.329405\nEpoch [376/800], Batch [30/98], Loss: 5.2732, LR: 0.329405\nEpoch [376/800], Batch [40/98], Loss: 5.3111, LR: 0.329405\nEpoch [376/800], Batch [50/98], Loss: 5.2889, LR: 0.329405\nEpoch [376/800], Batch [60/98], Loss: 5.2922, LR: 0.329405\nEpoch [376/800], Batch [70/98], Loss: 5.2786, LR: 0.329405\nEpoch [376/800], Batch [80/98], Loss: 5.3056, LR: 0.329405\nEpoch [376/800], Batch [90/98], Loss: 5.2979, LR: 0.329405\nEpoch [376/800] Average Loss: 5.2896\n\nEpoch [377/800], Batch [0/98], Loss: 5.2757, LR: 0.328232\nEpoch [377/800], Batch [10/98], Loss: 5.2854, LR: 0.328232\nEpoch [377/800], Batch [20/98], Loss: 5.2878, LR: 0.328232\nEpoch [377/800], Batch [30/98], Loss: 5.3017, LR: 0.328232\nEpoch [377/800], Batch [40/98], Loss: 5.3066, LR: 0.328232\nEpoch [377/800], Batch [50/98], Loss: 5.3008, LR: 0.328232\nEpoch [377/800], Batch [60/98], Loss: 5.2719, LR: 0.328232\nEpoch [377/800], Batch [70/98], Loss: 5.2861, LR: 0.328232\nEpoch [377/800], Batch [80/98], Loss: 5.2833, LR: 0.328232\nEpoch [377/800], Batch [90/98], Loss: 5.2749, LR: 0.328232\nEpoch [377/800] Average Loss: 5.2832\n\nEpoch [378/800], Batch [0/98], Loss: 5.3042, LR: 0.327059\nEpoch [378/800], Batch [10/98], Loss: 5.3220, LR: 0.327059\nEpoch [378/800], Batch [20/98], Loss: 5.2867, LR: 0.327059\nEpoch [378/800], Batch [30/98], Loss: 5.2871, LR: 0.327059\nEpoch [378/800], Batch [40/98], Loss: 5.2838, LR: 0.327059\nEpoch [378/800], Batch [50/98], Loss: 5.2833, LR: 0.327059\nEpoch [378/800], Batch [60/98], Loss: 5.2850, LR: 0.327059\nEpoch [378/800], Batch [70/98], Loss: 5.3124, LR: 0.327059\nEpoch [378/800], Batch [80/98], Loss: 5.2850, LR: 0.327059\nEpoch [378/800], Batch [90/98], Loss: 5.3037, LR: 0.327059\nEpoch [378/800] Average Loss: 5.2859\n\nEpoch [379/800], Batch [0/98], Loss: 5.2732, LR: 0.325886\nEpoch [379/800], Batch [10/98], Loss: 5.3020, LR: 0.325886\nEpoch [379/800], Batch [20/98], Loss: 5.2963, LR: 0.325886\nEpoch [379/800], Batch [30/98], Loss: 5.3002, LR: 0.325886\nEpoch [379/800], Batch [40/98], Loss: 5.2786, LR: 0.325886\nEpoch [379/800], Batch [50/98], Loss: 5.2777, LR: 0.325886\nEpoch [379/800], Batch [60/98], Loss: 5.2749, LR: 0.325886\nEpoch [379/800], Batch [70/98], Loss: 5.2899, LR: 0.325886\nEpoch [379/800], Batch [80/98], Loss: 5.3044, LR: 0.325886\nEpoch [379/800], Batch [90/98], Loss: 5.3222, LR: 0.325886\nEpoch [379/800] Average Loss: 5.2864\n\nEpoch [380/800], Batch [0/98], Loss: 5.2777, LR: 0.324712\nEpoch [380/800], Batch [10/98], Loss: 5.3189, LR: 0.324712\nEpoch [380/800], Batch [20/98], Loss: 5.2717, LR: 0.324712\nEpoch [380/800], Batch [30/98], Loss: 5.2669, LR: 0.324712\nEpoch [380/800], Batch [40/98], Loss: 5.2597, LR: 0.324712\nEpoch [380/800], Batch [50/98], Loss: 5.2688, LR: 0.324712\nEpoch [380/800], Batch [60/98], Loss: 5.3101, LR: 0.324712\nEpoch [380/800], Batch [70/98], Loss: 5.3112, LR: 0.324712\nEpoch [380/800], Batch [80/98], Loss: 5.2680, LR: 0.324712\nEpoch [380/800], Batch [90/98], Loss: 5.2891, LR: 0.324712\nEpoch [380/800] Average Loss: 5.2858\n\nEpoch [381/800], Batch [0/98], Loss: 5.2717, LR: 0.323538\nEpoch [381/800], Batch [10/98], Loss: 5.3170, LR: 0.323538\nEpoch [381/800], Batch [20/98], Loss: 5.3058, LR: 0.323538\nEpoch [381/800], Batch [30/98], Loss: 5.2986, LR: 0.323538\nEpoch [381/800], Batch [40/98], Loss: 5.2857, LR: 0.323538\nEpoch [381/800], Batch [50/98], Loss: 5.3014, LR: 0.323538\nEpoch [381/800], Batch [60/98], Loss: 5.2848, LR: 0.323538\nEpoch [381/800], Batch [70/98], Loss: 5.2873, LR: 0.323538\nEpoch [381/800], Batch [80/98], Loss: 5.2988, LR: 0.323538\nEpoch [381/800], Batch [90/98], Loss: 5.2901, LR: 0.323538\nEpoch [381/800] Average Loss: 5.2846\n\nEpoch [382/800], Batch [0/98], Loss: 5.2784, LR: 0.322363\nEpoch [382/800], Batch [10/98], Loss: 5.2689, LR: 0.322363\nEpoch [382/800], Batch [20/98], Loss: 5.2571, LR: 0.322363\nEpoch [382/800], Batch [30/98], Loss: 5.2850, LR: 0.322363\nEpoch [382/800], Batch [40/98], Loss: 5.2826, LR: 0.322363\nEpoch [382/800], Batch [50/98], Loss: 5.2978, LR: 0.322363\nEpoch [382/800], Batch [60/98], Loss: 5.3009, LR: 0.322363\nEpoch [382/800], Batch [70/98], Loss: 5.2851, LR: 0.322363\nEpoch [382/800], Batch [80/98], Loss: 5.3015, LR: 0.322363\nEpoch [382/800], Batch [90/98], Loss: 5.2701, LR: 0.322363\nEpoch [382/800] Average Loss: 5.2851\n\nEpoch [383/800], Batch [0/98], Loss: 5.2826, LR: 0.321188\nEpoch [383/800], Batch [10/98], Loss: 5.2864, LR: 0.321188\nEpoch [383/800], Batch [20/98], Loss: 5.2866, LR: 0.321188\nEpoch [383/800], Batch [30/98], Loss: 5.2726, LR: 0.321188\nEpoch [383/800], Batch [40/98], Loss: 5.2896, LR: 0.321188\nEpoch [383/800], Batch [50/98], Loss: 5.2981, LR: 0.321188\nEpoch [383/800], Batch [60/98], Loss: 5.2869, LR: 0.321188\nEpoch [383/800], Batch [70/98], Loss: 5.2892, LR: 0.321188\nEpoch [383/800], Batch [80/98], Loss: 5.2762, LR: 0.321188\nEpoch [383/800], Batch [90/98], Loss: 5.3044, LR: 0.321188\nEpoch [383/800] Average Loss: 5.2848\n\nEpoch [384/800], Batch [0/98], Loss: 5.2905, LR: 0.320013\nEpoch [384/800], Batch [10/98], Loss: 5.2899, LR: 0.320013\nEpoch [384/800], Batch [20/98], Loss: 5.2771, LR: 0.320013\nEpoch [384/800], Batch [30/98], Loss: 5.2948, LR: 0.320013\nEpoch [384/800], Batch [40/98], Loss: 5.3004, LR: 0.320013\nEpoch [384/800], Batch [50/98], Loss: 5.3154, LR: 0.320013\nEpoch [384/800], Batch [60/98], Loss: 5.2808, LR: 0.320013\nEpoch [384/800], Batch [70/98], Loss: 5.3164, LR: 0.320013\nEpoch [384/800], Batch [80/98], Loss: 5.2755, LR: 0.320013\nEpoch [384/800], Batch [90/98], Loss: 5.2793, LR: 0.320013\nEpoch [384/800] Average Loss: 5.2831\n\nEpoch [385/800], Batch [0/98], Loss: 5.2918, LR: 0.318837\nEpoch [385/800], Batch [10/98], Loss: 5.2859, LR: 0.318837\nEpoch [385/800], Batch [20/98], Loss: 5.2886, LR: 0.318837\nEpoch [385/800], Batch [30/98], Loss: 5.2748, LR: 0.318837\nEpoch [385/800], Batch [40/98], Loss: 5.2623, LR: 0.318837\nEpoch [385/800], Batch [50/98], Loss: 5.2740, LR: 0.318837\nEpoch [385/800], Batch [60/98], Loss: 5.2751, LR: 0.318837\nEpoch [385/800], Batch [70/98], Loss: 5.2936, LR: 0.318837\nEpoch [385/800], Batch [80/98], Loss: 5.2943, LR: 0.318837\nEpoch [385/800], Batch [90/98], Loss: 5.2768, LR: 0.318837\nEpoch [385/800] Average Loss: 5.2825\n\nEpoch [386/800], Batch [0/98], Loss: 5.2787, LR: 0.317661\nEpoch [386/800], Batch [10/98], Loss: 5.2908, LR: 0.317661\nEpoch [386/800], Batch [20/98], Loss: 5.2743, LR: 0.317661\nEpoch [386/800], Batch [30/98], Loss: 5.2853, LR: 0.317661\nEpoch [386/800], Batch [40/98], Loss: 5.3038, LR: 0.317661\nEpoch [386/800], Batch [50/98], Loss: 5.3125, LR: 0.317661\nEpoch [386/800], Batch [60/98], Loss: 5.2975, LR: 0.317661\nEpoch [386/800], Batch [70/98], Loss: 5.3079, LR: 0.317661\nEpoch [386/800], Batch [80/98], Loss: 5.2885, LR: 0.317661\nEpoch [386/800], Batch [90/98], Loss: 5.3080, LR: 0.317661\nEpoch [386/800] Average Loss: 5.2849\n\nEpoch [387/800], Batch [0/98], Loss: 5.2830, LR: 0.316485\nEpoch [387/800], Batch [10/98], Loss: 5.3091, LR: 0.316485\nEpoch [387/800], Batch [20/98], Loss: 5.3097, LR: 0.316485\nEpoch [387/800], Batch [30/98], Loss: 5.2643, LR: 0.316485\nEpoch [387/800], Batch [40/98], Loss: 5.2832, LR: 0.316485\nEpoch [387/800], Batch [50/98], Loss: 5.2709, LR: 0.316485\nEpoch [387/800], Batch [60/98], Loss: 5.2816, LR: 0.316485\nEpoch [387/800], Batch [70/98], Loss: 5.3004, LR: 0.316485\nEpoch [387/800], Batch [80/98], Loss: 5.2801, LR: 0.316485\nEpoch [387/800], Batch [90/98], Loss: 5.3239, LR: 0.316485\nEpoch [387/800] Average Loss: 5.2851\n\nEpoch [388/800], Batch [0/98], Loss: 5.2782, LR: 0.315309\nEpoch [388/800], Batch [10/98], Loss: 5.2744, LR: 0.315309\nEpoch [388/800], Batch [20/98], Loss: 5.3041, LR: 0.315309\nEpoch [388/800], Batch [30/98], Loss: 5.2961, LR: 0.315309\nEpoch [388/800], Batch [40/98], Loss: 5.3078, LR: 0.315309\nEpoch [388/800], Batch [50/98], Loss: 5.2841, LR: 0.315309\nEpoch [388/800], Batch [60/98], Loss: 5.2614, LR: 0.315309\nEpoch [388/800], Batch [70/98], Loss: 5.3017, LR: 0.315309\nEpoch [388/800], Batch [80/98], Loss: 5.2814, LR: 0.315309\nEpoch [388/800], Batch [90/98], Loss: 5.3023, LR: 0.315309\nEpoch [388/800] Average Loss: 5.2804\n\nEpoch [389/800], Batch [0/98], Loss: 5.2774, LR: 0.314132\nEpoch [389/800], Batch [10/98], Loss: 5.2980, LR: 0.314132\nEpoch [389/800], Batch [20/98], Loss: 5.2875, LR: 0.314132\nEpoch [389/800], Batch [30/98], Loss: 5.2877, LR: 0.314132\nEpoch [389/800], Batch [40/98], Loss: 5.2763, LR: 0.314132\nEpoch [389/800], Batch [50/98], Loss: 5.2708, LR: 0.314132\nEpoch [389/800], Batch [60/98], Loss: 5.2779, LR: 0.314132\nEpoch [389/800], Batch [70/98], Loss: 5.2780, LR: 0.314132\nEpoch [389/800], Batch [80/98], Loss: 5.3228, LR: 0.314132\nEpoch [389/800], Batch [90/98], Loss: 5.2839, LR: 0.314132\nEpoch [389/800] Average Loss: 5.2849\n\nEpoch [390/800], Batch [0/98], Loss: 5.2746, LR: 0.312955\nEpoch [390/800], Batch [10/98], Loss: 5.2970, LR: 0.312955\nEpoch [390/800], Batch [20/98], Loss: 5.2890, LR: 0.312955\nEpoch [390/800], Batch [30/98], Loss: 5.2855, LR: 0.312955\nEpoch [390/800], Batch [40/98], Loss: 5.2717, LR: 0.312955\nEpoch [390/800], Batch [50/98], Loss: 5.2924, LR: 0.312955\nEpoch [390/800], Batch [60/98], Loss: 5.2769, LR: 0.312955\nEpoch [390/800], Batch [70/98], Loss: 5.2863, LR: 0.312955\nEpoch [390/800], Batch [80/98], Loss: 5.3156, LR: 0.312955\nEpoch [390/800], Batch [90/98], Loss: 5.2784, LR: 0.312955\nEpoch [390/800] Average Loss: 5.2846\n\nEpoch [391/800], Batch [0/98], Loss: 5.2949, LR: 0.311778\nEpoch [391/800], Batch [10/98], Loss: 5.2852, LR: 0.311778\nEpoch [391/800], Batch [20/98], Loss: 5.3092, LR: 0.311778\nEpoch [391/800], Batch [30/98], Loss: 5.2959, LR: 0.311778\nEpoch [391/800], Batch [40/98], Loss: 5.2631, LR: 0.311778\nEpoch [391/800], Batch [50/98], Loss: 5.3035, LR: 0.311778\nEpoch [391/800], Batch [60/98], Loss: 5.2520, LR: 0.311778\nEpoch [391/800], Batch [70/98], Loss: 5.3144, LR: 0.311778\nEpoch [391/800], Batch [80/98], Loss: 5.2756, LR: 0.311778\nEpoch [391/800], Batch [90/98], Loss: 5.2914, LR: 0.311778\nEpoch [391/800] Average Loss: 5.2869\n\nEpoch [392/800], Batch [0/98], Loss: 5.2920, LR: 0.310601\nEpoch [392/800], Batch [10/98], Loss: 5.3042, LR: 0.310601\nEpoch [392/800], Batch [20/98], Loss: 5.2920, LR: 0.310601\nEpoch [392/800], Batch [30/98], Loss: 5.2564, LR: 0.310601\nEpoch [392/800], Batch [40/98], Loss: 5.2757, LR: 0.310601\nEpoch [392/800], Batch [50/98], Loss: 5.2747, LR: 0.310601\nEpoch [392/800], Batch [60/98], Loss: 5.2598, LR: 0.310601\nEpoch [392/800], Batch [70/98], Loss: 5.2670, LR: 0.310601\nEpoch [392/800], Batch [80/98], Loss: 5.3083, LR: 0.310601\nEpoch [392/800], Batch [90/98], Loss: 5.2961, LR: 0.310601\nEpoch [392/800] Average Loss: 5.2866\n\nEpoch [393/800], Batch [0/98], Loss: 5.2926, LR: 0.309423\nEpoch [393/800], Batch [10/98], Loss: 5.2671, LR: 0.309423\nEpoch [393/800], Batch [20/98], Loss: 5.2733, LR: 0.309423\nEpoch [393/800], Batch [30/98], Loss: 5.2889, LR: 0.309423\nEpoch [393/800], Batch [40/98], Loss: 5.2737, LR: 0.309423\nEpoch [393/800], Batch [50/98], Loss: 5.2873, LR: 0.309423\nEpoch [393/800], Batch [60/98], Loss: 5.2852, LR: 0.309423\nEpoch [393/800], Batch [70/98], Loss: 5.2864, LR: 0.309423\nEpoch [393/800], Batch [80/98], Loss: 5.2860, LR: 0.309423\nEpoch [393/800], Batch [90/98], Loss: 5.2813, LR: 0.309423\nEpoch [393/800] Average Loss: 5.2843\n\nEpoch [394/800], Batch [0/98], Loss: 5.2961, LR: 0.308246\nEpoch [394/800], Batch [10/98], Loss: 5.2918, LR: 0.308246\nEpoch [394/800], Batch [20/98], Loss: 5.2831, LR: 0.308246\nEpoch [394/800], Batch [30/98], Loss: 5.2772, LR: 0.308246\nEpoch [394/800], Batch [40/98], Loss: 5.3094, LR: 0.308246\nEpoch [394/800], Batch [50/98], Loss: 5.2948, LR: 0.308246\nEpoch [394/800], Batch [60/98], Loss: 5.2972, LR: 0.308246\nEpoch [394/800], Batch [70/98], Loss: 5.3022, LR: 0.308246\nEpoch [394/800], Batch [80/98], Loss: 5.2861, LR: 0.308246\nEpoch [394/800], Batch [90/98], Loss: 5.2552, LR: 0.308246\nEpoch [394/800] Average Loss: 5.2849\n\nEpoch [395/800], Batch [0/98], Loss: 5.2808, LR: 0.307068\nEpoch [395/800], Batch [10/98], Loss: 5.2656, LR: 0.307068\nEpoch [395/800], Batch [20/98], Loss: 5.2895, LR: 0.307068\nEpoch [395/800], Batch [30/98], Loss: 5.2900, LR: 0.307068\nEpoch [395/800], Batch [40/98], Loss: 5.3009, LR: 0.307068\nEpoch [395/800], Batch [50/98], Loss: 5.3059, LR: 0.307068\nEpoch [395/800], Batch [60/98], Loss: 5.2685, LR: 0.307068\nEpoch [395/800], Batch [70/98], Loss: 5.2934, LR: 0.307068\nEpoch [395/800], Batch [80/98], Loss: 5.2780, LR: 0.307068\nEpoch [395/800], Batch [90/98], Loss: 5.2905, LR: 0.307068\nEpoch [395/800] Average Loss: 5.2837\n\nEpoch [396/800], Batch [0/98], Loss: 5.3097, LR: 0.305890\nEpoch [396/800], Batch [10/98], Loss: 5.2884, LR: 0.305890\nEpoch [396/800], Batch [20/98], Loss: 5.2900, LR: 0.305890\nEpoch [396/800], Batch [30/98], Loss: 5.2972, LR: 0.305890\nEpoch [396/800], Batch [40/98], Loss: 5.2588, LR: 0.305890\nEpoch [396/800], Batch [50/98], Loss: 5.2953, LR: 0.305890\nEpoch [396/800], Batch [60/98], Loss: 5.2715, LR: 0.305890\nEpoch [396/800], Batch [70/98], Loss: 5.2835, LR: 0.305890\nEpoch [396/800], Batch [80/98], Loss: 5.3044, LR: 0.305890\nEpoch [396/800], Batch [90/98], Loss: 5.3064, LR: 0.305890\nEpoch [396/800] Average Loss: 5.2832\n\nEpoch [397/800], Batch [0/98], Loss: 5.2902, LR: 0.304712\nEpoch [397/800], Batch [10/98], Loss: 5.2924, LR: 0.304712\nEpoch [397/800], Batch [20/98], Loss: 5.2837, LR: 0.304712\nEpoch [397/800], Batch [30/98], Loss: 5.3058, LR: 0.304712\nEpoch [397/800], Batch [40/98], Loss: 5.2907, LR: 0.304712\nEpoch [397/800], Batch [50/98], Loss: 5.3101, LR: 0.304712\nEpoch [397/800], Batch [60/98], Loss: 5.2946, LR: 0.304712\nEpoch [397/800], Batch [70/98], Loss: 5.2642, LR: 0.304712\nEpoch [397/800], Batch [80/98], Loss: 5.2735, LR: 0.304712\nEpoch [397/800], Batch [90/98], Loss: 5.2874, LR: 0.304712\nEpoch [397/800] Average Loss: 5.2813\n\nEpoch [398/800], Batch [0/98], Loss: 5.2798, LR: 0.303534\nEpoch [398/800], Batch [10/98], Loss: 5.2876, LR: 0.303534\nEpoch [398/800], Batch [20/98], Loss: 5.2876, LR: 0.303534\nEpoch [398/800], Batch [30/98], Loss: 5.2935, LR: 0.303534\nEpoch [398/800], Batch [40/98], Loss: 5.2975, LR: 0.303534\nEpoch [398/800], Batch [50/98], Loss: 5.2931, LR: 0.303534\nEpoch [398/800], Batch [60/98], Loss: 5.2815, LR: 0.303534\nEpoch [398/800], Batch [70/98], Loss: 5.2577, LR: 0.303534\nEpoch [398/800], Batch [80/98], Loss: 5.2896, LR: 0.303534\nEpoch [398/800], Batch [90/98], Loss: 5.3016, LR: 0.303534\nEpoch [398/800] Average Loss: 5.2821\n\nEpoch [399/800], Batch [0/98], Loss: 5.2688, LR: 0.302356\nEpoch [399/800], Batch [10/98], Loss: 5.2837, LR: 0.302356\nEpoch [399/800], Batch [20/98], Loss: 5.3146, LR: 0.302356\nEpoch [399/800], Batch [30/98], Loss: 5.2840, LR: 0.302356\nEpoch [399/800], Batch [40/98], Loss: 5.2662, LR: 0.302356\nEpoch [399/800], Batch [50/98], Loss: 5.2676, LR: 0.302356\nEpoch [399/800], Batch [60/98], Loss: 5.3130, LR: 0.302356\nEpoch [399/800], Batch [70/98], Loss: 5.2718, LR: 0.302356\nEpoch [399/800], Batch [80/98], Loss: 5.2888, LR: 0.302356\nEpoch [399/800], Batch [90/98], Loss: 5.2891, LR: 0.302356\nEpoch [399/800] Average Loss: 5.2804\n\nEpoch [400/800], Batch [0/98], Loss: 5.2874, LR: 0.301178\nEpoch [400/800], Batch [10/98], Loss: 5.2915, LR: 0.301178\nEpoch [400/800], Batch [20/98], Loss: 5.2880, LR: 0.301178\nEpoch [400/800], Batch [30/98], Loss: 5.3020, LR: 0.301178\nEpoch [400/800], Batch [40/98], Loss: 5.2812, LR: 0.301178\nEpoch [400/800], Batch [50/98], Loss: 5.2817, LR: 0.301178\nEpoch [400/800], Batch [60/98], Loss: 5.2727, LR: 0.301178\nEpoch [400/800], Batch [70/98], Loss: 5.3104, LR: 0.301178\nEpoch [400/800], Batch [80/98], Loss: 5.2916, LR: 0.301178\nEpoch [400/800], Batch [90/98], Loss: 5.2858, LR: 0.301178\nEpoch [400/800] Average Loss: 5.2827\n\nEpoch [401/800], Batch [0/98], Loss: 5.2776, LR: 0.300000\nEpoch [401/800], Batch [10/98], Loss: 5.2883, LR: 0.300000\nEpoch [401/800], Batch [20/98], Loss: 5.2579, LR: 0.300000\nEpoch [401/800], Batch [30/98], Loss: 5.2753, LR: 0.300000\nEpoch [401/800], Batch [40/98], Loss: 5.2716, LR: 0.300000\nEpoch [401/800], Batch [50/98], Loss: 5.2742, LR: 0.300000\nEpoch [401/800], Batch [60/98], Loss: 5.2833, LR: 0.300000\nEpoch [401/800], Batch [70/98], Loss: 5.2752, LR: 0.300000\nEpoch [401/800], Batch [80/98], Loss: 5.2633, LR: 0.300000\nEpoch [401/800], Batch [90/98], Loss: 5.2812, LR: 0.300000\nEpoch [401/800] Average Loss: 5.2804\n\nEpoch [402/800], Batch [0/98], Loss: 5.3045, LR: 0.298822\nEpoch [402/800], Batch [10/98], Loss: 5.3099, LR: 0.298822\nEpoch [402/800], Batch [20/98], Loss: 5.2837, LR: 0.298822\nEpoch [402/800], Batch [30/98], Loss: 5.2973, LR: 0.298822\nEpoch [402/800], Batch [40/98], Loss: 5.3024, LR: 0.298822\nEpoch [402/800], Batch [50/98], Loss: 5.3046, LR: 0.298822\nEpoch [402/800], Batch [60/98], Loss: 5.3004, LR: 0.298822\nEpoch [402/800], Batch [70/98], Loss: 5.2858, LR: 0.298822\nEpoch [402/800], Batch [80/98], Loss: 5.2896, LR: 0.298822\nEpoch [402/800], Batch [90/98], Loss: 5.2802, LR: 0.298822\nEpoch [402/800] Average Loss: 5.2828\n\nEpoch [403/800], Batch [0/98], Loss: 5.2880, LR: 0.297644\nEpoch [403/800], Batch [10/98], Loss: 5.2964, LR: 0.297644\nEpoch [403/800], Batch [20/98], Loss: 5.3034, LR: 0.297644\nEpoch [403/800], Batch [30/98], Loss: 5.2784, LR: 0.297644\nEpoch [403/800], Batch [40/98], Loss: 5.3089, LR: 0.297644\nEpoch [403/800], Batch [50/98], Loss: 5.2824, LR: 0.297644\nEpoch [403/800], Batch [60/98], Loss: 5.2578, LR: 0.297644\nEpoch [403/800], Batch [70/98], Loss: 5.2890, LR: 0.297644\nEpoch [403/800], Batch [80/98], Loss: 5.2719, LR: 0.297644\nEpoch [403/800], Batch [90/98], Loss: 5.2604, LR: 0.297644\nEpoch [403/800] Average Loss: 5.2816\n\nEpoch [404/800], Batch [0/98], Loss: 5.2823, LR: 0.296466\nEpoch [404/800], Batch [10/98], Loss: 5.2864, LR: 0.296466\nEpoch [404/800], Batch [20/98], Loss: 5.2769, LR: 0.296466\nEpoch [404/800], Batch [30/98], Loss: 5.2864, LR: 0.296466\nEpoch [404/800], Batch [40/98], Loss: 5.2727, LR: 0.296466\nEpoch [404/800], Batch [50/98], Loss: 5.2621, LR: 0.296466\nEpoch [404/800], Batch [60/98], Loss: 5.2913, LR: 0.296466\nEpoch [404/800], Batch [70/98], Loss: 5.2756, LR: 0.296466\nEpoch [404/800], Batch [80/98], Loss: 5.3003, LR: 0.296466\nEpoch [404/800], Batch [90/98], Loss: 5.2730, LR: 0.296466\nEpoch [404/800] Average Loss: 5.2801\n\nEpoch [405/800], Batch [0/98], Loss: 5.2739, LR: 0.295288\nEpoch [405/800], Batch [10/98], Loss: 5.2749, LR: 0.295288\nEpoch [405/800], Batch [20/98], Loss: 5.2589, LR: 0.295288\nEpoch [405/800], Batch [30/98], Loss: 5.2908, LR: 0.295288\nEpoch [405/800], Batch [40/98], Loss: 5.2986, LR: 0.295288\nEpoch [405/800], Batch [50/98], Loss: 5.2967, LR: 0.295288\nEpoch [405/800], Batch [60/98], Loss: 5.2844, LR: 0.295288\nEpoch [405/800], Batch [70/98], Loss: 5.2935, LR: 0.295288\nEpoch [405/800], Batch [80/98], Loss: 5.2938, LR: 0.295288\nEpoch [405/800], Batch [90/98], Loss: 5.2710, LR: 0.295288\nEpoch [405/800] Average Loss: 5.2816\n\nEpoch [406/800], Batch [0/98], Loss: 5.2658, LR: 0.294110\nEpoch [406/800], Batch [10/98], Loss: 5.3130, LR: 0.294110\nEpoch [406/800], Batch [20/98], Loss: 5.2603, LR: 0.294110\nEpoch [406/800], Batch [30/98], Loss: 5.2894, LR: 0.294110\nEpoch [406/800], Batch [40/98], Loss: 5.2638, LR: 0.294110\nEpoch [406/800], Batch [50/98], Loss: 5.2680, LR: 0.294110\nEpoch [406/800], Batch [60/98], Loss: 5.2769, LR: 0.294110\nEpoch [406/800], Batch [70/98], Loss: 5.3125, LR: 0.294110\nEpoch [406/800], Batch [80/98], Loss: 5.2614, LR: 0.294110\nEpoch [406/800], Batch [90/98], Loss: 5.2618, LR: 0.294110\nEpoch [406/800] Average Loss: 5.2767\n\nEpoch [407/800], Batch [0/98], Loss: 5.3241, LR: 0.292932\nEpoch [407/800], Batch [10/98], Loss: 5.2601, LR: 0.292932\nEpoch [407/800], Batch [20/98], Loss: 5.2939, LR: 0.292932\nEpoch [407/800], Batch [30/98], Loss: 5.2954, LR: 0.292932\nEpoch [407/800], Batch [40/98], Loss: 5.2879, LR: 0.292932\nEpoch [407/800], Batch [50/98], Loss: 5.2869, LR: 0.292932\nEpoch [407/800], Batch [60/98], Loss: 5.2857, LR: 0.292932\nEpoch [407/800], Batch [70/98], Loss: 5.2889, LR: 0.292932\nEpoch [407/800], Batch [80/98], Loss: 5.2948, LR: 0.292932\nEpoch [407/800], Batch [90/98], Loss: 5.2912, LR: 0.292932\nEpoch [407/800] Average Loss: 5.2824\n\nEpoch [408/800], Batch [0/98], Loss: 5.2849, LR: 0.291754\nEpoch [408/800], Batch [10/98], Loss: 5.2818, LR: 0.291754\nEpoch [408/800], Batch [20/98], Loss: 5.2606, LR: 0.291754\nEpoch [408/800], Batch [30/98], Loss: 5.2746, LR: 0.291754\nEpoch [408/800], Batch [40/98], Loss: 5.2724, LR: 0.291754\nEpoch [408/800], Batch [50/98], Loss: 5.2734, LR: 0.291754\nEpoch [408/800], Batch [60/98], Loss: 5.2791, LR: 0.291754\nEpoch [408/800], Batch [70/98], Loss: 5.2959, LR: 0.291754\nEpoch [408/800], Batch [80/98], Loss: 5.2637, LR: 0.291754\nEpoch [408/800], Batch [90/98], Loss: 5.2616, LR: 0.291754\nEpoch [408/800] Average Loss: 5.2798\n\nEpoch [409/800], Batch [0/98], Loss: 5.2817, LR: 0.290577\nEpoch [409/800], Batch [10/98], Loss: 5.3038, LR: 0.290577\nEpoch [409/800], Batch [20/98], Loss: 5.2918, LR: 0.290577\nEpoch [409/800], Batch [30/98], Loss: 5.2857, LR: 0.290577\nEpoch [409/800], Batch [40/98], Loss: 5.2836, LR: 0.290577\nEpoch [409/800], Batch [50/98], Loss: 5.2917, LR: 0.290577\nEpoch [409/800], Batch [60/98], Loss: 5.2632, LR: 0.290577\nEpoch [409/800], Batch [70/98], Loss: 5.2912, LR: 0.290577\nEpoch [409/800], Batch [80/98], Loss: 5.2640, LR: 0.290577\nEpoch [409/800], Batch [90/98], Loss: 5.2738, LR: 0.290577\nEpoch [409/800] Average Loss: 5.2799\n\nEpoch [410/800], Batch [0/98], Loss: 5.2658, LR: 0.289399\nEpoch [410/800], Batch [10/98], Loss: 5.2726, LR: 0.289399\nEpoch [410/800], Batch [20/98], Loss: 5.2976, LR: 0.289399\nEpoch [410/800], Batch [30/98], Loss: 5.3073, LR: 0.289399\nEpoch [410/800], Batch [40/98], Loss: 5.2942, LR: 0.289399\nEpoch [410/800], Batch [50/98], Loss: 5.2858, LR: 0.289399\nEpoch [410/800], Batch [60/98], Loss: 5.2839, LR: 0.289399\nEpoch [410/800], Batch [70/98], Loss: 5.3078, LR: 0.289399\nEpoch [410/800], Batch [80/98], Loss: 5.2769, LR: 0.289399\nEpoch [410/800], Batch [90/98], Loss: 5.2889, LR: 0.289399\nEpoch [410/800] Average Loss: 5.2776\n\nEpoch [411/800], Batch [0/98], Loss: 5.2899, LR: 0.288222\nEpoch [411/800], Batch [10/98], Loss: 5.2708, LR: 0.288222\nEpoch [411/800], Batch [20/98], Loss: 5.2753, LR: 0.288222\nEpoch [411/800], Batch [30/98], Loss: 5.2820, LR: 0.288222\nEpoch [411/800], Batch [40/98], Loss: 5.2997, LR: 0.288222\nEpoch [411/800], Batch [50/98], Loss: 5.2743, LR: 0.288222\nEpoch [411/800], Batch [60/98], Loss: 5.2808, LR: 0.288222\nEpoch [411/800], Batch [70/98], Loss: 5.2802, LR: 0.288222\nEpoch [411/800], Batch [80/98], Loss: 5.2917, LR: 0.288222\nEpoch [411/800], Batch [90/98], Loss: 5.2863, LR: 0.288222\nEpoch [411/800] Average Loss: 5.2788\n\nEpoch [412/800], Batch [0/98], Loss: 5.2913, LR: 0.287045\nEpoch [412/800], Batch [10/98], Loss: 5.2480, LR: 0.287045\nEpoch [412/800], Batch [20/98], Loss: 5.2701, LR: 0.287045\nEpoch [412/800], Batch [30/98], Loss: 5.2720, LR: 0.287045\nEpoch [412/800], Batch [40/98], Loss: 5.2864, LR: 0.287045\nEpoch [412/800], Batch [50/98], Loss: 5.2997, LR: 0.287045\nEpoch [412/800], Batch [60/98], Loss: 5.3039, LR: 0.287045\nEpoch [412/800], Batch [70/98], Loss: 5.2897, LR: 0.287045\nEpoch [412/800], Batch [80/98], Loss: 5.2684, LR: 0.287045\nEpoch [412/800], Batch [90/98], Loss: 5.2698, LR: 0.287045\nEpoch [412/800] Average Loss: 5.2783\n\nEpoch [413/800], Batch [0/98], Loss: 5.2648, LR: 0.285868\nEpoch [413/800], Batch [10/98], Loss: 5.2948, LR: 0.285868\nEpoch [413/800], Batch [20/98], Loss: 5.2837, LR: 0.285868\nEpoch [413/800], Batch [30/98], Loss: 5.2750, LR: 0.285868\nEpoch [413/800], Batch [40/98], Loss: 5.2808, LR: 0.285868\nEpoch [413/800], Batch [50/98], Loss: 5.2745, LR: 0.285868\nEpoch [413/800], Batch [60/98], Loss: 5.3185, LR: 0.285868\nEpoch [413/800], Batch [70/98], Loss: 5.2829, LR: 0.285868\nEpoch [413/800], Batch [80/98], Loss: 5.2749, LR: 0.285868\nEpoch [413/800], Batch [90/98], Loss: 5.2917, LR: 0.285868\nEpoch [413/800] Average Loss: 5.2809\n\nEpoch [414/800], Batch [0/98], Loss: 5.2878, LR: 0.284691\nEpoch [414/800], Batch [10/98], Loss: 5.2770, LR: 0.284691\nEpoch [414/800], Batch [20/98], Loss: 5.2895, LR: 0.284691\nEpoch [414/800], Batch [30/98], Loss: 5.2802, LR: 0.284691\nEpoch [414/800], Batch [40/98], Loss: 5.2728, LR: 0.284691\nEpoch [414/800], Batch [50/98], Loss: 5.2687, LR: 0.284691\nEpoch [414/800], Batch [60/98], Loss: 5.2792, LR: 0.284691\nEpoch [414/800], Batch [70/98], Loss: 5.2574, LR: 0.284691\nEpoch [414/800], Batch [80/98], Loss: 5.2811, LR: 0.284691\nEpoch [414/800], Batch [90/98], Loss: 5.2724, LR: 0.284691\nEpoch [414/800] Average Loss: 5.2777\n\nEpoch [415/800], Batch [0/98], Loss: 5.3038, LR: 0.283515\nEpoch [415/800], Batch [10/98], Loss: 5.2749, LR: 0.283515\nEpoch [415/800], Batch [20/98], Loss: 5.2758, LR: 0.283515\nEpoch [415/800], Batch [30/98], Loss: 5.2557, LR: 0.283515\nEpoch [415/800], Batch [40/98], Loss: 5.2619, LR: 0.283515\nEpoch [415/800], Batch [50/98], Loss: 5.2944, LR: 0.283515\nEpoch [415/800], Batch [60/98], Loss: 5.2905, LR: 0.283515\nEpoch [415/800], Batch [70/98], Loss: 5.2841, LR: 0.283515\nEpoch [415/800], Batch [80/98], Loss: 5.2780, LR: 0.283515\nEpoch [415/800], Batch [90/98], Loss: 5.2963, LR: 0.283515\nEpoch [415/800] Average Loss: 5.2813\n\nEpoch [416/800], Batch [0/98], Loss: 5.2794, LR: 0.282339\nEpoch [416/800], Batch [10/98], Loss: 5.2921, LR: 0.282339\nEpoch [416/800], Batch [20/98], Loss: 5.3121, LR: 0.282339\nEpoch [416/800], Batch [30/98], Loss: 5.2695, LR: 0.282339\nEpoch [416/800], Batch [40/98], Loss: 5.2621, LR: 0.282339\nEpoch [416/800], Batch [50/98], Loss: 5.3069, LR: 0.282339\nEpoch [416/800], Batch [60/98], Loss: 5.2722, LR: 0.282339\nEpoch [416/800], Batch [70/98], Loss: 5.2931, LR: 0.282339\nEpoch [416/800], Batch [80/98], Loss: 5.2848, LR: 0.282339\nEpoch [416/800], Batch [90/98], Loss: 5.3122, LR: 0.282339\nEpoch [416/800] Average Loss: 5.2809\n\nEpoch [417/800], Batch [0/98], Loss: 5.2874, LR: 0.281163\nEpoch [417/800], Batch [10/98], Loss: 5.2690, LR: 0.281163\nEpoch [417/800], Batch [20/98], Loss: 5.3019, LR: 0.281163\nEpoch [417/800], Batch [30/98], Loss: 5.2852, LR: 0.281163\nEpoch [417/800], Batch [40/98], Loss: 5.2770, LR: 0.281163\nEpoch [417/800], Batch [50/98], Loss: 5.2562, LR: 0.281163\nEpoch [417/800], Batch [60/98], Loss: 5.2594, LR: 0.281163\nEpoch [417/800], Batch [70/98], Loss: 5.2896, LR: 0.281163\nEpoch [417/800], Batch [80/98], Loss: 5.2978, LR: 0.281163\nEpoch [417/800], Batch [90/98], Loss: 5.2692, LR: 0.281163\nEpoch [417/800] Average Loss: 5.2787\n\nEpoch [418/800], Batch [0/98], Loss: 5.2896, LR: 0.279987\nEpoch [418/800], Batch [10/98], Loss: 5.2824, LR: 0.279987\nEpoch [418/800], Batch [20/98], Loss: 5.2909, LR: 0.279987\nEpoch [418/800], Batch [30/98], Loss: 5.2926, LR: 0.279987\nEpoch [418/800], Batch [40/98], Loss: 5.2798, LR: 0.279987\nEpoch [418/800], Batch [50/98], Loss: 5.2874, LR: 0.279987\nEpoch [418/800], Batch [60/98], Loss: 5.2905, LR: 0.279987\nEpoch [418/800], Batch [70/98], Loss: 5.2716, LR: 0.279987\nEpoch [418/800], Batch [80/98], Loss: 5.2855, LR: 0.279987\nEpoch [418/800], Batch [90/98], Loss: 5.3082, LR: 0.279987\nEpoch [418/800] Average Loss: 5.2809\n\nEpoch [419/800], Batch [0/98], Loss: 5.2712, LR: 0.278812\nEpoch [419/800], Batch [10/98], Loss: 5.2821, LR: 0.278812\nEpoch [419/800], Batch [20/98], Loss: 5.2696, LR: 0.278812\nEpoch [419/800], Batch [30/98], Loss: 5.2828, LR: 0.278812\nEpoch [419/800], Batch [40/98], Loss: 5.2647, LR: 0.278812\nEpoch [419/800], Batch [50/98], Loss: 5.2888, LR: 0.278812\nEpoch [419/800], Batch [60/98], Loss: 5.2864, LR: 0.278812\nEpoch [419/800], Batch [70/98], Loss: 5.2819, LR: 0.278812\nEpoch [419/800], Batch [80/98], Loss: 5.2818, LR: 0.278812\nEpoch [419/800], Batch [90/98], Loss: 5.2760, LR: 0.278812\nEpoch [419/800] Average Loss: 5.2806\n\nEpoch [420/800], Batch [0/98], Loss: 5.2825, LR: 0.277637\nEpoch [420/800], Batch [10/98], Loss: 5.2661, LR: 0.277637\nEpoch [420/800], Batch [20/98], Loss: 5.2668, LR: 0.277637\nEpoch [420/800], Batch [30/98], Loss: 5.2642, LR: 0.277637\nEpoch [420/800], Batch [40/98], Loss: 5.2791, LR: 0.277637\nEpoch [420/800], Batch [50/98], Loss: 5.2743, LR: 0.277637\nEpoch [420/800], Batch [60/98], Loss: 5.2912, LR: 0.277637\nEpoch [420/800], Batch [70/98], Loss: 5.2949, LR: 0.277637\nEpoch [420/800], Batch [80/98], Loss: 5.2786, LR: 0.277637\nEpoch [420/800], Batch [90/98], Loss: 5.2877, LR: 0.277637\nEpoch [420/800] Average Loss: 5.2773\n\nEpoch [421/800], Batch [0/98], Loss: 5.2514, LR: 0.276462\nEpoch [421/800], Batch [10/98], Loss: 5.2770, LR: 0.276462\nEpoch [421/800], Batch [20/98], Loss: 5.3153, LR: 0.276462\nEpoch [421/800], Batch [30/98], Loss: 5.2704, LR: 0.276462\nEpoch [421/800], Batch [40/98], Loss: 5.2797, LR: 0.276462\nEpoch [421/800], Batch [50/98], Loss: 5.2529, LR: 0.276462\nEpoch [421/800], Batch [60/98], Loss: 5.2983, LR: 0.276462\nEpoch [421/800], Batch [70/98], Loss: 5.2877, LR: 0.276462\nEpoch [421/800], Batch [80/98], Loss: 5.2744, LR: 0.276462\nEpoch [421/800], Batch [90/98], Loss: 5.2499, LR: 0.276462\nEpoch [421/800] Average Loss: 5.2786\n\nEpoch [422/800], Batch [0/98], Loss: 5.2703, LR: 0.275288\nEpoch [422/800], Batch [10/98], Loss: 5.2738, LR: 0.275288\nEpoch [422/800], Batch [20/98], Loss: 5.2540, LR: 0.275288\nEpoch [422/800], Batch [30/98], Loss: 5.2830, LR: 0.275288\nEpoch [422/800], Batch [40/98], Loss: 5.3022, LR: 0.275288\nEpoch [422/800], Batch [50/98], Loss: 5.2824, LR: 0.275288\nEpoch [422/800], Batch [60/98], Loss: 5.2613, LR: 0.275288\nEpoch [422/800], Batch [70/98], Loss: 5.2643, LR: 0.275288\nEpoch [422/800], Batch [80/98], Loss: 5.2911, LR: 0.275288\nEpoch [422/800], Batch [90/98], Loss: 5.2766, LR: 0.275288\nEpoch [422/800] Average Loss: 5.2754\n\nEpoch [423/800], Batch [0/98], Loss: 5.2842, LR: 0.274114\nEpoch [423/800], Batch [10/98], Loss: 5.3072, LR: 0.274114\nEpoch [423/800], Batch [20/98], Loss: 5.2715, LR: 0.274114\nEpoch [423/800], Batch [30/98], Loss: 5.2912, LR: 0.274114\nEpoch [423/800], Batch [40/98], Loss: 5.3043, LR: 0.274114\nEpoch [423/800], Batch [50/98], Loss: 5.3149, LR: 0.274114\nEpoch [423/800], Batch [60/98], Loss: 5.2590, LR: 0.274114\nEpoch [423/800], Batch [70/98], Loss: 5.2973, LR: 0.274114\nEpoch [423/800], Batch [80/98], Loss: 5.3024, LR: 0.274114\nEpoch [423/800], Batch [90/98], Loss: 5.2836, LR: 0.274114\nEpoch [423/800] Average Loss: 5.2765\n\nEpoch [424/800], Batch [0/98], Loss: 5.2761, LR: 0.272941\nEpoch [424/800], Batch [10/98], Loss: 5.2940, LR: 0.272941\nEpoch [424/800], Batch [20/98], Loss: 5.2773, LR: 0.272941\nEpoch [424/800], Batch [30/98], Loss: 5.2691, LR: 0.272941\nEpoch [424/800], Batch [40/98], Loss: 5.2668, LR: 0.272941\nEpoch [424/800], Batch [50/98], Loss: 5.2615, LR: 0.272941\nEpoch [424/800], Batch [60/98], Loss: 5.2772, LR: 0.272941\nEpoch [424/800], Batch [70/98], Loss: 5.2843, LR: 0.272941\nEpoch [424/800], Batch [80/98], Loss: 5.3057, LR: 0.272941\nEpoch [424/800], Batch [90/98], Loss: 5.3010, LR: 0.272941\nEpoch [424/800] Average Loss: 5.2782\n\nEpoch [425/800], Batch [0/98], Loss: 5.2782, LR: 0.271768\nEpoch [425/800], Batch [10/98], Loss: 5.2745, LR: 0.271768\nEpoch [425/800], Batch [20/98], Loss: 5.3005, LR: 0.271768\nEpoch [425/800], Batch [30/98], Loss: 5.2866, LR: 0.271768\nEpoch [425/800], Batch [40/98], Loss: 5.2770, LR: 0.271768\nEpoch [425/800], Batch [50/98], Loss: 5.2659, LR: 0.271768\nEpoch [425/800], Batch [60/98], Loss: 5.2776, LR: 0.271768\nEpoch [425/800], Batch [70/98], Loss: 5.2790, LR: 0.271768\nEpoch [425/800], Batch [80/98], Loss: 5.3061, LR: 0.271768\nEpoch [425/800], Batch [90/98], Loss: 5.2919, LR: 0.271768\nEpoch [425/800] Average Loss: 5.2773\n\nEpoch [426/800], Batch [0/98], Loss: 5.2704, LR: 0.270595\nEpoch [426/800], Batch [10/98], Loss: 5.2875, LR: 0.270595\nEpoch [426/800], Batch [20/98], Loss: 5.2715, LR: 0.270595\nEpoch [426/800], Batch [30/98], Loss: 5.2765, LR: 0.270595\nEpoch [426/800], Batch [40/98], Loss: 5.3061, LR: 0.270595\nEpoch [426/800], Batch [50/98], Loss: 5.2767, LR: 0.270595\nEpoch [426/800], Batch [60/98], Loss: 5.2897, LR: 0.270595\nEpoch [426/800], Batch [70/98], Loss: 5.2546, LR: 0.270595\nEpoch [426/800], Batch [80/98], Loss: 5.2746, LR: 0.270595\nEpoch [426/800], Batch [90/98], Loss: 5.2919, LR: 0.270595\nEpoch [426/800] Average Loss: 5.2766\n\nEpoch [427/800], Batch [0/98], Loss: 5.2761, LR: 0.269423\nEpoch [427/800], Batch [10/98], Loss: 5.2775, LR: 0.269423\nEpoch [427/800], Batch [20/98], Loss: 5.2592, LR: 0.269423\nEpoch [427/800], Batch [30/98], Loss: 5.2952, LR: 0.269423\nEpoch [427/800], Batch [40/98], Loss: 5.3150, LR: 0.269423\nEpoch [427/800], Batch [50/98], Loss: 5.2879, LR: 0.269423\nEpoch [427/800], Batch [60/98], Loss: 5.2849, LR: 0.269423\nEpoch [427/800], Batch [70/98], Loss: 5.3241, LR: 0.269423\nEpoch [427/800], Batch [80/98], Loss: 5.2773, LR: 0.269423\nEpoch [427/800], Batch [90/98], Loss: 5.2802, LR: 0.269423\nEpoch [427/800] Average Loss: 5.2776\n\nEpoch [428/800], Batch [0/98], Loss: 5.2616, LR: 0.268251\nEpoch [428/800], Batch [10/98], Loss: 5.2899, LR: 0.268251\nEpoch [428/800], Batch [20/98], Loss: 5.2763, LR: 0.268251\nEpoch [428/800], Batch [30/98], Loss: 5.2627, LR: 0.268251\nEpoch [428/800], Batch [40/98], Loss: 5.2818, LR: 0.268251\nEpoch [428/800], Batch [50/98], Loss: 5.2797, LR: 0.268251\nEpoch [428/800], Batch [60/98], Loss: 5.2792, LR: 0.268251\nEpoch [428/800], Batch [70/98], Loss: 5.2882, LR: 0.268251\nEpoch [428/800], Batch [80/98], Loss: 5.3048, LR: 0.268251\nEpoch [428/800], Batch [90/98], Loss: 5.2826, LR: 0.268251\nEpoch [428/800] Average Loss: 5.2766\n\nEpoch [429/800], Batch [0/98], Loss: 5.2819, LR: 0.267080\nEpoch [429/800], Batch [10/98], Loss: 5.2737, LR: 0.267080\nEpoch [429/800], Batch [20/98], Loss: 5.3013, LR: 0.267080\nEpoch [429/800], Batch [30/98], Loss: 5.2622, LR: 0.267080\nEpoch [429/800], Batch [40/98], Loss: 5.2804, LR: 0.267080\nEpoch [429/800], Batch [50/98], Loss: 5.2845, LR: 0.267080\nEpoch [429/800], Batch [60/98], Loss: 5.2782, LR: 0.267080\nEpoch [429/800], Batch [70/98], Loss: 5.2933, LR: 0.267080\nEpoch [429/800], Batch [80/98], Loss: 5.2773, LR: 0.267080\nEpoch [429/800], Batch [90/98], Loss: 5.2539, LR: 0.267080\nEpoch [429/800] Average Loss: 5.2770\n\nEpoch [430/800], Batch [0/98], Loss: 5.2598, LR: 0.265909\nEpoch [430/800], Batch [10/98], Loss: 5.2788, LR: 0.265909\nEpoch [430/800], Batch [20/98], Loss: 5.2656, LR: 0.265909\nEpoch [430/800], Batch [30/98], Loss: 5.2805, LR: 0.265909\nEpoch [430/800], Batch [40/98], Loss: 5.2812, LR: 0.265909\nEpoch [430/800], Batch [50/98], Loss: 5.2725, LR: 0.265909\nEpoch [430/800], Batch [60/98], Loss: 5.2724, LR: 0.265909\nEpoch [430/800], Batch [70/98], Loss: 5.2719, LR: 0.265909\nEpoch [430/800], Batch [80/98], Loss: 5.2643, LR: 0.265909\nEpoch [430/800], Batch [90/98], Loss: 5.3110, LR: 0.265909\nEpoch [430/800] Average Loss: 5.2754\n\nEpoch [431/800], Batch [0/98], Loss: 5.2707, LR: 0.264739\nEpoch [431/800], Batch [10/98], Loss: 5.2886, LR: 0.264739\nEpoch [431/800], Batch [20/98], Loss: 5.2900, LR: 0.264739\nEpoch [431/800], Batch [30/98], Loss: 5.2978, LR: 0.264739\nEpoch [431/800], Batch [40/98], Loss: 5.2564, LR: 0.264739\nEpoch [431/800], Batch [50/98], Loss: 5.2732, LR: 0.264739\nEpoch [431/800], Batch [60/98], Loss: 5.2781, LR: 0.264739\nEpoch [431/800], Batch [70/98], Loss: 5.2586, LR: 0.264739\nEpoch [431/800], Batch [80/98], Loss: 5.3080, LR: 0.264739\nEpoch [431/800], Batch [90/98], Loss: 5.2997, LR: 0.264739\nEpoch [431/800] Average Loss: 5.2759\n\nEpoch [432/800], Batch [0/98], Loss: 5.2812, LR: 0.263569\nEpoch [432/800], Batch [10/98], Loss: 5.2805, LR: 0.263569\nEpoch [432/800], Batch [20/98], Loss: 5.2832, LR: 0.263569\nEpoch [432/800], Batch [30/98], Loss: 5.2986, LR: 0.263569\nEpoch [432/800], Batch [40/98], Loss: 5.2919, LR: 0.263569\nEpoch [432/800], Batch [50/98], Loss: 5.2952, LR: 0.263569\nEpoch [432/800], Batch [60/98], Loss: 5.2773, LR: 0.263569\nEpoch [432/800], Batch [70/98], Loss: 5.2792, LR: 0.263569\nEpoch [432/800], Batch [80/98], Loss: 5.2926, LR: 0.263569\nEpoch [432/800], Batch [90/98], Loss: 5.2781, LR: 0.263569\nEpoch [432/800] Average Loss: 5.2772\n\nEpoch [433/800], Batch [0/98], Loss: 5.2655, LR: 0.262400\nEpoch [433/800], Batch [10/98], Loss: 5.2771, LR: 0.262400\nEpoch [433/800], Batch [20/98], Loss: 5.2864, LR: 0.262400\nEpoch [433/800], Batch [30/98], Loss: 5.2802, LR: 0.262400\nEpoch [433/800], Batch [40/98], Loss: 5.2789, LR: 0.262400\nEpoch [433/800], Batch [50/98], Loss: 5.2801, LR: 0.262400\nEpoch [433/800], Batch [60/98], Loss: 5.2756, LR: 0.262400\nEpoch [433/800], Batch [70/98], Loss: 5.2779, LR: 0.262400\nEpoch [433/800], Batch [80/98], Loss: 5.2698, LR: 0.262400\nEpoch [433/800], Batch [90/98], Loss: 5.2841, LR: 0.262400\nEpoch [433/800] Average Loss: 5.2761\n\nEpoch [434/800], Batch [0/98], Loss: 5.3081, LR: 0.261232\nEpoch [434/800], Batch [10/98], Loss: 5.2872, LR: 0.261232\nEpoch [434/800], Batch [20/98], Loss: 5.2589, LR: 0.261232\nEpoch [434/800], Batch [30/98], Loss: 5.3125, LR: 0.261232\nEpoch [434/800], Batch [40/98], Loss: 5.2831, LR: 0.261232\nEpoch [434/800], Batch [50/98], Loss: 5.3039, LR: 0.261232\nEpoch [434/800], Batch [60/98], Loss: 5.2848, LR: 0.261232\nEpoch [434/800], Batch [70/98], Loss: 5.2590, LR: 0.261232\nEpoch [434/800], Batch [80/98], Loss: 5.2867, LR: 0.261232\nEpoch [434/800], Batch [90/98], Loss: 5.2837, LR: 0.261232\nEpoch [434/800] Average Loss: 5.2760\n\nEpoch [435/800], Batch [0/98], Loss: 5.2635, LR: 0.260064\nEpoch [435/800], Batch [10/98], Loss: 5.2910, LR: 0.260064\nEpoch [435/800], Batch [20/98], Loss: 5.2937, LR: 0.260064\nEpoch [435/800], Batch [30/98], Loss: 5.2826, LR: 0.260064\nEpoch [435/800], Batch [40/98], Loss: 5.2869, LR: 0.260064\nEpoch [435/800], Batch [50/98], Loss: 5.2703, LR: 0.260064\nEpoch [435/800], Batch [60/98], Loss: 5.2679, LR: 0.260064\nEpoch [435/800], Batch [70/98], Loss: 5.2774, LR: 0.260064\nEpoch [435/800], Batch [80/98], Loss: 5.2890, LR: 0.260064\nEpoch [435/800], Batch [90/98], Loss: 5.2573, LR: 0.260064\nEpoch [435/800] Average Loss: 5.2774\n\nEpoch [436/800], Batch [0/98], Loss: 5.2697, LR: 0.258896\nEpoch [436/800], Batch [10/98], Loss: 5.2671, LR: 0.258896\nEpoch [436/800], Batch [20/98], Loss: 5.2923, LR: 0.258896\nEpoch [436/800], Batch [30/98], Loss: 5.2935, LR: 0.258896\nEpoch [436/800], Batch [40/98], Loss: 5.3046, LR: 0.258896\nEpoch [436/800], Batch [50/98], Loss: 5.2894, LR: 0.258896\nEpoch [436/800], Batch [60/98], Loss: 5.2680, LR: 0.258896\nEpoch [436/800], Batch [70/98], Loss: 5.2693, LR: 0.258896\nEpoch [436/800], Batch [80/98], Loss: 5.2956, LR: 0.258896\nEpoch [436/800], Batch [90/98], Loss: 5.2820, LR: 0.258896\nEpoch [436/800] Average Loss: 5.2745\n\nEpoch [437/800], Batch [0/98], Loss: 5.2732, LR: 0.257730\nEpoch [437/800], Batch [10/98], Loss: 5.2634, LR: 0.257730\nEpoch [437/800], Batch [20/98], Loss: 5.2927, LR: 0.257730\nEpoch [437/800], Batch [30/98], Loss: 5.2856, LR: 0.257730\nEpoch [437/800], Batch [40/98], Loss: 5.2623, LR: 0.257730\nEpoch [437/800], Batch [50/98], Loss: 5.2620, LR: 0.257730\nEpoch [437/800], Batch [60/98], Loss: 5.2699, LR: 0.257730\nEpoch [437/800], Batch [70/98], Loss: 5.2713, LR: 0.257730\nEpoch [437/800], Batch [80/98], Loss: 5.3072, LR: 0.257730\nEpoch [437/800], Batch [90/98], Loss: 5.2775, LR: 0.257730\nEpoch [437/800] Average Loss: 5.2750\n\nEpoch [438/800], Batch [0/98], Loss: 5.2703, LR: 0.256564\nEpoch [438/800], Batch [10/98], Loss: 5.2747, LR: 0.256564\nEpoch [438/800], Batch [20/98], Loss: 5.2695, LR: 0.256564\nEpoch [438/800], Batch [30/98], Loss: 5.2772, LR: 0.256564\nEpoch [438/800], Batch [40/98], Loss: 5.2693, LR: 0.256564\nEpoch [438/800], Batch [50/98], Loss: 5.2833, LR: 0.256564\nEpoch [438/800], Batch [60/98], Loss: 5.2703, LR: 0.256564\nEpoch [438/800], Batch [70/98], Loss: 5.2724, LR: 0.256564\nEpoch [438/800], Batch [80/98], Loss: 5.2632, LR: 0.256564\nEpoch [438/800], Batch [90/98], Loss: 5.2777, LR: 0.256564\nEpoch [438/800] Average Loss: 5.2723\n\nEpoch [439/800], Batch [0/98], Loss: 5.2805, LR: 0.255398\nEpoch [439/800], Batch [10/98], Loss: 5.2673, LR: 0.255398\nEpoch [439/800], Batch [20/98], Loss: 5.2848, LR: 0.255398\nEpoch [439/800], Batch [30/98], Loss: 5.2804, LR: 0.255398\nEpoch [439/800], Batch [40/98], Loss: 5.2767, LR: 0.255398\nEpoch [439/800], Batch [50/98], Loss: 5.2600, LR: 0.255398\nEpoch [439/800], Batch [60/98], Loss: 5.2902, LR: 0.255398\nEpoch [439/800], Batch [70/98], Loss: 5.2722, LR: 0.255398\nEpoch [439/800], Batch [80/98], Loss: 5.2707, LR: 0.255398\nEpoch [439/800], Batch [90/98], Loss: 5.2661, LR: 0.255398\nEpoch [439/800] Average Loss: 5.2755\n\nEpoch [440/800], Batch [0/98], Loss: 5.2765, LR: 0.254234\nEpoch [440/800], Batch [10/98], Loss: 5.2687, LR: 0.254234\nEpoch [440/800], Batch [20/98], Loss: 5.2941, LR: 0.254234\nEpoch [440/800], Batch [30/98], Loss: 5.3047, LR: 0.254234\nEpoch [440/800], Batch [40/98], Loss: 5.2832, LR: 0.254234\nEpoch [440/800], Batch [50/98], Loss: 5.2909, LR: 0.254234\nEpoch [440/800], Batch [60/98], Loss: 5.2740, LR: 0.254234\nEpoch [440/800], Batch [70/98], Loss: 5.2908, LR: 0.254234\nEpoch [440/800], Batch [80/98], Loss: 5.3098, LR: 0.254234\nEpoch [440/800], Batch [90/98], Loss: 5.2765, LR: 0.254234\nEpoch [440/800] Average Loss: 5.2762\n\nEpoch [441/800], Batch [0/98], Loss: 5.2836, LR: 0.253070\nEpoch [441/800], Batch [10/98], Loss: 5.3010, LR: 0.253070\nEpoch [441/800], Batch [20/98], Loss: 5.2945, LR: 0.253070\nEpoch [441/800], Batch [30/98], Loss: 5.2353, LR: 0.253070\nEpoch [441/800], Batch [40/98], Loss: 5.2885, LR: 0.253070\nEpoch [441/800], Batch [50/98], Loss: 5.2802, LR: 0.253070\nEpoch [441/800], Batch [60/98], Loss: 5.2815, LR: 0.253070\nEpoch [441/800], Batch [70/98], Loss: 5.2728, LR: 0.253070\nEpoch [441/800], Batch [80/98], Loss: 5.2702, LR: 0.253070\nEpoch [441/800], Batch [90/98], Loss: 5.2862, LR: 0.253070\nEpoch [441/800] Average Loss: 5.2745\n\nEpoch [442/800], Batch [0/98], Loss: 5.3038, LR: 0.251906\nEpoch [442/800], Batch [10/98], Loss: 5.2725, LR: 0.251906\nEpoch [442/800], Batch [20/98], Loss: 5.2843, LR: 0.251906\nEpoch [442/800], Batch [30/98], Loss: 5.2808, LR: 0.251906\nEpoch [442/800], Batch [40/98], Loss: 5.3007, LR: 0.251906\nEpoch [442/800], Batch [50/98], Loss: 5.2716, LR: 0.251906\nEpoch [442/800], Batch [60/98], Loss: 5.2671, LR: 0.251906\nEpoch [442/800], Batch [70/98], Loss: 5.2780, LR: 0.251906\nEpoch [442/800], Batch [80/98], Loss: 5.2719, LR: 0.251906\nEpoch [442/800], Batch [90/98], Loss: 5.2800, LR: 0.251906\nEpoch [442/800] Average Loss: 5.2742\n\nEpoch [443/800], Batch [0/98], Loss: 5.2634, LR: 0.250744\nEpoch [443/800], Batch [10/98], Loss: 5.2843, LR: 0.250744\nEpoch [443/800], Batch [20/98], Loss: 5.2546, LR: 0.250744\nEpoch [443/800], Batch [30/98], Loss: 5.2846, LR: 0.250744\nEpoch [443/800], Batch [40/98], Loss: 5.2751, LR: 0.250744\nEpoch [443/800], Batch [50/98], Loss: 5.2706, LR: 0.250744\nEpoch [443/800], Batch [60/98], Loss: 5.2811, LR: 0.250744\nEpoch [443/800], Batch [70/98], Loss: 5.2870, LR: 0.250744\nEpoch [443/800], Batch [80/98], Loss: 5.2921, LR: 0.250744\nEpoch [443/800], Batch [90/98], Loss: 5.2874, LR: 0.250744\nEpoch [443/800] Average Loss: 5.2739\n\nEpoch [444/800], Batch [0/98], Loss: 5.2810, LR: 0.249582\nEpoch [444/800], Batch [10/98], Loss: 5.2705, LR: 0.249582\nEpoch [444/800], Batch [20/98], Loss: 5.2705, LR: 0.249582\nEpoch [444/800], Batch [30/98], Loss: 5.3024, LR: 0.249582\nEpoch [444/800], Batch [40/98], Loss: 5.2763, LR: 0.249582\nEpoch [444/800], Batch [50/98], Loss: 5.2823, LR: 0.249582\nEpoch [444/800], Batch [60/98], Loss: 5.2713, LR: 0.249582\nEpoch [444/800], Batch [70/98], Loss: 5.2778, LR: 0.249582\nEpoch [444/800], Batch [80/98], Loss: 5.2697, LR: 0.249582\nEpoch [444/800], Batch [90/98], Loss: 5.2761, LR: 0.249582\nEpoch [444/800] Average Loss: 5.2709\n\nEpoch [445/800], Batch [0/98], Loss: 5.2832, LR: 0.248421\nEpoch [445/800], Batch [10/98], Loss: 5.2692, LR: 0.248421\nEpoch [445/800], Batch [20/98], Loss: 5.2912, LR: 0.248421\nEpoch [445/800], Batch [30/98], Loss: 5.2634, LR: 0.248421\nEpoch [445/800], Batch [40/98], Loss: 5.2979, LR: 0.248421\nEpoch [445/800], Batch [50/98], Loss: 5.2816, LR: 0.248421\nEpoch [445/800], Batch [60/98], Loss: 5.2652, LR: 0.248421\nEpoch [445/800], Batch [70/98], Loss: 5.2700, LR: 0.248421\nEpoch [445/800], Batch [80/98], Loss: 5.2695, LR: 0.248421\nEpoch [445/800], Batch [90/98], Loss: 5.2866, LR: 0.248421\nEpoch [445/800] Average Loss: 5.2729\n\nEpoch [446/800], Batch [0/98], Loss: 5.2796, LR: 0.247261\nEpoch [446/800], Batch [10/98], Loss: 5.3087, LR: 0.247261\nEpoch [446/800], Batch [20/98], Loss: 5.2796, LR: 0.247261\nEpoch [446/800], Batch [30/98], Loss: 5.2724, LR: 0.247261\nEpoch [446/800], Batch [40/98], Loss: 5.2873, LR: 0.247261\nEpoch [446/800], Batch [50/98], Loss: 5.2625, LR: 0.247261\nEpoch [446/800], Batch [60/98], Loss: 5.3045, LR: 0.247261\nEpoch [446/800], Batch [70/98], Loss: 5.2802, LR: 0.247261\nEpoch [446/800], Batch [80/98], Loss: 5.3038, LR: 0.247261\nEpoch [446/800], Batch [90/98], Loss: 5.2993, LR: 0.247261\nEpoch [446/800] Average Loss: 5.2729\n\nEpoch [447/800], Batch [0/98], Loss: 5.2811, LR: 0.246102\nEpoch [447/800], Batch [10/98], Loss: 5.2592, LR: 0.246102\nEpoch [447/800], Batch [20/98], Loss: 5.3051, LR: 0.246102\nEpoch [447/800], Batch [30/98], Loss: 5.2994, LR: 0.246102\nEpoch [447/800], Batch [40/98], Loss: 5.2814, LR: 0.246102\nEpoch [447/800], Batch [50/98], Loss: 5.2535, LR: 0.246102\nEpoch [447/800], Batch [60/98], Loss: 5.2847, LR: 0.246102\nEpoch [447/800], Batch [70/98], Loss: 5.2618, LR: 0.246102\nEpoch [447/800], Batch [80/98], Loss: 5.2916, LR: 0.246102\nEpoch [447/800], Batch [90/98], Loss: 5.2728, LR: 0.246102\nEpoch [447/800] Average Loss: 5.2728\n\nEpoch [448/800], Batch [0/98], Loss: 5.2619, LR: 0.244943\nEpoch [448/800], Batch [10/98], Loss: 5.2914, LR: 0.244943\nEpoch [448/800], Batch [20/98], Loss: 5.2622, LR: 0.244943\nEpoch [448/800], Batch [30/98], Loss: 5.2815, LR: 0.244943\nEpoch [448/800], Batch [40/98], Loss: 5.2917, LR: 0.244943\nEpoch [448/800], Batch [50/98], Loss: 5.3010, LR: 0.244943\nEpoch [448/800], Batch [60/98], Loss: 5.2908, LR: 0.244943\nEpoch [448/800], Batch [70/98], Loss: 5.2721, LR: 0.244943\nEpoch [448/800], Batch [80/98], Loss: 5.2728, LR: 0.244943\nEpoch [448/800], Batch [90/98], Loss: 5.2682, LR: 0.244943\nEpoch [448/800] Average Loss: 5.2761\n\nEpoch [449/800], Batch [0/98], Loss: 5.2838, LR: 0.243786\nEpoch [449/800], Batch [10/98], Loss: 5.2689, LR: 0.243786\nEpoch [449/800], Batch [20/98], Loss: 5.2859, LR: 0.243786\nEpoch [449/800], Batch [30/98], Loss: 5.2942, LR: 0.243786\nEpoch [449/800], Batch [40/98], Loss: 5.2677, LR: 0.243786\nEpoch [449/800], Batch [50/98], Loss: 5.2778, LR: 0.243786\nEpoch [449/800], Batch [60/98], Loss: 5.2791, LR: 0.243786\nEpoch [449/800], Batch [70/98], Loss: 5.2698, LR: 0.243786\nEpoch [449/800], Batch [80/98], Loss: 5.2755, LR: 0.243786\nEpoch [449/800], Batch [90/98], Loss: 5.2844, LR: 0.243786\nEpoch [449/800] Average Loss: 5.2733\n\nEpoch [450/800], Batch [0/98], Loss: 5.2875, LR: 0.242629\nEpoch [450/800], Batch [10/98], Loss: 5.2675, LR: 0.242629\nEpoch [450/800], Batch [20/98], Loss: 5.2830, LR: 0.242629\nEpoch [450/800], Batch [30/98], Loss: 5.2859, LR: 0.242629\nEpoch [450/800], Batch [40/98], Loss: 5.2660, LR: 0.242629\nEpoch [450/800], Batch [50/98], Loss: 5.2615, LR: 0.242629\nEpoch [450/800], Batch [60/98], Loss: 5.2702, LR: 0.242629\nEpoch [450/800], Batch [70/98], Loss: 5.2504, LR: 0.242629\nEpoch [450/800], Batch [80/98], Loss: 5.2822, LR: 0.242629\nEpoch [450/800], Batch [90/98], Loss: 5.2646, LR: 0.242629\nEpoch [450/800] Average Loss: 5.2703\n\nEpoch [451/800], Batch [0/98], Loss: 5.2760, LR: 0.241473\nEpoch [451/800], Batch [10/98], Loss: 5.2627, LR: 0.241473\nEpoch [451/800], Batch [20/98], Loss: 5.2917, LR: 0.241473\nEpoch [451/800], Batch [30/98], Loss: 5.2811, LR: 0.241473\nEpoch [451/800], Batch [40/98], Loss: 5.2669, LR: 0.241473\nEpoch [451/800], Batch [50/98], Loss: 5.2507, LR: 0.241473\nEpoch [451/800], Batch [60/98], Loss: 5.2641, LR: 0.241473\nEpoch [451/800], Batch [70/98], Loss: 5.2980, LR: 0.241473\nEpoch [451/800], Batch [80/98], Loss: 5.2853, LR: 0.241473\nEpoch [451/800], Batch [90/98], Loss: 5.2781, LR: 0.241473\nEpoch [451/800] Average Loss: 5.2698\n\nEpoch [452/800], Batch [0/98], Loss: 5.2786, LR: 0.240318\nEpoch [452/800], Batch [10/98], Loss: 5.2403, LR: 0.240318\nEpoch [452/800], Batch [20/98], Loss: 5.2646, LR: 0.240318\nEpoch [452/800], Batch [30/98], Loss: 5.2832, LR: 0.240318\nEpoch [452/800], Batch [40/98], Loss: 5.2995, LR: 0.240318\nEpoch [452/800], Batch [50/98], Loss: 5.2746, LR: 0.240318\nEpoch [452/800], Batch [60/98], Loss: 5.2771, LR: 0.240318\nEpoch [452/800], Batch [70/98], Loss: 5.2844, LR: 0.240318\nEpoch [452/800], Batch [80/98], Loss: 5.2889, LR: 0.240318\nEpoch [452/800], Batch [90/98], Loss: 5.2631, LR: 0.240318\nEpoch [452/800] Average Loss: 5.2719\n\nEpoch [453/800], Batch [0/98], Loss: 5.2479, LR: 0.239164\nEpoch [453/800], Batch [10/98], Loss: 5.2764, LR: 0.239164\nEpoch [453/800], Batch [20/98], Loss: 5.2724, LR: 0.239164\nEpoch [453/800], Batch [30/98], Loss: 5.2654, LR: 0.239164\nEpoch [453/800], Batch [40/98], Loss: 5.2693, LR: 0.239164\nEpoch [453/800], Batch [50/98], Loss: 5.2942, LR: 0.239164\nEpoch [453/800], Batch [60/98], Loss: 5.2848, LR: 0.239164\nEpoch [453/800], Batch [70/98], Loss: 5.2949, LR: 0.239164\nEpoch [453/800], Batch [80/98], Loss: 5.2679, LR: 0.239164\nEpoch [453/800], Batch [90/98], Loss: 5.3118, LR: 0.239164\nEpoch [453/800] Average Loss: 5.2701\n\nEpoch [454/800], Batch [0/98], Loss: 5.2806, LR: 0.238011\nEpoch [454/800], Batch [10/98], Loss: 5.2961, LR: 0.238011\nEpoch [454/800], Batch [20/98], Loss: 5.2458, LR: 0.238011\nEpoch [454/800], Batch [30/98], Loss: 5.2864, LR: 0.238011\nEpoch [454/800], Batch [40/98], Loss: 5.2724, LR: 0.238011\nEpoch [454/800], Batch [50/98], Loss: 5.2671, LR: 0.238011\nEpoch [454/800], Batch [60/98], Loss: 5.2785, LR: 0.238011\nEpoch [454/800], Batch [70/98], Loss: 5.2769, LR: 0.238011\nEpoch [454/800], Batch [80/98], Loss: 5.2797, LR: 0.238011\nEpoch [454/800], Batch [90/98], Loss: 5.2857, LR: 0.238011\nEpoch [454/800] Average Loss: 5.2724\n\nEpoch [455/800], Batch [0/98], Loss: 5.2685, LR: 0.236858\nEpoch [455/800], Batch [10/98], Loss: 5.2608, LR: 0.236858\nEpoch [455/800], Batch [20/98], Loss: 5.2639, LR: 0.236858\nEpoch [455/800], Batch [30/98], Loss: 5.2683, LR: 0.236858\nEpoch [455/800], Batch [40/98], Loss: 5.2414, LR: 0.236858\nEpoch [455/800], Batch [50/98], Loss: 5.2933, LR: 0.236858\nEpoch [455/800], Batch [60/98], Loss: 5.2973, LR: 0.236858\nEpoch [455/800], Batch [70/98], Loss: 5.2833, LR: 0.236858\nEpoch [455/800], Batch [80/98], Loss: 5.2645, LR: 0.236858\nEpoch [455/800], Batch [90/98], Loss: 5.2696, LR: 0.236858\nEpoch [455/800] Average Loss: 5.2711\n\nEpoch [456/800], Batch [0/98], Loss: 5.2795, LR: 0.235707\nEpoch [456/800], Batch [10/98], Loss: 5.2583, LR: 0.235707\nEpoch [456/800], Batch [20/98], Loss: 5.2736, LR: 0.235707\nEpoch [456/800], Batch [30/98], Loss: 5.2784, LR: 0.235707\nEpoch [456/800], Batch [40/98], Loss: 5.2926, LR: 0.235707\nEpoch [456/800], Batch [50/98], Loss: 5.2762, LR: 0.235707\nEpoch [456/800], Batch [60/98], Loss: 5.2722, LR: 0.235707\nEpoch [456/800], Batch [70/98], Loss: 5.2825, LR: 0.235707\nEpoch [456/800], Batch [80/98], Loss: 5.2785, LR: 0.235707\nEpoch [456/800], Batch [90/98], Loss: 5.2675, LR: 0.235707\nEpoch [456/800] Average Loss: 5.2714\n\nEpoch [457/800], Batch [0/98], Loss: 5.2784, LR: 0.234557\nEpoch [457/800], Batch [10/98], Loss: 5.2830, LR: 0.234557\nEpoch [457/800], Batch [20/98], Loss: 5.2665, LR: 0.234557\nEpoch [457/800], Batch [30/98], Loss: 5.2964, LR: 0.234557\nEpoch [457/800], Batch [40/98], Loss: 5.2857, LR: 0.234557\nEpoch [457/800], Batch [50/98], Loss: 5.2735, LR: 0.234557\nEpoch [457/800], Batch [60/98], Loss: 5.2849, LR: 0.234557\nEpoch [457/800], Batch [70/98], Loss: 5.2801, LR: 0.234557\nEpoch [457/800], Batch [80/98], Loss: 5.2605, LR: 0.234557\nEpoch [457/800], Batch [90/98], Loss: 5.2719, LR: 0.234557\nEpoch [457/800] Average Loss: 5.2730\n\nEpoch [458/800], Batch [0/98], Loss: 5.2696, LR: 0.233408\nEpoch [458/800], Batch [10/98], Loss: 5.2582, LR: 0.233408\nEpoch [458/800], Batch [20/98], Loss: 5.2852, LR: 0.233408\nEpoch [458/800], Batch [30/98], Loss: 5.2653, LR: 0.233408\nEpoch [458/800], Batch [40/98], Loss: 5.2721, LR: 0.233408\nEpoch [458/800], Batch [50/98], Loss: 5.2888, LR: 0.233408\nEpoch [458/800], Batch [60/98], Loss: 5.2924, LR: 0.233408\nEpoch [458/800], Batch [70/98], Loss: 5.2681, LR: 0.233408\nEpoch [458/800], Batch [80/98], Loss: 5.2786, LR: 0.233408\nEpoch [458/800], Batch [90/98], Loss: 5.2919, LR: 0.233408\nEpoch [458/800] Average Loss: 5.2713\n\nEpoch [459/800], Batch [0/98], Loss: 5.2779, LR: 0.232260\nEpoch [459/800], Batch [10/98], Loss: 5.2715, LR: 0.232260\nEpoch [459/800], Batch [20/98], Loss: 5.2838, LR: 0.232260\nEpoch [459/800], Batch [30/98], Loss: 5.2615, LR: 0.232260\nEpoch [459/800], Batch [40/98], Loss: 5.2656, LR: 0.232260\nEpoch [459/800], Batch [50/98], Loss: 5.2700, LR: 0.232260\nEpoch [459/800], Batch [60/98], Loss: 5.2654, LR: 0.232260\nEpoch [459/800], Batch [70/98], Loss: 5.2846, LR: 0.232260\nEpoch [459/800], Batch [80/98], Loss: 5.2835, LR: 0.232260\nEpoch [459/800], Batch [90/98], Loss: 5.2972, LR: 0.232260\nEpoch [459/800] Average Loss: 5.2708\n\nEpoch [460/800], Batch [0/98], Loss: 5.2577, LR: 0.231112\nEpoch [460/800], Batch [10/98], Loss: 5.2980, LR: 0.231112\nEpoch [460/800], Batch [20/98], Loss: 5.2669, LR: 0.231112\nEpoch [460/800], Batch [30/98], Loss: 5.2749, LR: 0.231112\nEpoch [460/800], Batch [40/98], Loss: 5.2615, LR: 0.231112\nEpoch [460/800], Batch [50/98], Loss: 5.2809, LR: 0.231112\nEpoch [460/800], Batch [60/98], Loss: 5.2838, LR: 0.231112\nEpoch [460/800], Batch [70/98], Loss: 5.2728, LR: 0.231112\nEpoch [460/800], Batch [80/98], Loss: 5.2729, LR: 0.231112\nEpoch [460/800], Batch [90/98], Loss: 5.2790, LR: 0.231112\nEpoch [460/800] Average Loss: 5.2703\n\nEpoch [461/800], Batch [0/98], Loss: 5.2712, LR: 0.229966\nEpoch [461/800], Batch [10/98], Loss: 5.2513, LR: 0.229966\nEpoch [461/800], Batch [20/98], Loss: 5.2718, LR: 0.229966\nEpoch [461/800], Batch [30/98], Loss: 5.2638, LR: 0.229966\nEpoch [461/800], Batch [40/98], Loss: 5.2675, LR: 0.229966\nEpoch [461/800], Batch [50/98], Loss: 5.2566, LR: 0.229966\nEpoch [461/800], Batch [60/98], Loss: 5.2541, LR: 0.229966\nEpoch [461/800], Batch [70/98], Loss: 5.2751, LR: 0.229966\nEpoch [461/800], Batch [80/98], Loss: 5.2778, LR: 0.229966\nEpoch [461/800], Batch [90/98], Loss: 5.2775, LR: 0.229966\nEpoch [461/800] Average Loss: 5.2706\n\nEpoch [462/800], Batch [0/98], Loss: 5.2620, LR: 0.228821\nEpoch [462/800], Batch [10/98], Loss: 5.2660, LR: 0.228821\nEpoch [462/800], Batch [20/98], Loss: 5.2795, LR: 0.228821\nEpoch [462/800], Batch [30/98], Loss: 5.2706, LR: 0.228821\nEpoch [462/800], Batch [40/98], Loss: 5.2859, LR: 0.228821\nEpoch [462/800], Batch [50/98], Loss: 5.3037, LR: 0.228821\nEpoch [462/800], Batch [60/98], Loss: 5.2670, LR: 0.228821\nEpoch [462/800], Batch [70/98], Loss: 5.2762, LR: 0.228821\nEpoch [462/800], Batch [80/98], Loss: 5.2561, LR: 0.228821\nEpoch [462/800], Batch [90/98], Loss: 5.2910, LR: 0.228821\nEpoch [462/800] Average Loss: 5.2687\n\nEpoch [463/800], Batch [0/98], Loss: 5.2650, LR: 0.227677\nEpoch [463/800], Batch [10/98], Loss: 5.2600, LR: 0.227677\nEpoch [463/800], Batch [20/98], Loss: 5.2863, LR: 0.227677\nEpoch [463/800], Batch [30/98], Loss: 5.2841, LR: 0.227677\nEpoch [463/800], Batch [40/98], Loss: 5.2631, LR: 0.227677\nEpoch [463/800], Batch [50/98], Loss: 5.2588, LR: 0.227677\nEpoch [463/800], Batch [60/98], Loss: 5.2643, LR: 0.227677\nEpoch [463/800], Batch [70/98], Loss: 5.2576, LR: 0.227677\nEpoch [463/800], Batch [80/98], Loss: 5.2806, LR: 0.227677\nEpoch [463/800], Batch [90/98], Loss: 5.2732, LR: 0.227677\nEpoch [463/800] Average Loss: 5.2670\n\nEpoch [464/800], Batch [0/98], Loss: 5.2683, LR: 0.226535\nEpoch [464/800], Batch [10/98], Loss: 5.2826, LR: 0.226535\nEpoch [464/800], Batch [20/98], Loss: 5.2531, LR: 0.226535\nEpoch [464/800], Batch [30/98], Loss: 5.2791, LR: 0.226535\nEpoch [464/800], Batch [40/98], Loss: 5.2830, LR: 0.226535\nEpoch [464/800], Batch [50/98], Loss: 5.2653, LR: 0.226535\nEpoch [464/800], Batch [60/98], Loss: 5.2638, LR: 0.226535\nEpoch [464/800], Batch [70/98], Loss: 5.2733, LR: 0.226535\nEpoch [464/800], Batch [80/98], Loss: 5.2642, LR: 0.226535\nEpoch [464/800], Batch [90/98], Loss: 5.2821, LR: 0.226535\nEpoch [464/800] Average Loss: 5.2686\n\nEpoch [465/800], Batch [0/98], Loss: 5.2731, LR: 0.225393\nEpoch [465/800], Batch [10/98], Loss: 5.2677, LR: 0.225393\nEpoch [465/800], Batch [20/98], Loss: 5.2566, LR: 0.225393\nEpoch [465/800], Batch [30/98], Loss: 5.2916, LR: 0.225393\nEpoch [465/800], Batch [40/98], Loss: 5.2686, LR: 0.225393\nEpoch [465/800], Batch [50/98], Loss: 5.2934, LR: 0.225393\nEpoch [465/800], Batch [60/98], Loss: 5.2907, LR: 0.225393\nEpoch [465/800], Batch [70/98], Loss: 5.2609, LR: 0.225393\nEpoch [465/800], Batch [80/98], Loss: 5.2600, LR: 0.225393\nEpoch [465/800], Batch [90/98], Loss: 5.2528, LR: 0.225393\nEpoch [465/800] Average Loss: 5.2714\n\nEpoch [466/800], Batch [0/98], Loss: 5.2537, LR: 0.224253\nEpoch [466/800], Batch [10/98], Loss: 5.2677, LR: 0.224253\nEpoch [466/800], Batch [20/98], Loss: 5.2445, LR: 0.224253\nEpoch [466/800], Batch [30/98], Loss: 5.2646, LR: 0.224253\nEpoch [466/800], Batch [40/98], Loss: 5.2639, LR: 0.224253\nEpoch [466/800], Batch [50/98], Loss: 5.2498, LR: 0.224253\nEpoch [466/800], Batch [60/98], Loss: 5.2713, LR: 0.224253\nEpoch [466/800], Batch [70/98], Loss: 5.2935, LR: 0.224253\nEpoch [466/800], Batch [80/98], Loss: 5.2743, LR: 0.224253\nEpoch [466/800], Batch [90/98], Loss: 5.2854, LR: 0.224253\nEpoch [466/800] Average Loss: 5.2669\n\nEpoch [467/800], Batch [0/98], Loss: 5.2735, LR: 0.223113\nEpoch [467/800], Batch [10/98], Loss: 5.2754, LR: 0.223113\nEpoch [467/800], Batch [20/98], Loss: 5.2758, LR: 0.223113\nEpoch [467/800], Batch [30/98], Loss: 5.2680, LR: 0.223113\nEpoch [467/800], Batch [40/98], Loss: 5.2741, LR: 0.223113\nEpoch [467/800], Batch [50/98], Loss: 5.2654, LR: 0.223113\nEpoch [467/800], Batch [60/98], Loss: 5.2517, LR: 0.223113\nEpoch [467/800], Batch [70/98], Loss: 5.2760, LR: 0.223113\nEpoch [467/800], Batch [80/98], Loss: 5.2683, LR: 0.223113\nEpoch [467/800], Batch [90/98], Loss: 5.2556, LR: 0.223113\nEpoch [467/800] Average Loss: 5.2720\n\nEpoch [468/800], Batch [0/98], Loss: 5.2713, LR: 0.221975\nEpoch [468/800], Batch [10/98], Loss: 5.2595, LR: 0.221975\nEpoch [468/800], Batch [20/98], Loss: 5.2537, LR: 0.221975\nEpoch [468/800], Batch [30/98], Loss: 5.2740, LR: 0.221975\nEpoch [468/800], Batch [40/98], Loss: 5.2920, LR: 0.221975\nEpoch [468/800], Batch [50/98], Loss: 5.2772, LR: 0.221975\nEpoch [468/800], Batch [60/98], Loss: 5.2992, LR: 0.221975\nEpoch [468/800], Batch [70/98], Loss: 5.2716, LR: 0.221975\nEpoch [468/800], Batch [80/98], Loss: 5.2777, LR: 0.221975\nEpoch [468/800], Batch [90/98], Loss: 5.2617, LR: 0.221975\nEpoch [468/800] Average Loss: 5.2697\n\nEpoch [469/800], Batch [0/98], Loss: 5.2729, LR: 0.220838\nEpoch [469/800], Batch [10/98], Loss: 5.2886, LR: 0.220838\nEpoch [469/800], Batch [20/98], Loss: 5.2775, LR: 0.220838\nEpoch [469/800], Batch [30/98], Loss: 5.2733, LR: 0.220838\nEpoch [469/800], Batch [40/98], Loss: 5.2564, LR: 0.220838\nEpoch [469/800], Batch [50/98], Loss: 5.2498, LR: 0.220838\nEpoch [469/800], Batch [60/98], Loss: 5.2786, LR: 0.220838\nEpoch [469/800], Batch [70/98], Loss: 5.2746, LR: 0.220838\nEpoch [469/800], Batch [80/98], Loss: 5.2414, LR: 0.220838\nEpoch [469/800], Batch [90/98], Loss: 5.2771, LR: 0.220838\nEpoch [469/800] Average Loss: 5.2694\n\nEpoch [470/800], Batch [0/98], Loss: 5.3005, LR: 0.219702\nEpoch [470/800], Batch [10/98], Loss: 5.2604, LR: 0.219702\nEpoch [470/800], Batch [20/98], Loss: 5.2789, LR: 0.219702\nEpoch [470/800], Batch [30/98], Loss: 5.2659, LR: 0.219702\nEpoch [470/800], Batch [40/98], Loss: 5.2817, LR: 0.219702\nEpoch [470/800], Batch [50/98], Loss: 5.2704, LR: 0.219702\nEpoch [470/800], Batch [60/98], Loss: 5.2681, LR: 0.219702\nEpoch [470/800], Batch [70/98], Loss: 5.2766, LR: 0.219702\nEpoch [470/800], Batch [80/98], Loss: 5.2303, LR: 0.219702\nEpoch [470/800], Batch [90/98], Loss: 5.2805, LR: 0.219702\nEpoch [470/800] Average Loss: 5.2698\n\nEpoch [471/800], Batch [0/98], Loss: 5.2411, LR: 0.218568\nEpoch [471/800], Batch [10/98], Loss: 5.2564, LR: 0.218568\nEpoch [471/800], Batch [20/98], Loss: 5.2530, LR: 0.218568\nEpoch [471/800], Batch [30/98], Loss: 5.2634, LR: 0.218568\nEpoch [471/800], Batch [40/98], Loss: 5.2481, LR: 0.218568\nEpoch [471/800], Batch [50/98], Loss: 5.2470, LR: 0.218568\nEpoch [471/800], Batch [60/98], Loss: 5.2760, LR: 0.218568\nEpoch [471/800], Batch [70/98], Loss: 5.2733, LR: 0.218568\nEpoch [471/800], Batch [80/98], Loss: 5.2677, LR: 0.218568\nEpoch [471/800], Batch [90/98], Loss: 5.2950, LR: 0.218568\nEpoch [471/800] Average Loss: 5.2698\n\nEpoch [472/800], Batch [0/98], Loss: 5.2733, LR: 0.217435\nEpoch [472/800], Batch [10/98], Loss: 5.2627, LR: 0.217435\nEpoch [472/800], Batch [20/98], Loss: 5.2710, LR: 0.217435\nEpoch [472/800], Batch [30/98], Loss: 5.2646, LR: 0.217435\nEpoch [472/800], Batch [40/98], Loss: 5.2446, LR: 0.217435\nEpoch [472/800], Batch [50/98], Loss: 5.2845, LR: 0.217435\nEpoch [472/800], Batch [60/98], Loss: 5.2651, LR: 0.217435\nEpoch [472/800], Batch [70/98], Loss: 5.2616, LR: 0.217435\nEpoch [472/800], Batch [80/98], Loss: 5.2615, LR: 0.217435\nEpoch [472/800], Batch [90/98], Loss: 5.2509, LR: 0.217435\nEpoch [472/800] Average Loss: 5.2691\n\nEpoch [473/800], Batch [0/98], Loss: 5.2683, LR: 0.216303\nEpoch [473/800], Batch [10/98], Loss: 5.2814, LR: 0.216303\nEpoch [473/800], Batch [20/98], Loss: 5.2606, LR: 0.216303\nEpoch [473/800], Batch [30/98], Loss: 5.2403, LR: 0.216303\nEpoch [473/800], Batch [40/98], Loss: 5.2655, LR: 0.216303\nEpoch [473/800], Batch [50/98], Loss: 5.2795, LR: 0.216303\nEpoch [473/800], Batch [60/98], Loss: 5.2581, LR: 0.216303\nEpoch [473/800], Batch [70/98], Loss: 5.2821, LR: 0.216303\nEpoch [473/800], Batch [80/98], Loss: 5.2731, LR: 0.216303\nEpoch [473/800], Batch [90/98], Loss: 5.2830, LR: 0.216303\nEpoch [473/800] Average Loss: 5.2666\n\nEpoch [474/800], Batch [0/98], Loss: 5.2918, LR: 0.215172\nEpoch [474/800], Batch [10/98], Loss: 5.2675, LR: 0.215172\nEpoch [474/800], Batch [20/98], Loss: 5.2839, LR: 0.215172\nEpoch [474/800], Batch [30/98], Loss: 5.2665, LR: 0.215172\nEpoch [474/800], Batch [40/98], Loss: 5.2820, LR: 0.215172\nEpoch [474/800], Batch [50/98], Loss: 5.2677, LR: 0.215172\nEpoch [474/800], Batch [60/98], Loss: 5.2675, LR: 0.215172\nEpoch [474/800], Batch [70/98], Loss: 5.2583, LR: 0.215172\nEpoch [474/800], Batch [80/98], Loss: 5.2703, LR: 0.215172\nEpoch [474/800], Batch [90/98], Loss: 5.2618, LR: 0.215172\nEpoch [474/800] Average Loss: 5.2660\n\nEpoch [475/800], Batch [0/98], Loss: 5.2876, LR: 0.214043\nEpoch [475/800], Batch [10/98], Loss: 5.2716, LR: 0.214043\nEpoch [475/800], Batch [20/98], Loss: 5.2577, LR: 0.214043\nEpoch [475/800], Batch [30/98], Loss: 5.3197, LR: 0.214043\nEpoch [475/800], Batch [40/98], Loss: 5.2654, LR: 0.214043\nEpoch [475/800], Batch [50/98], Loss: 5.2593, LR: 0.214043\nEpoch [475/800], Batch [60/98], Loss: 5.2615, LR: 0.214043\nEpoch [475/800], Batch [70/98], Loss: 5.2802, LR: 0.214043\nEpoch [475/800], Batch [80/98], Loss: 5.2625, LR: 0.214043\nEpoch [475/800], Batch [90/98], Loss: 5.2876, LR: 0.214043\nEpoch [475/800] Average Loss: 5.2680\n\nEpoch [476/800], Batch [0/98], Loss: 5.2712, LR: 0.212915\nEpoch [476/800], Batch [10/98], Loss: 5.2766, LR: 0.212915\nEpoch [476/800], Batch [20/98], Loss: 5.2942, LR: 0.212915\nEpoch [476/800], Batch [30/98], Loss: 5.2785, LR: 0.212915\nEpoch [476/800], Batch [40/98], Loss: 5.2641, LR: 0.212915\nEpoch [476/800], Batch [50/98], Loss: 5.2700, LR: 0.212915\nEpoch [476/800], Batch [60/98], Loss: 5.2661, LR: 0.212915\nEpoch [476/800], Batch [70/98], Loss: 5.2569, LR: 0.212915\nEpoch [476/800], Batch [80/98], Loss: 5.2870, LR: 0.212915\nEpoch [476/800], Batch [90/98], Loss: 5.2895, LR: 0.212915\nEpoch [476/800] Average Loss: 5.2709\n\nEpoch [477/800], Batch [0/98], Loss: 5.2779, LR: 0.211788\nEpoch [477/800], Batch [10/98], Loss: 5.2660, LR: 0.211788\nEpoch [477/800], Batch [20/98], Loss: 5.2594, LR: 0.211788\nEpoch [477/800], Batch [30/98], Loss: 5.2436, LR: 0.211788\nEpoch [477/800], Batch [40/98], Loss: 5.2879, LR: 0.211788\nEpoch [477/800], Batch [50/98], Loss: 5.2771, LR: 0.211788\nEpoch [477/800], Batch [60/98], Loss: 5.2407, LR: 0.211788\nEpoch [477/800], Batch [70/98], Loss: 5.2874, LR: 0.211788\nEpoch [477/800], Batch [80/98], Loss: 5.2596, LR: 0.211788\nEpoch [477/800], Batch [90/98], Loss: 5.2598, LR: 0.211788\nEpoch [477/800] Average Loss: 5.2663\n\nEpoch [478/800], Batch [0/98], Loss: 5.2621, LR: 0.210663\nEpoch [478/800], Batch [10/98], Loss: 5.3146, LR: 0.210663\nEpoch [478/800], Batch [20/98], Loss: 5.2625, LR: 0.210663\nEpoch [478/800], Batch [30/98], Loss: 5.2735, LR: 0.210663\nEpoch [478/800], Batch [40/98], Loss: 5.2701, LR: 0.210663\nEpoch [478/800], Batch [50/98], Loss: 5.2629, LR: 0.210663\nEpoch [478/800], Batch [60/98], Loss: 5.2524, LR: 0.210663\nEpoch [478/800], Batch [70/98], Loss: 5.2609, LR: 0.210663\nEpoch [478/800], Batch [80/98], Loss: 5.2818, LR: 0.210663\nEpoch [478/800], Batch [90/98], Loss: 5.2810, LR: 0.210663\nEpoch [478/800] Average Loss: 5.2688\n\nEpoch [479/800], Batch [0/98], Loss: 5.2825, LR: 0.209539\nEpoch [479/800], Batch [10/98], Loss: 5.2648, LR: 0.209539\nEpoch [479/800], Batch [20/98], Loss: 5.2730, LR: 0.209539\nEpoch [479/800], Batch [30/98], Loss: 5.2601, LR: 0.209539\nEpoch [479/800], Batch [40/98], Loss: 5.2594, LR: 0.209539\nEpoch [479/800], Batch [50/98], Loss: 5.2623, LR: 0.209539\nEpoch [479/800], Batch [60/98], Loss: 5.2951, LR: 0.209539\nEpoch [479/800], Batch [70/98], Loss: 5.2777, LR: 0.209539\nEpoch [479/800], Batch [80/98], Loss: 5.2641, LR: 0.209539\nEpoch [479/800], Batch [90/98], Loss: 5.2556, LR: 0.209539\nEpoch [479/800] Average Loss: 5.2671\n\nEpoch [480/800], Batch [0/98], Loss: 5.2480, LR: 0.208416\nEpoch [480/800], Batch [10/98], Loss: 5.2551, LR: 0.208416\nEpoch [480/800], Batch [20/98], Loss: 5.2707, LR: 0.208416\nEpoch [480/800], Batch [30/98], Loss: 5.2560, LR: 0.208416\nEpoch [480/800], Batch [40/98], Loss: 5.2630, LR: 0.208416\nEpoch [480/800], Batch [50/98], Loss: 5.2688, LR: 0.208416\nEpoch [480/800], Batch [60/98], Loss: 5.2828, LR: 0.208416\nEpoch [480/800], Batch [70/98], Loss: 5.2725, LR: 0.208416\nEpoch [480/800], Batch [80/98], Loss: 5.2799, LR: 0.208416\nEpoch [480/800], Batch [90/98], Loss: 5.3052, LR: 0.208416\nEpoch [480/800] Average Loss: 5.2679\n\nEpoch [481/800], Batch [0/98], Loss: 5.2389, LR: 0.207295\nEpoch [481/800], Batch [10/98], Loss: 5.2574, LR: 0.207295\nEpoch [481/800], Batch [20/98], Loss: 5.2884, LR: 0.207295\nEpoch [481/800], Batch [30/98], Loss: 5.2510, LR: 0.207295\nEpoch [481/800], Batch [40/98], Loss: 5.2715, LR: 0.207295\nEpoch [481/800], Batch [50/98], Loss: 5.2806, LR: 0.207295\nEpoch [481/800], Batch [60/98], Loss: 5.2680, LR: 0.207295\nEpoch [481/800], Batch [70/98], Loss: 5.2870, LR: 0.207295\nEpoch [481/800], Batch [80/98], Loss: 5.2851, LR: 0.207295\nEpoch [481/800], Batch [90/98], Loss: 5.2925, LR: 0.207295\nEpoch [481/800] Average Loss: 5.2673\n\nEpoch [482/800], Batch [0/98], Loss: 5.2673, LR: 0.206175\nEpoch [482/800], Batch [10/98], Loss: 5.2635, LR: 0.206175\nEpoch [482/800], Batch [20/98], Loss: 5.2706, LR: 0.206175\nEpoch [482/800], Batch [30/98], Loss: 5.2823, LR: 0.206175\nEpoch [482/800], Batch [40/98], Loss: 5.2570, LR: 0.206175\nEpoch [482/800], Batch [50/98], Loss: 5.2835, LR: 0.206175\nEpoch [482/800], Batch [60/98], Loss: 5.2620, LR: 0.206175\nEpoch [482/800], Batch [70/98], Loss: 5.2727, LR: 0.206175\nEpoch [482/800], Batch [80/98], Loss: 5.2819, LR: 0.206175\nEpoch [482/800], Batch [90/98], Loss: 5.2837, LR: 0.206175\nEpoch [482/800] Average Loss: 5.2683\n\nEpoch [483/800], Batch [0/98], Loss: 5.2683, LR: 0.205057\nEpoch [483/800], Batch [10/98], Loss: 5.2489, LR: 0.205057\nEpoch [483/800], Batch [20/98], Loss: 5.2787, LR: 0.205057\nEpoch [483/800], Batch [30/98], Loss: 5.2827, LR: 0.205057\nEpoch [483/800], Batch [40/98], Loss: 5.2883, LR: 0.205057\nEpoch [483/800], Batch [50/98], Loss: 5.2821, LR: 0.205057\nEpoch [483/800], Batch [60/98], Loss: 5.2737, LR: 0.205057\nEpoch [483/800], Batch [70/98], Loss: 5.2444, LR: 0.205057\nEpoch [483/800], Batch [80/98], Loss: 5.2586, LR: 0.205057\nEpoch [483/800], Batch [90/98], Loss: 5.2583, LR: 0.205057\nEpoch [483/800] Average Loss: 5.2633\n\nEpoch [484/800], Batch [0/98], Loss: 5.2702, LR: 0.203940\nEpoch [484/800], Batch [10/98], Loss: 5.2714, LR: 0.203940\nEpoch [484/800], Batch [20/98], Loss: 5.2582, LR: 0.203940\nEpoch [484/800], Batch [40/98], Loss: 5.2617, LR: 0.203940\nEpoch [484/800], Batch [50/98], Loss: 5.2724, LR: 0.203940\nEpoch [484/800], Batch [60/98], Loss: 5.2560, LR: 0.203940\nEpoch [484/800], Batch [70/98], Loss: 5.2540, LR: 0.203940\nEpoch [484/800], Batch [80/98], Loss: 5.2813, LR: 0.203940\nEpoch [484/800], Batch [90/98], Loss: 5.2693, LR: 0.203940\nEpoch [484/800] Average Loss: 5.2665\n\nEpoch [485/800], Batch [0/98], Loss: 5.2604, LR: 0.202825\nEpoch [485/800], Batch [10/98], Loss: 5.2711, LR: 0.202825\nEpoch [485/800], Batch [20/98], Loss: 5.2769, LR: 0.202825\nEpoch [485/800], Batch [30/98], Loss: 5.2924, LR: 0.202825\nEpoch [485/800], Batch [40/98], Loss: 5.2756, LR: 0.202825\nEpoch [485/800], Batch [50/98], Loss: 5.2690, LR: 0.202825\nEpoch [485/800], Batch [60/98], Loss: 5.2739, LR: 0.202825\nEpoch [485/800], Batch [70/98], Loss: 5.2602, LR: 0.202825\nEpoch [485/800], Batch [80/98], Loss: 5.2606, LR: 0.202825\nEpoch [485/800], Batch [90/98], Loss: 5.2710, LR: 0.202825\nEpoch [485/800] Average Loss: 5.2670\n\nEpoch [486/800], Batch [0/98], Loss: 5.2580, LR: 0.201711\nEpoch [486/800], Batch [10/98], Loss: 5.2741, LR: 0.201711\nEpoch [486/800], Batch [20/98], Loss: 5.2931, LR: 0.201711\nEpoch [486/800], Batch [30/98], Loss: 5.2629, LR: 0.201711\nEpoch [486/800], Batch [40/98], Loss: 5.2582, LR: 0.201711\nEpoch [486/800], Batch [50/98], Loss: 5.2504, LR: 0.201711\nEpoch [486/800], Batch [60/98], Loss: 5.2634, LR: 0.201711\nEpoch [486/800], Batch [70/98], Loss: 5.2468, LR: 0.201711\nEpoch [486/800], Batch [80/98], Loss: 5.2746, LR: 0.201711\nEpoch [486/800], Batch [90/98], Loss: 5.2867, LR: 0.201711\nEpoch [486/800] Average Loss: 5.2656\n\nEpoch [487/800], Batch [0/98], Loss: 5.2807, LR: 0.200599\nEpoch [487/800], Batch [10/98], Loss: 5.2471, LR: 0.200599\nEpoch [487/800], Batch [20/98], Loss: 5.2768, LR: 0.200599\nEpoch [487/800], Batch [30/98], Loss: 5.2865, LR: 0.200599\nEpoch [487/800], Batch [40/98], Loss: 5.2712, LR: 0.200599\nEpoch [487/800], Batch [50/98], Loss: 5.2602, LR: 0.200599\nEpoch [487/800], Batch [60/98], Loss: 5.2688, LR: 0.200599\nEpoch [487/800], Batch [70/98], Loss: 5.2791, LR: 0.200599\nEpoch [487/800], Batch [80/98], Loss: 5.2572, LR: 0.200599\nEpoch [487/800], Batch [90/98], Loss: 5.2692, LR: 0.200599\nEpoch [487/800] Average Loss: 5.2680\n\nEpoch [488/800], Batch [0/98], Loss: 5.2881, LR: 0.199488\nEpoch [488/800], Batch [10/98], Loss: 5.2446, LR: 0.199488\nEpoch [488/800], Batch [20/98], Loss: 5.2988, LR: 0.199488\nEpoch [488/800], Batch [30/98], Loss: 5.2803, LR: 0.199488\nEpoch [488/800], Batch [40/98], Loss: 5.2481, LR: 0.199488\nEpoch [488/800], Batch [50/98], Loss: 5.2552, LR: 0.199488\nEpoch [488/800], Batch [60/98], Loss: 5.2566, LR: 0.199488\nEpoch [488/800], Batch [70/98], Loss: 5.2683, LR: 0.199488\nEpoch [488/800], Batch [80/98], Loss: 5.2705, LR: 0.199488\nEpoch [488/800], Batch [90/98], Loss: 5.2917, LR: 0.199488\nEpoch [488/800] Average Loss: 5.2667\n\nEpoch [489/800], Batch [0/98], Loss: 5.2843, LR: 0.198379\nEpoch [489/800], Batch [10/98], Loss: 5.3032, LR: 0.198379\nEpoch [489/800], Batch [20/98], Loss: 5.2709, LR: 0.198379\nEpoch [489/800], Batch [30/98], Loss: 5.2593, LR: 0.198379\nEpoch [489/800], Batch [40/98], Loss: 5.2932, LR: 0.198379\nEpoch [489/800], Batch [50/98], Loss: 5.2791, LR: 0.198379\nEpoch [489/800], Batch [60/98], Loss: 5.2563, LR: 0.198379\nEpoch [489/800], Batch [70/98], Loss: 5.2678, LR: 0.198379\nEpoch [489/800], Batch [80/98], Loss: 5.2844, LR: 0.198379\nEpoch [489/800], Batch [90/98], Loss: 5.2752, LR: 0.198379\nEpoch [489/800] Average Loss: 5.2658\n\nEpoch [490/800], Batch [0/98], Loss: 5.2797, LR: 0.197271\nEpoch [490/800], Batch [10/98], Loss: 5.2608, LR: 0.197271\nEpoch [490/800], Batch [20/98], Loss: 5.2644, LR: 0.197271\nEpoch [490/800], Batch [30/98], Loss: 5.2401, LR: 0.197271\nEpoch [490/800], Batch [40/98], Loss: 5.2639, LR: 0.197271\nEpoch [490/800], Batch [50/98], Loss: 5.2605, LR: 0.197271\nEpoch [490/800], Batch [60/98], Loss: 5.2796, LR: 0.197271\nEpoch [490/800], Batch [70/98], Loss: 5.2808, LR: 0.197271\nEpoch [490/800], Batch [80/98], Loss: 5.2761, LR: 0.197271\nEpoch [490/800], Batch [90/98], Loss: 5.2676, LR: 0.197271\nEpoch [490/800] Average Loss: 5.2656\n\nEpoch [491/800], Batch [0/98], Loss: 5.2748, LR: 0.196165\nEpoch [491/800], Batch [10/98], Loss: 5.2878, LR: 0.196165\nEpoch [491/800], Batch [20/98], Loss: 5.2907, LR: 0.196165\nEpoch [491/800], Batch [30/98], Loss: 5.2592, LR: 0.196165\nEpoch [491/800], Batch [40/98], Loss: 5.2687, LR: 0.196165\nEpoch [491/800], Batch [50/98], Loss: 5.2901, LR: 0.196165\nEpoch [491/800], Batch [60/98], Loss: 5.2629, LR: 0.196165\nEpoch [491/800], Batch [70/98], Loss: 5.2614, LR: 0.196165\nEpoch [491/800], Batch [80/98], Loss: 5.2519, LR: 0.196165\nEpoch [491/800], Batch [90/98], Loss: 5.2594, LR: 0.196165\nEpoch [491/800] Average Loss: 5.2638\n\nEpoch [492/800], Batch [0/98], Loss: 5.2746, LR: 0.195060\nEpoch [492/800], Batch [10/98], Loss: 5.2808, LR: 0.195060\nEpoch [492/800], Batch [20/98], Loss: 5.2848, LR: 0.195060\nEpoch [492/800], Batch [30/98], Loss: 5.2995, LR: 0.195060\nEpoch [492/800], Batch [40/98], Loss: 5.2548, LR: 0.195060\nEpoch [492/800], Batch [50/98], Loss: 5.2709, LR: 0.195060\nEpoch [492/800], Batch [60/98], Loss: 5.2582, LR: 0.195060\nEpoch [492/800], Batch [70/98], Loss: 5.2749, LR: 0.195060\nEpoch [492/800], Batch [80/98], Loss: 5.2818, LR: 0.195060\nEpoch [492/800], Batch [90/98], Loss: 5.2907, LR: 0.195060\nEpoch [492/800] Average Loss: 5.2681\n\nEpoch [493/800], Batch [0/98], Loss: 5.2451, LR: 0.193958\nEpoch [493/800], Batch [10/98], Loss: 5.2629, LR: 0.193958\nEpoch [493/800], Batch [20/98], Loss: 5.2579, LR: 0.193958\nEpoch [493/800], Batch [30/98], Loss: 5.2563, LR: 0.193958\nEpoch [493/800], Batch [40/98], Loss: 5.2558, LR: 0.193958\nEpoch [493/800], Batch [50/98], Loss: 5.2760, LR: 0.193958\nEpoch [493/800], Batch [60/98], Loss: 5.2732, LR: 0.193958\nEpoch [493/800], Batch [70/98], Loss: 5.2609, LR: 0.193958\nEpoch [493/800], Batch [80/98], Loss: 5.2435, LR: 0.193958\nEpoch [493/800], Batch [90/98], Loss: 5.2734, LR: 0.193958\nEpoch [493/800] Average Loss: 5.2647\n\nEpoch [494/800], Batch [0/98], Loss: 5.2772, LR: 0.192856\nEpoch [494/800], Batch [10/98], Loss: 5.2471, LR: 0.192856\nEpoch [494/800], Batch [20/98], Loss: 5.2930, LR: 0.192856\nEpoch [494/800], Batch [30/98], Loss: 5.2505, LR: 0.192856\nEpoch [494/800], Batch [40/98], Loss: 5.2751, LR: 0.192856\nEpoch [494/800], Batch [50/98], Loss: 5.2679, LR: 0.192856\nEpoch [494/800], Batch [60/98], Loss: 5.2717, LR: 0.192856\nEpoch [494/800], Batch [70/98], Loss: 5.2699, LR: 0.192856\nEpoch [494/800], Batch [80/98], Loss: 5.2633, LR: 0.192856\nEpoch [494/800], Batch [90/98], Loss: 5.2575, LR: 0.192856\nEpoch [494/800] Average Loss: 5.2663\n\nEpoch [495/800], Batch [0/98], Loss: 5.2672, LR: 0.191757\nEpoch [495/800], Batch [10/98], Loss: 5.2661, LR: 0.191757\nEpoch [495/800], Batch [20/98], Loss: 5.2826, LR: 0.191757\nEpoch [495/800], Batch [30/98], Loss: 5.2727, LR: 0.191757\nEpoch [495/800], Batch [40/98], Loss: 5.2786, LR: 0.191757\nEpoch [495/800], Batch [50/98], Loss: 5.2510, LR: 0.191757\nEpoch [495/800], Batch [60/98], Loss: 5.2552, LR: 0.191757\nEpoch [495/800], Batch [70/98], Loss: 5.2613, LR: 0.191757\nEpoch [495/800], Batch [80/98], Loss: 5.2838, LR: 0.191757\nEpoch [495/800], Batch [90/98], Loss: 5.2707, LR: 0.191757\nEpoch [495/800] Average Loss: 5.2658\n\nEpoch [496/800], Batch [0/98], Loss: 5.2766, LR: 0.190659\nEpoch [496/800], Batch [10/98], Loss: 5.2806, LR: 0.190659\nEpoch [496/800], Batch [20/98], Loss: 5.2621, LR: 0.190659\nEpoch [496/800], Batch [30/98], Loss: 5.2516, LR: 0.190659\nEpoch [496/800], Batch [40/98], Loss: 5.2802, LR: 0.190659\nEpoch [496/800], Batch [50/98], Loss: 5.2783, LR: 0.190659\nEpoch [496/800], Batch [60/98], Loss: 5.2792, LR: 0.190659\nEpoch [496/800], Batch [70/98], Loss: 5.2816, LR: 0.190659\nEpoch [496/800], Batch [80/98], Loss: 5.2716, LR: 0.190659\nEpoch [496/800], Batch [90/98], Loss: 5.2580, LR: 0.190659\nEpoch [496/800] Average Loss: 5.2650\n\nEpoch [497/800], Batch [0/98], Loss: 5.2667, LR: 0.189563\nEpoch [497/800], Batch [10/98], Loss: 5.2604, LR: 0.189563\nEpoch [497/800], Batch [20/98], Loss: 5.2719, LR: 0.189563\nEpoch [497/800], Batch [30/98], Loss: 5.2683, LR: 0.189563\nEpoch [497/800], Batch [40/98], Loss: 5.2585, LR: 0.189563\nEpoch [497/800], Batch [50/98], Loss: 5.2476, LR: 0.189563\nEpoch [497/800], Batch [60/98], Loss: 5.2821, LR: 0.189563\nEpoch [497/800], Batch [70/98], Loss: 5.2963, LR: 0.189563\nEpoch [497/800], Batch [80/98], Loss: 5.2897, LR: 0.189563\nEpoch [497/800], Batch [90/98], Loss: 5.2419, LR: 0.189563\nEpoch [497/800] Average Loss: 5.2642\n\nEpoch [498/800], Batch [0/98], Loss: 5.2394, LR: 0.188468\nEpoch [498/800], Batch [10/98], Loss: 5.2547, LR: 0.188468\nEpoch [498/800], Batch [20/98], Loss: 5.2475, LR: 0.188468\nEpoch [498/800], Batch [30/98], Loss: 5.2524, LR: 0.188468\nEpoch [498/800], Batch [40/98], Loss: 5.2495, LR: 0.188468\nEpoch [498/800], Batch [50/98], Loss: 5.2781, LR: 0.188468\nEpoch [498/800], Batch [60/98], Loss: 5.2495, LR: 0.188468\nEpoch [498/800], Batch [70/98], Loss: 5.2710, LR: 0.188468\nEpoch [498/800], Batch [80/98], Loss: 5.2814, LR: 0.188468\nEpoch [498/800], Batch [90/98], Loss: 5.2572, LR: 0.188468\nEpoch [498/800] Average Loss: 5.2642\n\nEpoch [499/800], Batch [0/98], Loss: 5.2641, LR: 0.187375\nEpoch [499/800], Batch [10/98], Loss: 5.2929, LR: 0.187375\nEpoch [499/800], Batch [20/98], Loss: 5.2668, LR: 0.187375\nEpoch [499/800], Batch [30/98], Loss: 5.2465, LR: 0.187375\nEpoch [499/800], Batch [40/98], Loss: 5.2658, LR: 0.187375\nEpoch [499/800], Batch [50/98], Loss: 5.2699, LR: 0.187375\nEpoch [499/800], Batch [60/98], Loss: 5.2588, LR: 0.187375\nEpoch [499/800], Batch [70/98], Loss: 5.2600, LR: 0.187375\nEpoch [499/800], Batch [80/98], Loss: 5.2762, LR: 0.187375\nEpoch [499/800], Batch [90/98], Loss: 5.2924, LR: 0.187375\nEpoch [499/800] Average Loss: 5.2646\n\nEpoch [500/800], Batch [0/98], Loss: 5.2886, LR: 0.186284\nEpoch [500/800], Batch [10/98], Loss: 5.2691, LR: 0.186284\nEpoch [500/800], Batch [20/98], Loss: 5.2601, LR: 0.186284\nEpoch [500/800], Batch [30/98], Loss: 5.2493, LR: 0.186284\nEpoch [500/800], Batch [40/98], Loss: 5.2665, LR: 0.186284\nEpoch [500/800], Batch [50/98], Loss: 5.2763, LR: 0.186284\nEpoch [500/800], Batch [60/98], Loss: 5.2588, LR: 0.186284\nEpoch [500/800], Batch [70/98], Loss: 5.2919, LR: 0.186284\nEpoch [500/800], Batch [80/98], Loss: 5.2600, LR: 0.186284\nEpoch [500/800], Batch [90/98], Loss: 5.2837, LR: 0.186284\nEpoch [500/800] Average Loss: 5.2644\n\nEpoch [501/800], Batch [0/98], Loss: 5.2456, LR: 0.185195\nEpoch [501/800], Batch [10/98], Loss: 5.2689, LR: 0.185195\nEpoch [501/800], Batch [20/98], Loss: 5.2616, LR: 0.185195\nEpoch [501/800], Batch [30/98], Loss: 5.2454, LR: 0.185195\nEpoch [501/800], Batch [40/98], Loss: 5.2758, LR: 0.185195\nEpoch [501/800], Batch [50/98], Loss: 5.2700, LR: 0.185195\nEpoch [501/800], Batch [60/98], Loss: 5.2482, LR: 0.185195\nEpoch [501/800], Batch [70/98], Loss: 5.2710, LR: 0.185195\nEpoch [501/800], Batch [80/98], Loss: 5.2602, LR: 0.185195\nEpoch [501/800], Batch [90/98], Loss: 5.2762, LR: 0.185195\nEpoch [501/800] Average Loss: 5.2643\n\nEpoch [502/800], Batch [0/98], Loss: 5.2841, LR: 0.184107\nEpoch [502/800], Batch [10/98], Loss: 5.2563, LR: 0.184107\nEpoch [502/800], Batch [20/98], Loss: 5.2562, LR: 0.184107\nEpoch [502/800], Batch [30/98], Loss: 5.2529, LR: 0.184107\nEpoch [502/800], Batch [40/98], Loss: 5.2425, LR: 0.184107\nEpoch [502/800], Batch [50/98], Loss: 5.2608, LR: 0.184107\nEpoch [502/800], Batch [60/98], Loss: 5.2372, LR: 0.184107\nEpoch [502/800], Batch [70/98], Loss: 5.2631, LR: 0.184107\nEpoch [502/800], Batch [80/98], Loss: 5.2565, LR: 0.184107\nEpoch [502/800], Batch [90/98], Loss: 5.2652, LR: 0.184107\nEpoch [502/800] Average Loss: 5.2659\n\nEpoch [503/800], Batch [0/98], Loss: 5.2815, LR: 0.183022\nEpoch [503/800], Batch [10/98], Loss: 5.2796, LR: 0.183022\nEpoch [503/800], Batch [20/98], Loss: 5.2435, LR: 0.183022\nEpoch [503/800], Batch [30/98], Loss: 5.2630, LR: 0.183022\nEpoch [503/800], Batch [40/98], Loss: 5.2707, LR: 0.183022\nEpoch [503/800], Batch [50/98], Loss: 5.2812, LR: 0.183022\nEpoch [503/800], Batch [60/98], Loss: 5.2698, LR: 0.183022\nEpoch [503/800], Batch [70/98], Loss: 5.2672, LR: 0.183022\nEpoch [503/800], Batch [80/98], Loss: 5.2851, LR: 0.183022\nEpoch [503/800], Batch [90/98], Loss: 5.2846, LR: 0.183022\nEpoch [503/800] Average Loss: 5.2640\n\nEpoch [504/800], Batch [0/98], Loss: 5.2728, LR: 0.181938\nEpoch [504/800], Batch [10/98], Loss: 5.2522, LR: 0.181938\nEpoch [504/800], Batch [20/98], Loss: 5.2514, LR: 0.181938\nEpoch [504/800], Batch [30/98], Loss: 5.2850, LR: 0.181938\nEpoch [504/800], Batch [40/98], Loss: 5.2873, LR: 0.181938\nEpoch [504/800], Batch [50/98], Loss: 5.2950, LR: 0.181938\nEpoch [504/800], Batch [60/98], Loss: 5.2763, LR: 0.181938\nEpoch [504/800], Batch [70/98], Loss: 5.2662, LR: 0.181938\nEpoch [504/800], Batch [80/98], Loss: 5.2705, LR: 0.181938\nEpoch [504/800], Batch [90/98], Loss: 5.2987, LR: 0.181938\nEpoch [504/800] Average Loss: 5.2645\n\nEpoch [505/800], Batch [0/98], Loss: 5.2577, LR: 0.180856\nEpoch [505/800], Batch [10/98], Loss: 5.2658, LR: 0.180856\nEpoch [505/800], Batch [20/98], Loss: 5.2674, LR: 0.180856\nEpoch [505/800], Batch [30/98], Loss: 5.2645, LR: 0.180856\nEpoch [505/800], Batch [40/98], Loss: 5.2649, LR: 0.180856\nEpoch [505/800], Batch [50/98], Loss: 5.2630, LR: 0.180856\nEpoch [505/800], Batch [60/98], Loss: 5.2791, LR: 0.180856\nEpoch [505/800], Batch [70/98], Loss: 5.2761, LR: 0.180856\nEpoch [505/800], Batch [80/98], Loss: 5.2425, LR: 0.180856\nEpoch [505/800], Batch [90/98], Loss: 5.2601, LR: 0.180856\nEpoch [505/800] Average Loss: 5.2606\n\nEpoch [506/800], Batch [0/98], Loss: 5.2541, LR: 0.179775\nEpoch [506/800], Batch [10/98], Loss: 5.2454, LR: 0.179775\nEpoch [506/800], Batch [20/98], Loss: 5.2808, LR: 0.179775\nEpoch [506/800], Batch [30/98], Loss: 5.2678, LR: 0.179775\nEpoch [506/800], Batch [40/98], Loss: 5.2777, LR: 0.179775\nEpoch [506/800], Batch [50/98], Loss: 5.2616, LR: 0.179775\nEpoch [506/800], Batch [60/98], Loss: 5.2478, LR: 0.179775\nEpoch [506/800], Batch [70/98], Loss: 5.2644, LR: 0.179775\nEpoch [506/800], Batch [80/98], Loss: 5.2579, LR: 0.179775\nEpoch [506/800], Batch [90/98], Loss: 5.2656, LR: 0.179775\nEpoch [506/800] Average Loss: 5.2650\n\nEpoch [507/800], Batch [0/98], Loss: 5.2566, LR: 0.178697\nEpoch [507/800], Batch [10/98], Loss: 5.2859, LR: 0.178697\nEpoch [507/800], Batch [20/98], Loss: 5.2867, LR: 0.178697\nEpoch [507/800], Batch [30/98], Loss: 5.2756, LR: 0.178697\nEpoch [507/800], Batch [40/98], Loss: 5.2594, LR: 0.178697\nEpoch [507/800], Batch [50/98], Loss: 5.2660, LR: 0.178697\nEpoch [507/800], Batch [60/98], Loss: 5.2544, LR: 0.178697\nEpoch [507/800], Batch [70/98], Loss: 5.2657, LR: 0.178697\nEpoch [507/800], Batch [80/98], Loss: 5.2753, LR: 0.178697\nEpoch [507/800], Batch [90/98], Loss: 5.2428, LR: 0.178697\nEpoch [507/800] Average Loss: 5.2628\n\nEpoch [508/800], Batch [0/98], Loss: 5.2702, LR: 0.177620\nEpoch [508/800], Batch [10/98], Loss: 5.2630, LR: 0.177620\nEpoch [508/800], Batch [20/98], Loss: 5.2708, LR: 0.177620\nEpoch [508/800], Batch [30/98], Loss: 5.2828, LR: 0.177620\nEpoch [508/800], Batch [40/98], Loss: 5.2786, LR: 0.177620\nEpoch [508/800], Batch [50/98], Loss: 5.2617, LR: 0.177620\nEpoch [508/800], Batch [60/98], Loss: 5.2624, LR: 0.177620\nEpoch [508/800], Batch [70/98], Loss: 5.2578, LR: 0.177620\nEpoch [508/800], Batch [80/98], Loss: 5.2638, LR: 0.177620\nEpoch [508/800], Batch [90/98], Loss: 5.2703, LR: 0.177620\nEpoch [508/800] Average Loss: 5.2655\n\nEpoch [509/800], Batch [0/98], Loss: 5.2793, LR: 0.176546\nEpoch [509/800], Batch [10/98], Loss: 5.2538, LR: 0.176546\nEpoch [509/800], Batch [20/98], Loss: 5.2444, LR: 0.176546\nEpoch [509/800], Batch [30/98], Loss: 5.2662, LR: 0.176546\nEpoch [509/800], Batch [40/98], Loss: 5.2847, LR: 0.176546\nEpoch [509/800], Batch [50/98], Loss: 5.2573, LR: 0.176546\nEpoch [509/800], Batch [60/98], Loss: 5.2902, LR: 0.176546\nEpoch [509/800], Batch [70/98], Loss: 5.2602, LR: 0.176546\nEpoch [509/800], Batch [80/98], Loss: 5.2548, LR: 0.176546\nEpoch [509/800], Batch [90/98], Loss: 5.2669, LR: 0.176546\nEpoch [509/800] Average Loss: 5.2634\n\nEpoch [510/800], Batch [0/98], Loss: 5.2645, LR: 0.175473\nEpoch [510/800], Batch [10/98], Loss: 5.2740, LR: 0.175473\nEpoch [510/800], Batch [20/98], Loss: 5.2783, LR: 0.175473\nEpoch [510/800], Batch [30/98], Loss: 5.2701, LR: 0.175473\nEpoch [510/800], Batch [40/98], Loss: 5.2740, LR: 0.175473\nEpoch [510/800], Batch [50/98], Loss: 5.2734, LR: 0.175473\nEpoch [510/800], Batch [60/98], Loss: 5.2621, LR: 0.175473\nEpoch [510/800], Batch [70/98], Loss: 5.2519, LR: 0.175473\nEpoch [510/800], Batch [80/98], Loss: 5.2641, LR: 0.175473\nEpoch [510/800], Batch [90/98], Loss: 5.2527, LR: 0.175473\nEpoch [510/800] Average Loss: 5.2621\n\nEpoch [511/800], Batch [0/98], Loss: 5.2625, LR: 0.174402\nEpoch [511/800], Batch [10/98], Loss: 5.2565, LR: 0.174402\nEpoch [511/800], Batch [20/98], Loss: 5.2682, LR: 0.174402\nEpoch [511/800], Batch [30/98], Loss: 5.2713, LR: 0.174402\nEpoch [511/800], Batch [40/98], Loss: 5.2691, LR: 0.174402\nEpoch [511/800], Batch [50/98], Loss: 5.2938, LR: 0.174402\nEpoch [511/800], Batch [60/98], Loss: 5.2647, LR: 0.174402\nEpoch [511/800], Batch [70/98], Loss: 5.2819, LR: 0.174402\nEpoch [511/800], Batch [80/98], Loss: 5.2774, LR: 0.174402\nEpoch [511/800], Batch [90/98], Loss: 5.2763, LR: 0.174402\nEpoch [511/800] Average Loss: 5.2625\n\nEpoch [512/800], Batch [0/98], Loss: 5.2877, LR: 0.173333\nEpoch [512/800], Batch [10/98], Loss: 5.2863, LR: 0.173333\nEpoch [512/800], Batch [20/98], Loss: 5.2600, LR: 0.173333\nEpoch [512/800], Batch [30/98], Loss: 5.2607, LR: 0.173333\nEpoch [512/800], Batch [40/98], Loss: 5.2740, LR: 0.173333\nEpoch [512/800], Batch [50/98], Loss: 5.2466, LR: 0.173333\nEpoch [512/800], Batch [60/98], Loss: 5.2606, LR: 0.173333\nEpoch [512/800], Batch [70/98], Loss: 5.2534, LR: 0.173333\nEpoch [512/800], Batch [80/98], Loss: 5.2623, LR: 0.173333\nEpoch [512/800], Batch [90/98], Loss: 5.2570, LR: 0.173333\nEpoch [512/800] Average Loss: 5.2630\n\nEpoch [513/800], Batch [0/98], Loss: 5.2502, LR: 0.172266\nEpoch [513/800], Batch [10/98], Loss: 5.2720, LR: 0.172266\nEpoch [513/800], Batch [20/98], Loss: 5.2847, LR: 0.172266\nEpoch [513/800], Batch [30/98], Loss: 5.2647, LR: 0.172266\nEpoch [513/800], Batch [40/98], Loss: 5.2434, LR: 0.172266\nEpoch [513/800], Batch [50/98], Loss: 5.2940, LR: 0.172266\nEpoch [513/800], Batch [60/98], Loss: 5.2809, LR: 0.172266\nEpoch [513/800], Batch [70/98], Loss: 5.2771, LR: 0.172266\nEpoch [513/800], Batch [80/98], Loss: 5.2677, LR: 0.172266\nEpoch [513/800], Batch [90/98], Loss: 5.2735, LR: 0.172266\nEpoch [513/800] Average Loss: 5.2628\n\nEpoch [514/800], Batch [0/98], Loss: 5.2471, LR: 0.171201\nEpoch [514/800], Batch [10/98], Loss: 5.2570, LR: 0.171201\nEpoch [514/800], Batch [20/98], Loss: 5.2753, LR: 0.171201\nEpoch [514/800], Batch [30/98], Loss: 5.2699, LR: 0.171201\nEpoch [514/800], Batch [40/98], Loss: 5.2738, LR: 0.171201\nEpoch [514/800], Batch [50/98], Loss: 5.2818, LR: 0.171201\nEpoch [514/800], Batch [60/98], Loss: 5.2502, LR: 0.171201\nEpoch [514/800], Batch [70/98], Loss: 5.2749, LR: 0.171201\nEpoch [514/800], Batch [80/98], Loss: 5.2681, LR: 0.171201\nEpoch [514/800], Batch [90/98], Loss: 5.2724, LR: 0.171201\nEpoch [514/800] Average Loss: 5.2630\n\nEpoch [515/800], Batch [0/98], Loss: 5.2893, LR: 0.170138\nEpoch [515/800], Batch [10/98], Loss: 5.2686, LR: 0.170138\nEpoch [515/800], Batch [20/98], Loss: 5.2431, LR: 0.170138\nEpoch [515/800], Batch [30/98], Loss: 5.2575, LR: 0.170138\nEpoch [515/800], Batch [40/98], Loss: 5.2688, LR: 0.170138\nEpoch [515/800], Batch [50/98], Loss: 5.2643, LR: 0.170138\nEpoch [515/800], Batch [60/98], Loss: 5.2619, LR: 0.170138\nEpoch [515/800], Batch [70/98], Loss: 5.2783, LR: 0.170138\nEpoch [515/800], Batch [80/98], Loss: 5.2633, LR: 0.170138\nEpoch [515/800], Batch [90/98], Loss: 5.2630, LR: 0.170138\nEpoch [515/800] Average Loss: 5.2619\n\nEpoch [516/800], Batch [0/98], Loss: 5.2614, LR: 0.169077\nEpoch [516/800], Batch [10/98], Loss: 5.2594, LR: 0.169077\nEpoch [516/800], Batch [20/98], Loss: 5.2848, LR: 0.169077\nEpoch [516/800], Batch [30/98], Loss: 5.2595, LR: 0.169077\nEpoch [516/800], Batch [40/98], Loss: 5.2415, LR: 0.169077\nEpoch [516/800], Batch [50/98], Loss: 5.2609, LR: 0.169077\nEpoch [516/800], Batch [60/98], Loss: 5.2666, LR: 0.169077\nEpoch [516/800], Batch [70/98], Loss: 5.2848, LR: 0.169077\nEpoch [516/800], Batch [80/98], Loss: 5.2857, LR: 0.169077\nEpoch [516/800], Batch [90/98], Loss: 5.2442, LR: 0.169077\nEpoch [516/800] Average Loss: 5.2618\n\nEpoch [517/800], Batch [0/98], Loss: 5.2705, LR: 0.168018\nEpoch [517/800], Batch [10/98], Loss: 5.2477, LR: 0.168018\nEpoch [517/800], Batch [20/98], Loss: 5.2725, LR: 0.168018\nEpoch [517/800], Batch [30/98], Loss: 5.2612, LR: 0.168018\nEpoch [517/800], Batch [40/98], Loss: 5.2588, LR: 0.168018\nEpoch [517/800], Batch [50/98], Loss: 5.2412, LR: 0.168018\nEpoch [517/800], Batch [60/98], Loss: 5.2635, LR: 0.168018\nEpoch [517/800], Batch [70/98], Loss: 5.2442, LR: 0.168018\nEpoch [517/800], Batch [80/98], Loss: 5.2484, LR: 0.168018\nEpoch [517/800], Batch [90/98], Loss: 5.2541, LR: 0.168018\nEpoch [517/800] Average Loss: 5.2622\n\nEpoch [518/800], Batch [0/98], Loss: 5.2992, LR: 0.166961\nEpoch [518/800], Batch [10/98], Loss: 5.2543, LR: 0.166961\nEpoch [518/800], Batch [20/98], Loss: 5.2662, LR: 0.166961\nEpoch [518/800], Batch [30/98], Loss: 5.2588, LR: 0.166961\nEpoch [518/800], Batch [40/98], Loss: 5.2579, LR: 0.166961\nEpoch [518/800], Batch [50/98], Loss: 5.2764, LR: 0.166961\nEpoch [518/800], Batch [60/98], Loss: 5.2587, LR: 0.166961\nEpoch [518/800], Batch [70/98], Loss: 5.2520, LR: 0.166961\nEpoch [518/800], Batch [80/98], Loss: 5.2719, LR: 0.166961\nEpoch [518/800], Batch [90/98], Loss: 5.2911, LR: 0.166961\nEpoch [518/800] Average Loss: 5.2636\n\nEpoch [519/800], Batch [0/98], Loss: 5.2731, LR: 0.165906\nEpoch [519/800], Batch [10/98], Loss: 5.2901, LR: 0.165906\nEpoch [519/800], Batch [20/98], Loss: 5.2685, LR: 0.165906\nEpoch [519/800], Batch [30/98], Loss: 5.2710, LR: 0.165906\nEpoch [519/800], Batch [40/98], Loss: 5.2680, LR: 0.165906\nEpoch [519/800], Batch [50/98], Loss: 5.2515, LR: 0.165906\nEpoch [519/800], Batch [60/98], Loss: 5.2591, LR: 0.165906\nEpoch [519/800], Batch [70/98], Loss: 5.2681, LR: 0.165906\nEpoch [519/800], Batch [80/98], Loss: 5.2859, LR: 0.165906\nEpoch [519/800], Batch [90/98], Loss: 5.2662, LR: 0.165906\nEpoch [519/800] Average Loss: 5.2606\n\nEpoch [520/800], Batch [0/98], Loss: 5.2556, LR: 0.164854\nEpoch [520/800], Batch [10/98], Loss: 5.2559, LR: 0.164854\nEpoch [520/800], Batch [20/98], Loss: 5.2640, LR: 0.164854\nEpoch [520/800], Batch [30/98], Loss: 5.2609, LR: 0.164854\nEpoch [520/800], Batch [40/98], Loss: 5.2524, LR: 0.164854\nEpoch [520/800], Batch [50/98], Loss: 5.2420, LR: 0.164854\nEpoch [520/800], Batch [60/98], Loss: 5.2726, LR: 0.164854\nEpoch [520/800], Batch [70/98], Loss: 5.2414, LR: 0.164854\nEpoch [520/800], Batch [80/98], Loss: 5.2918, LR: 0.164854\nEpoch [520/800], Batch [90/98], Loss: 5.2801, LR: 0.164854\nEpoch [520/800] Average Loss: 5.2590\n\nEpoch [521/800], Batch [0/98], Loss: 5.2574, LR: 0.163803\nEpoch [521/800], Batch [10/98], Loss: 5.2650, LR: 0.163803\nEpoch [521/800], Batch [20/98], Loss: 5.2757, LR: 0.163803\nEpoch [521/800], Batch [30/98], Loss: 5.2527, LR: 0.163803\nEpoch [521/800], Batch [40/98], Loss: 5.2558, LR: 0.163803\nEpoch [521/800], Batch [50/98], Loss: 5.2555, LR: 0.163803\nEpoch [521/800], Batch [60/98], Loss: 5.2600, LR: 0.163803\nEpoch [521/800], Batch [70/98], Loss: 5.2430, LR: 0.163803\nEpoch [521/800], Batch [80/98], Loss: 5.2817, LR: 0.163803\nEpoch [521/800], Batch [90/98], Loss: 5.2954, LR: 0.163803\nEpoch [521/800] Average Loss: 5.2590\n\nEpoch [522/800], Batch [0/98], Loss: 5.2495, LR: 0.162754\nEpoch [522/800], Batch [10/98], Loss: 5.2762, LR: 0.162754\nEpoch [522/800], Batch [20/98], Loss: 5.2807, LR: 0.162754\nEpoch [522/800], Batch [30/98], Loss: 5.2693, LR: 0.162754\nEpoch [522/800], Batch [40/98], Loss: 5.2589, LR: 0.162754\nEpoch [522/800], Batch [50/98], Loss: 5.2703, LR: 0.162754\nEpoch [522/800], Batch [60/98], Loss: 5.2382, LR: 0.162754\nEpoch [522/800], Batch [70/98], Loss: 5.2655, LR: 0.162754\nEpoch [522/800], Batch [80/98], Loss: 5.2939, LR: 0.162754\nEpoch [522/800], Batch [90/98], Loss: 5.2655, LR: 0.162754\nEpoch [522/800] Average Loss: 5.2582\n\nEpoch [523/800], Batch [0/98], Loss: 5.2879, LR: 0.161708\nEpoch [523/800], Batch [10/98], Loss: 5.2655, LR: 0.161708\nEpoch [523/800], Batch [20/98], Loss: 5.2508, LR: 0.161708\nEpoch [523/800], Batch [30/98], Loss: 5.2509, LR: 0.161708\nEpoch [523/800], Batch [40/98], Loss: 5.2522, LR: 0.161708\nEpoch [523/800], Batch [50/98], Loss: 5.2739, LR: 0.161708\nEpoch [523/800], Batch [60/98], Loss: 5.2649, LR: 0.161708\nEpoch [523/800], Batch [70/98], Loss: 5.2735, LR: 0.161708\nEpoch [523/800], Batch [80/98], Loss: 5.2799, LR: 0.161708\nEpoch [523/800], Batch [90/98], Loss: 5.2615, LR: 0.161708\nEpoch [523/800] Average Loss: 5.2609\n\nEpoch [524/800], Batch [0/98], Loss: 5.2641, LR: 0.160663\nEpoch [524/800], Batch [10/98], Loss: 5.2582, LR: 0.160663\nEpoch [524/800], Batch [20/98], Loss: 5.2783, LR: 0.160663\nEpoch [524/800], Batch [30/98], Loss: 5.2539, LR: 0.160663\nEpoch [524/800], Batch [40/98], Loss: 5.2789, LR: 0.160663\nEpoch [524/800], Batch [50/98], Loss: 5.2682, LR: 0.160663\nEpoch [524/800], Batch [60/98], Loss: 5.2481, LR: 0.160663\nEpoch [524/800], Batch [70/98], Loss: 5.2738, LR: 0.160663\nEpoch [524/800], Batch [80/98], Loss: 5.2747, LR: 0.160663\nEpoch [524/800], Batch [90/98], Loss: 5.2381, LR: 0.160663\nEpoch [524/800] Average Loss: 5.2569\n\nEpoch [525/800], Batch [0/98], Loss: 5.2588, LR: 0.159621\nEpoch [525/800], Batch [10/98], Loss: 5.2806, LR: 0.159621\nEpoch [525/800], Batch [20/98], Loss: 5.2648, LR: 0.159621\nEpoch [525/800], Batch [30/98], Loss: 5.2789, LR: 0.159621\nEpoch [525/800], Batch [40/98], Loss: 5.2709, LR: 0.159621\nEpoch [525/800], Batch [50/98], Loss: 5.2554, LR: 0.159621\nEpoch [525/800], Batch [60/98], Loss: 5.2649, LR: 0.159621\nEpoch [525/800], Batch [70/98], Loss: 5.2894, LR: 0.159621\nEpoch [525/800], Batch [80/98], Loss: 5.2729, LR: 0.159621\nEpoch [525/800], Batch [90/98], Loss: 5.2562, LR: 0.159621\nEpoch [525/800] Average Loss: 5.2605\n\nEpoch [526/800], Batch [0/98], Loss: 5.2588, LR: 0.158581\nEpoch [526/800], Batch [10/98], Loss: 5.2565, LR: 0.158581\nEpoch [526/800], Batch [20/98], Loss: 5.2886, LR: 0.158581\nEpoch [526/800], Batch [30/98], Loss: 5.2696, LR: 0.158581\nEpoch [526/800], Batch [40/98], Loss: 5.2489, LR: 0.158581\nEpoch [526/800], Batch [50/98], Loss: 5.2532, LR: 0.158581\nEpoch [526/800], Batch [60/98], Loss: 5.2720, LR: 0.158581\nEpoch [526/800], Batch [70/98], Loss: 5.2674, LR: 0.158581\nEpoch [526/800], Batch [80/98], Loss: 5.2668, LR: 0.158581\nEpoch [526/800], Batch [90/98], Loss: 5.2784, LR: 0.158581\nEpoch [526/800] Average Loss: 5.2595\n\nEpoch [527/800], Batch [0/98], Loss: 5.2521, LR: 0.157543\nEpoch [527/800], Batch [10/98], Loss: 5.2861, LR: 0.157543\nEpoch [527/800], Batch [20/98], Loss: 5.2810, LR: 0.157543\nEpoch [527/800], Batch [30/98], Loss: 5.2524, LR: 0.157543\nEpoch [527/800], Batch [40/98], Loss: 5.2816, LR: 0.157543\nEpoch [527/800], Batch [50/98], Loss: 5.2784, LR: 0.157543\nEpoch [527/800], Batch [60/98], Loss: 5.2751, LR: 0.157543\nEpoch [527/800], Batch [70/98], Loss: 5.2898, LR: 0.157543\nEpoch [527/800], Batch [80/98], Loss: 5.2711, LR: 0.157543\nEpoch [527/800], Batch [90/98], Loss: 5.2775, LR: 0.157543\nEpoch [527/800] Average Loss: 5.2604\n\nEpoch [528/800], Batch [0/98], Loss: 5.2485, LR: 0.156507\nEpoch [528/800], Batch [10/98], Loss: 5.2380, LR: 0.156507\nEpoch [528/800], Batch [20/98], Loss: 5.2740, LR: 0.156507\nEpoch [528/800], Batch [30/98], Loss: 5.2968, LR: 0.156507\nEpoch [528/800], Batch [40/98], Loss: 5.2747, LR: 0.156507\nEpoch [528/800], Batch [50/98], Loss: 5.2603, LR: 0.156507\nEpoch [528/800], Batch [60/98], Loss: 5.2637, LR: 0.156507\nEpoch [528/800], Batch [70/98], Loss: 5.2533, LR: 0.156507\nEpoch [528/800], Batch [80/98], Loss: 5.2786, LR: 0.156507\nEpoch [528/800], Batch [90/98], Loss: 5.2567, LR: 0.156507\nEpoch [528/800] Average Loss: 5.2565\n\nEpoch [529/800], Batch [0/98], Loss: 5.2698, LR: 0.155474\nEpoch [529/800], Batch [10/98], Loss: 5.2489, LR: 0.155474\nEpoch [529/800], Batch [20/98], Loss: 5.2899, LR: 0.155474\nEpoch [529/800], Batch [30/98], Loss: 5.2656, LR: 0.155474\nEpoch [529/800], Batch [40/98], Loss: 5.2559, LR: 0.155474\nEpoch [529/800], Batch [50/98], Loss: 5.2604, LR: 0.155474\nEpoch [529/800], Batch [60/98], Loss: 5.2520, LR: 0.155474\nEpoch [529/800], Batch [70/98], Loss: 5.2603, LR: 0.155474\nEpoch [529/800], Batch [80/98], Loss: 5.2815, LR: 0.155474\nEpoch [529/800], Batch [90/98], Loss: 5.2511, LR: 0.155474\nEpoch [529/800] Average Loss: 5.2587\n\nEpoch [530/800], Batch [0/98], Loss: 5.2623, LR: 0.154443\nEpoch [530/800], Batch [10/98], Loss: 5.2637, LR: 0.154443\nEpoch [530/800], Batch [20/98], Loss: 5.2536, LR: 0.154443\nEpoch [530/800], Batch [30/98], Loss: 5.2594, LR: 0.154443\nEpoch [530/800], Batch [40/98], Loss: 5.2679, LR: 0.154443\nEpoch [530/800], Batch [50/98], Loss: 5.2699, LR: 0.154443\nEpoch [530/800], Batch [60/98], Loss: 5.2782, LR: 0.154443\nEpoch [530/800], Batch [70/98], Loss: 5.2513, LR: 0.154443\nEpoch [530/800], Batch [80/98], Loss: 5.2721, LR: 0.154443\nEpoch [530/800], Batch [90/98], Loss: 5.2352, LR: 0.154443\nEpoch [530/800] Average Loss: 5.2557\n\nEpoch [531/800], Batch [0/98], Loss: 5.2797, LR: 0.153414\nEpoch [531/800], Batch [10/98], Loss: 5.2736, LR: 0.153414\nEpoch [531/800], Batch [20/98], Loss: 5.2602, LR: 0.153414\nEpoch [531/800], Batch [30/98], Loss: 5.2769, LR: 0.153414\nEpoch [531/800], Batch [40/98], Loss: 5.2593, LR: 0.153414\nEpoch [531/800], Batch [50/98], Loss: 5.2789, LR: 0.153414\nEpoch [531/800], Batch [60/98], Loss: 5.2738, LR: 0.153414\nEpoch [531/800], Batch [70/98], Loss: 5.2452, LR: 0.153414\nEpoch [531/800], Batch [80/98], Loss: 5.2651, LR: 0.153414\nEpoch [531/800], Batch [90/98], Loss: 5.2578, LR: 0.153414\nEpoch [531/800] Average Loss: 5.2580\n\nEpoch [532/800], Batch [0/98], Loss: 5.2514, LR: 0.152387\nEpoch [532/800], Batch [10/98], Loss: 5.2678, LR: 0.152387\nEpoch [532/800], Batch [20/98], Loss: 5.2694, LR: 0.152387\nEpoch [532/800], Batch [30/98], Loss: 5.2604, LR: 0.152387\nEpoch [532/800], Batch [40/98], Loss: 5.2585, LR: 0.152387\nEpoch [532/800], Batch [50/98], Loss: 5.2755, LR: 0.152387\nEpoch [532/800], Batch [60/98], Loss: 5.2591, LR: 0.152387\nEpoch [532/800], Batch [70/98], Loss: 5.2758, LR: 0.152387\nEpoch [532/800], Batch [80/98], Loss: 5.2297, LR: 0.152387\nEpoch [532/800], Batch [90/98], Loss: 5.2531, LR: 0.152387\nEpoch [532/800] Average Loss: 5.2592\n\nEpoch [533/800], Batch [0/98], Loss: 5.2582, LR: 0.151362\nEpoch [533/800], Batch [10/98], Loss: 5.2648, LR: 0.151362\nEpoch [533/800], Batch [20/98], Loss: 5.2648, LR: 0.151362\nEpoch [533/800], Batch [30/98], Loss: 5.2778, LR: 0.151362\nEpoch [533/800], Batch [40/98], Loss: 5.2798, LR: 0.151362\nEpoch [533/800], Batch [50/98], Loss: 5.2637, LR: 0.151362\nEpoch [533/800], Batch [60/98], Loss: 5.2745, LR: 0.151362\nEpoch [533/800], Batch [70/98], Loss: 5.2606, LR: 0.151362\nEpoch [533/800], Batch [80/98], Loss: 5.2584, LR: 0.151362\nEpoch [533/800], Batch [90/98], Loss: 5.2711, LR: 0.151362\nEpoch [533/800] Average Loss: 5.2585\n\nEpoch [534/800], Batch [0/98], Loss: 5.2584, LR: 0.150340\nEpoch [534/800], Batch [10/98], Loss: 5.2361, LR: 0.150340\nEpoch [534/800], Batch [20/98], Loss: 5.2617, LR: 0.150340\nEpoch [534/800], Batch [30/98], Loss: 5.2799, LR: 0.150340\nEpoch [534/800], Batch [40/98], Loss: 5.2860, LR: 0.150340\nEpoch [534/800], Batch [50/98], Loss: 5.2468, LR: 0.150340\nEpoch [534/800], Batch [60/98], Loss: 5.2529, LR: 0.150340\nEpoch [534/800], Batch [70/98], Loss: 5.2503, LR: 0.150340\nEpoch [534/800], Batch [80/98], Loss: 5.2606, LR: 0.150340\nEpoch [534/800], Batch [90/98], Loss: 5.2627, LR: 0.150340\nEpoch [534/800] Average Loss: 5.2584\n\nEpoch [535/800], Batch [0/98], Loss: 5.2541, LR: 0.149320\nEpoch [535/800], Batch [10/98], Loss: 5.2564, LR: 0.149320\nEpoch [535/800], Batch [20/98], Loss: 5.2624, LR: 0.149320\nEpoch [535/800], Batch [30/98], Loss: 5.2555, LR: 0.149320\nEpoch [535/800], Batch [40/98], Loss: 5.2508, LR: 0.149320\nEpoch [535/800], Batch [50/98], Loss: 5.2679, LR: 0.149320\nEpoch [535/800], Batch [60/98], Loss: 5.2590, LR: 0.149320\nEpoch [535/800], Batch [70/98], Loss: 5.2623, LR: 0.149320\nEpoch [535/800], Batch [80/98], Loss: 5.2829, LR: 0.149320\nEpoch [535/800], Batch [90/98], Loss: 5.2848, LR: 0.149320\nEpoch [535/800] Average Loss: 5.2563\n\nEpoch [536/800], Batch [0/98], Loss: 5.2852, LR: 0.148303\nEpoch [536/800], Batch [10/98], Loss: 5.2579, LR: 0.148303\nEpoch [536/800], Batch [20/98], Loss: 5.2667, LR: 0.148303\nEpoch [536/800], Batch [30/98], Loss: 5.2648, LR: 0.148303\nEpoch [536/800], Batch [40/98], Loss: 5.2942, LR: 0.148303\nEpoch [536/800], Batch [50/98], Loss: 5.2899, LR: 0.148303\nEpoch [536/800], Batch [60/98], Loss: 5.2733, LR: 0.148303\nEpoch [536/800], Batch [70/98], Loss: 5.2772, LR: 0.148303\nEpoch [536/800], Batch [80/98], Loss: 5.2621, LR: 0.148303\nEpoch [536/800], Batch [90/98], Loss: 5.2599, LR: 0.148303\nEpoch [536/800] Average Loss: 5.2604\n\nEpoch [537/800], Batch [0/98], Loss: 5.2703, LR: 0.147288\nEpoch [537/800], Batch [10/98], Loss: 5.2301, LR: 0.147288\nEpoch [537/800], Batch [20/98], Loss: 5.2793, LR: 0.147288\nEpoch [537/800], Batch [30/98], Loss: 5.2463, LR: 0.147288\nEpoch [537/800], Batch [40/98], Loss: 5.2637, LR: 0.147288\nEpoch [537/800], Batch [50/98], Loss: 5.2642, LR: 0.147288\nEpoch [537/800], Batch [60/98], Loss: 5.2604, LR: 0.147288\nEpoch [537/800], Batch [70/98], Loss: 5.2668, LR: 0.147288\nEpoch [537/800], Batch [80/98], Loss: 5.2860, LR: 0.147288\nEpoch [537/800], Batch [90/98], Loss: 5.2467, LR: 0.147288\nEpoch [537/800] Average Loss: 5.2590\n\nEpoch [538/800], Batch [0/98], Loss: 5.2695, LR: 0.146275\nEpoch [538/800], Batch [10/98], Loss: 5.2748, LR: 0.146275\nEpoch [538/800], Batch [20/98], Loss: 5.2676, LR: 0.146275\nEpoch [538/800], Batch [30/98], Loss: 5.2707, LR: 0.146275\nEpoch [538/800], Batch [40/98], Loss: 5.2400, LR: 0.146275\nEpoch [538/800], Batch [50/98], Loss: 5.2554, LR: 0.146275\nEpoch [538/800], Batch [60/98], Loss: 5.2498, LR: 0.146275\nEpoch [538/800], Batch [70/98], Loss: 5.2391, LR: 0.146275\nEpoch [538/800], Batch [80/98], Loss: 5.2458, LR: 0.146275\nEpoch [538/800], Batch [90/98], Loss: 5.2750, LR: 0.146275\nEpoch [538/800] Average Loss: 5.2572\n\nEpoch [539/800], Batch [0/98], Loss: 5.2857, LR: 0.145264\nEpoch [539/800], Batch [10/98], Loss: 5.2615, LR: 0.145264\nEpoch [539/800], Batch [20/98], Loss: 5.2566, LR: 0.145264\nEpoch [539/800], Batch [30/98], Loss: 5.2716, LR: 0.145264\nEpoch [539/800], Batch [40/98], Loss: 5.2650, LR: 0.145264\nEpoch [539/800], Batch [50/98], Loss: 5.2528, LR: 0.145264\nEpoch [539/800], Batch [60/98], Loss: 5.2357, LR: 0.145264\nEpoch [539/800], Batch [70/98], Loss: 5.2575, LR: 0.145264\nEpoch [539/800], Batch [80/98], Loss: 5.2702, LR: 0.145264\nEpoch [539/800], Batch [90/98], Loss: 5.2623, LR: 0.145264\nEpoch [539/800] Average Loss: 5.2550\n\nEpoch [540/800], Batch [0/98], Loss: 5.2527, LR: 0.144256\nEpoch [540/800], Batch [10/98], Loss: 5.2319, LR: 0.144256\nEpoch [540/800], Batch [20/98], Loss: 5.2753, LR: 0.144256\nEpoch [540/800], Batch [30/98], Loss: 5.2711, LR: 0.144256\nEpoch [540/800], Batch [40/98], Loss: 5.2535, LR: 0.144256\nEpoch [540/800], Batch [50/98], Loss: 5.2574, LR: 0.144256\nEpoch [540/800], Batch [60/98], Loss: 5.2520, LR: 0.144256\nEpoch [540/800], Batch [70/98], Loss: 5.2749, LR: 0.144256\nEpoch [540/800], Batch [80/98], Loss: 5.2516, LR: 0.144256\nEpoch [540/800], Batch [90/98], Loss: 5.2778, LR: 0.144256\nEpoch [540/800] Average Loss: 5.2568\n\nEpoch [541/800], Batch [0/98], Loss: 5.2351, LR: 0.143250\nEpoch [541/800], Batch [10/98], Loss: 5.2547, LR: 0.143250\nEpoch [541/800], Batch [20/98], Loss: 5.2633, LR: 0.143250\nEpoch [541/800], Batch [30/98], Loss: 5.2481, LR: 0.143250\nEpoch [541/800], Batch [40/98], Loss: 5.2900, LR: 0.143250\nEpoch [541/800], Batch [50/98], Loss: 5.2754, LR: 0.143250\nEpoch [541/800], Batch [60/98], Loss: 5.2519, LR: 0.143250\nEpoch [541/800], Batch [70/98], Loss: 5.2441, LR: 0.143250\nEpoch [541/800], Batch [80/98], Loss: 5.2615, LR: 0.143250\nEpoch [541/800], Batch [90/98], Loss: 5.2607, LR: 0.143250\nEpoch [541/800] Average Loss: 5.2563\n\nEpoch [542/800], Batch [0/98], Loss: 5.2614, LR: 0.142247\nEpoch [542/800], Batch [10/98], Loss: 5.2607, LR: 0.142247\nEpoch [542/800], Batch [20/98], Loss: 5.2484, LR: 0.142247\nEpoch [542/800], Batch [30/98], Loss: 5.2584, LR: 0.142247\nEpoch [542/800], Batch [40/98], Loss: 5.2645, LR: 0.142247\nEpoch [542/800], Batch [50/98], Loss: 5.2772, LR: 0.142247\nEpoch [542/800], Batch [60/98], Loss: 5.2557, LR: 0.142247\nEpoch [542/800], Batch [70/98], Loss: 5.2547, LR: 0.142247\nEpoch [542/800], Batch [80/98], Loss: 5.2465, LR: 0.142247\nEpoch [542/800], Batch [90/98], Loss: 5.2544, LR: 0.142247\nEpoch [542/800] Average Loss: 5.2578\n\nEpoch [543/800], Batch [0/98], Loss: 5.2351, LR: 0.141246\nEpoch [543/800], Batch [10/98], Loss: 5.2668, LR: 0.141246\nEpoch [543/800], Batch [20/98], Loss: 5.2819, LR: 0.141246\nEpoch [543/800], Batch [30/98], Loss: 5.2201, LR: 0.141246\nEpoch [543/800], Batch [40/98], Loss: 5.2593, LR: 0.141246\nEpoch [543/800], Batch [50/98], Loss: 5.2461, LR: 0.141246\nEpoch [543/800], Batch [60/98], Loss: 5.2713, LR: 0.141246\nEpoch [543/800], Batch [70/98], Loss: 5.2543, LR: 0.141246\nEpoch [543/800], Batch [80/98], Loss: 5.2867, LR: 0.141246\nEpoch [543/800], Batch [90/98], Loss: 5.2595, LR: 0.141246\nEpoch [543/800] Average Loss: 5.2562\n\nEpoch [544/800], Batch [0/98], Loss: 5.2756, LR: 0.140248\nEpoch [544/800], Batch [10/98], Loss: 5.2618, LR: 0.140248\nEpoch [544/800], Batch [20/98], Loss: 5.2590, LR: 0.140248\nEpoch [544/800], Batch [30/98], Loss: 5.2676, LR: 0.140248\nEpoch [544/800], Batch [40/98], Loss: 5.2705, LR: 0.140248\nEpoch [544/800], Batch [50/98], Loss: 5.2501, LR: 0.140248\nEpoch [544/800], Batch [60/98], Loss: 5.2734, LR: 0.140248\nEpoch [544/800], Batch [70/98], Loss: 5.2767, LR: 0.140248\nEpoch [544/800], Batch [80/98], Loss: 5.2948, LR: 0.140248\nEpoch [544/800], Batch [90/98], Loss: 5.2648, LR: 0.140248\nEpoch [544/800] Average Loss: 5.2567\n\nEpoch [545/800], Batch [0/98], Loss: 5.2564, LR: 0.139252\nEpoch [545/800], Batch [10/98], Loss: 5.2856, LR: 0.139252\nEpoch [545/800], Batch [20/98], Loss: 5.2779, LR: 0.139252\nEpoch [545/800], Batch [30/98], Loss: 5.2838, LR: 0.139252\nEpoch [545/800], Batch [40/98], Loss: 5.2470, LR: 0.139252\nEpoch [545/800], Batch [50/98], Loss: 5.2493, LR: 0.139252\nEpoch [545/800], Batch [60/98], Loss: 5.2683, LR: 0.139252\nEpoch [545/800], Batch [70/98], Loss: 5.2384, LR: 0.139252\nEpoch [545/800], Batch [80/98], Loss: 5.2477, LR: 0.139252\nEpoch [545/800], Batch [90/98], Loss: 5.2720, LR: 0.139252\nEpoch [545/800] Average Loss: 5.2599\n\nEpoch [546/800], Batch [0/98], Loss: 5.2740, LR: 0.138259\nEpoch [546/800], Batch [10/98], Loss: 5.2508, LR: 0.138259\nEpoch [546/800], Batch [20/98], Loss: 5.2541, LR: 0.138259\nEpoch [546/800], Batch [30/98], Loss: 5.2668, LR: 0.138259\nEpoch [546/800], Batch [40/98], Loss: 5.2688, LR: 0.138259\nEpoch [546/800], Batch [50/98], Loss: 5.2644, LR: 0.138259\nEpoch [546/800], Batch [60/98], Loss: 5.2569, LR: 0.138259\nEpoch [546/800], Batch [70/98], Loss: 5.2765, LR: 0.138259\nEpoch [546/800], Batch [80/98], Loss: 5.2767, LR: 0.138259\nEpoch [546/800], Batch [90/98], Loss: 5.2479, LR: 0.138259\nEpoch [546/800] Average Loss: 5.2561\n\nEpoch [547/800], Batch [0/98], Loss: 5.2266, LR: 0.137268\nEpoch [547/800], Batch [10/98], Loss: 5.2547, LR: 0.137268\nEpoch [547/800], Batch [20/98], Loss: 5.2534, LR: 0.137268\nEpoch [547/800], Batch [30/98], Loss: 5.2482, LR: 0.137268\nEpoch [547/800], Batch [40/98], Loss: 5.2352, LR: 0.137268\nEpoch [547/800], Batch [50/98], Loss: 5.2713, LR: 0.137268\nEpoch [547/800], Batch [60/98], Loss: 5.2565, LR: 0.137268\nEpoch [547/800], Batch [70/98], Loss: 5.2773, LR: 0.137268\nEpoch [547/800], Batch [80/98], Loss: 5.2598, LR: 0.137268\nEpoch [547/800], Batch [90/98], Loss: 5.2567, LR: 0.137268\nEpoch [547/800] Average Loss: 5.2555\n\nEpoch [548/800], Batch [0/98], Loss: 5.2730, LR: 0.136279\nEpoch [548/800], Batch [10/98], Loss: 5.2520, LR: 0.136279\nEpoch [548/800], Batch [20/98], Loss: 5.2495, LR: 0.136279\nEpoch [548/800], Batch [30/98], Loss: 5.2686, LR: 0.136279\nEpoch [548/800], Batch [40/98], Loss: 5.2730, LR: 0.136279\nEpoch [548/800], Batch [50/98], Loss: 5.2627, LR: 0.136279\nEpoch [548/800], Batch [60/98], Loss: 5.2641, LR: 0.136279\nEpoch [548/800], Batch [70/98], Loss: 5.2695, LR: 0.136279\nEpoch [548/800], Batch [80/98], Loss: 5.2528, LR: 0.136279\nEpoch [548/800], Batch [90/98], Loss: 5.2647, LR: 0.136279\nEpoch [548/800] Average Loss: 5.2576\n\nEpoch [549/800], Batch [0/98], Loss: 5.2529, LR: 0.135293\nEpoch [549/800], Batch [10/98], Loss: 5.2574, LR: 0.135293\nEpoch [549/800], Batch [20/98], Loss: 5.2620, LR: 0.135293\nEpoch [549/800], Batch [30/98], Loss: 5.2614, LR: 0.135293\nEpoch [549/800], Batch [40/98], Loss: 5.2735, LR: 0.135293\nEpoch [549/800], Batch [50/98], Loss: 5.2405, LR: 0.135293\nEpoch [549/800], Batch [60/98], Loss: 5.2417, LR: 0.135293\nEpoch [549/800], Batch [70/98], Loss: 5.2620, LR: 0.135293\nEpoch [549/800], Batch [80/98], Loss: 5.2757, LR: 0.135293\nEpoch [549/800], Batch [90/98], Loss: 5.2569, LR: 0.135293\nEpoch [549/800] Average Loss: 5.2553\n\nEpoch [550/800], Batch [0/98], Loss: 5.2680, LR: 0.134310\nEpoch [550/800], Batch [10/98], Loss: 5.2512, LR: 0.134310\nEpoch [550/800], Batch [20/98], Loss: 5.2393, LR: 0.134310\nEpoch [550/800], Batch [30/98], Loss: 5.2310, LR: 0.134310\nEpoch [550/800], Batch [40/98], Loss: 5.2742, LR: 0.134310\nEpoch [550/800], Batch [50/98], Loss: 5.2515, LR: 0.134310\nEpoch [550/800], Batch [60/98], Loss: 5.2568, LR: 0.134310\nEpoch [550/800], Batch [70/98], Loss: 5.2539, LR: 0.134310\nEpoch [550/800], Batch [80/98], Loss: 5.2809, LR: 0.134310\nEpoch [550/800], Batch [90/98], Loss: 5.2598, LR: 0.134310\nEpoch [550/800] Average Loss: 5.2557\n\nEpoch [551/800], Batch [0/98], Loss: 5.2841, LR: 0.133329\nEpoch [551/800], Batch [10/98], Loss: 5.2653, LR: 0.133329\nEpoch [551/800], Batch [20/98], Loss: 5.2540, LR: 0.133329\nEpoch [551/800], Batch [30/98], Loss: 5.2531, LR: 0.133329\nEpoch [551/800], Batch [40/98], Loss: 5.2532, LR: 0.133329\nEpoch [551/800], Batch [50/98], Loss: 5.2543, LR: 0.133329\nEpoch [551/800], Batch [60/98], Loss: 5.2471, LR: 0.133329\nEpoch [551/800], Batch [70/98], Loss: 5.2347, LR: 0.133329\nEpoch [551/800], Batch [80/98], Loss: 5.2605, LR: 0.133329\nEpoch [551/800], Batch [90/98], Loss: 5.2427, LR: 0.133329\nEpoch [551/800] Average Loss: 5.2537\n\nEpoch [552/800], Batch [0/98], Loss: 5.2842, LR: 0.132351\nEpoch [552/800], Batch [10/98], Loss: 5.2323, LR: 0.132351\nEpoch [552/800], Batch [20/98], Loss: 5.2691, LR: 0.132351\nEpoch [552/800], Batch [30/98], Loss: 5.2611, LR: 0.132351\nEpoch [552/800], Batch [40/98], Loss: 5.2848, LR: 0.132351\nEpoch [552/800], Batch [50/98], Loss: 5.2545, LR: 0.132351\nEpoch [552/800], Batch [60/98], Loss: 5.2767, LR: 0.132351\nEpoch [552/800], Batch [70/98], Loss: 5.2743, LR: 0.132351\nEpoch [552/800], Batch [80/98], Loss: 5.2847, LR: 0.132351\nEpoch [552/800], Batch [90/98], Loss: 5.2603, LR: 0.132351\nEpoch [552/800] Average Loss: 5.2588\n\nEpoch [553/800], Batch [0/98], Loss: 5.2593, LR: 0.131375\nEpoch [553/800], Batch [10/98], Loss: 5.2753, LR: 0.131375\nEpoch [553/800], Batch [20/98], Loss: 5.2592, LR: 0.131375\nEpoch [553/800], Batch [30/98], Loss: 5.2580, LR: 0.131375\nEpoch [553/800], Batch [40/98], Loss: 5.2505, LR: 0.131375\nEpoch [553/800], Batch [50/98], Loss: 5.2453, LR: 0.131375\nEpoch [553/800], Batch [60/98], Loss: 5.2866, LR: 0.131375\nEpoch [553/800], Batch [70/98], Loss: 5.2616, LR: 0.131375\nEpoch [553/800], Batch [80/98], Loss: 5.2480, LR: 0.131375\nEpoch [553/800], Batch [90/98], Loss: 5.2738, LR: 0.131375\nEpoch [553/800] Average Loss: 5.2561\n\nEpoch [554/800], Batch [0/98], Loss: 5.2749, LR: 0.130402\nEpoch [554/800], Batch [10/98], Loss: 5.2545, LR: 0.130402\nEpoch [554/800], Batch [20/98], Loss: 5.2752, LR: 0.130402\nEpoch [554/800], Batch [30/98], Loss: 5.2811, LR: 0.130402\nEpoch [554/800], Batch [40/98], Loss: 5.2597, LR: 0.130402\nEpoch [554/800], Batch [50/98], Loss: 5.2486, LR: 0.130402\nEpoch [554/800], Batch [60/98], Loss: 5.2367, LR: 0.130402\nEpoch [554/800], Batch [70/98], Loss: 5.2625, LR: 0.130402\nEpoch [554/800], Batch [80/98], Loss: 5.2247, LR: 0.130402\nEpoch [554/800], Batch [90/98], Loss: 5.2563, LR: 0.130402\nEpoch [554/800] Average Loss: 5.2547\n\nEpoch [555/800], Batch [0/98], Loss: 5.2392, LR: 0.129431\nEpoch [555/800], Batch [10/98], Loss: 5.2397, LR: 0.129431\nEpoch [555/800], Batch [20/98], Loss: 5.2565, LR: 0.129431\nEpoch [555/800], Batch [30/98], Loss: 5.2533, LR: 0.129431\nEpoch [555/800], Batch [40/98], Loss: 5.2523, LR: 0.129431\nEpoch [555/800], Batch [50/98], Loss: 5.2608, LR: 0.129431\nEpoch [555/800], Batch [60/98], Loss: 5.2784, LR: 0.129431\nEpoch [555/800], Batch [70/98], Loss: 5.2526, LR: 0.129431\nEpoch [555/800], Batch [80/98], Loss: 5.2795, LR: 0.129431\nEpoch [555/800], Batch [90/98], Loss: 5.2539, LR: 0.129431\nEpoch [555/800] Average Loss: 5.2549\n\nEpoch [556/800], Batch [0/98], Loss: 5.2421, LR: 0.128464\nEpoch [556/800], Batch [10/98], Loss: 5.2624, LR: 0.128464\nEpoch [556/800], Batch [20/98], Loss: 5.2640, LR: 0.128464\nEpoch [556/800], Batch [30/98], Loss: 5.2277, LR: 0.128464\nEpoch [556/800], Batch [40/98], Loss: 5.2532, LR: 0.128464\nEpoch [556/800], Batch [50/98], Loss: 5.2706, LR: 0.128464\nEpoch [556/800], Batch [60/98], Loss: 5.2555, LR: 0.128464\nEpoch [556/800], Batch [70/98], Loss: 5.2562, LR: 0.128464\nEpoch [556/800], Batch [80/98], Loss: 5.2642, LR: 0.128464\nEpoch [556/800], Batch [90/98], Loss: 5.2381, LR: 0.128464\nEpoch [556/800] Average Loss: 5.2552\n\nEpoch [557/800], Batch [0/98], Loss: 5.2499, LR: 0.127498\nEpoch [557/800], Batch [10/98], Loss: 5.2629, LR: 0.127498\nEpoch [557/800], Batch [20/98], Loss: 5.2673, LR: 0.127498\nEpoch [557/800], Batch [30/98], Loss: 5.2792, LR: 0.127498\nEpoch [557/800], Batch [40/98], Loss: 5.2603, LR: 0.127498\nEpoch [557/800], Batch [50/98], Loss: 5.2649, LR: 0.127498\nEpoch [557/800], Batch [60/98], Loss: 5.2593, LR: 0.127498\nEpoch [557/800], Batch [70/98], Loss: 5.2455, LR: 0.127498\nEpoch [557/800], Batch [80/98], Loss: 5.2531, LR: 0.127498\nEpoch [557/800], Batch [90/98], Loss: 5.2595, LR: 0.127498\nEpoch [557/800] Average Loss: 5.2568\n\nEpoch [558/800], Batch [0/98], Loss: 5.2478, LR: 0.126536\nEpoch [558/800], Batch [10/98], Loss: 5.2300, LR: 0.126536\nEpoch [558/800], Batch [20/98], Loss: 5.2485, LR: 0.126536\nEpoch [558/800], Batch [30/98], Loss: 5.2732, LR: 0.126536\nEpoch [558/800], Batch [40/98], Loss: 5.2603, LR: 0.126536\nEpoch [558/800], Batch [50/98], Loss: 5.2562, LR: 0.126536\nEpoch [558/800], Batch [60/98], Loss: 5.2563, LR: 0.126536\nEpoch [558/800], Batch [70/98], Loss: 5.2722, LR: 0.126536\nEpoch [558/800], Batch [80/98], Loss: 5.2678, LR: 0.126536\nEpoch [558/800], Batch [90/98], Loss: 5.2594, LR: 0.126536\nEpoch [558/800] Average Loss: 5.2530\n\nEpoch [559/800], Batch [0/98], Loss: 5.2604, LR: 0.125576\nEpoch [559/800], Batch [10/98], Loss: 5.2495, LR: 0.125576\nEpoch [559/800], Batch [20/98], Loss: 5.2810, LR: 0.125576\nEpoch [559/800], Batch [30/98], Loss: 5.2338, LR: 0.125576\nEpoch [559/800], Batch [40/98], Loss: 5.2595, LR: 0.125576\nEpoch [559/800], Batch [50/98], Loss: 5.2525, LR: 0.125576\nEpoch [559/800], Batch [60/98], Loss: 5.2605, LR: 0.125576\nEpoch [559/800], Batch [70/98], Loss: 5.2860, LR: 0.125576\nEpoch [559/800], Batch [80/98], Loss: 5.2621, LR: 0.125576\nEpoch [559/800], Batch [90/98], Loss: 5.2811, LR: 0.125576\nEpoch [559/800] Average Loss: 5.2564\n\nEpoch [560/800], Batch [0/98], Loss: 5.2461, LR: 0.124619\nEpoch [560/800], Batch [10/98], Loss: 5.2698, LR: 0.124619\nEpoch [560/800], Batch [20/98], Loss: 5.2445, LR: 0.124619\nEpoch [560/800], Batch [30/98], Loss: 5.2587, LR: 0.124619\nEpoch [560/800], Batch [40/98], Loss: 5.2379, LR: 0.124619\nEpoch [560/800], Batch [50/98], Loss: 5.2796, LR: 0.124619\nEpoch [560/800], Batch [60/98], Loss: 5.2578, LR: 0.124619\nEpoch [560/800], Batch [70/98], Loss: 5.2537, LR: 0.124619\nEpoch [560/800], Batch [80/98], Loss: 5.2803, LR: 0.124619\nEpoch [560/800], Batch [90/98], Loss: 5.2633, LR: 0.124619\nEpoch [560/800] Average Loss: 5.2540\n\nEpoch [561/800], Batch [0/98], Loss: 5.2634, LR: 0.123664\nEpoch [561/800], Batch [10/98], Loss: 5.2488, LR: 0.123664\nEpoch [561/800], Batch [20/98], Loss: 5.2503, LR: 0.123664\nEpoch [561/800], Batch [30/98], Loss: 5.2917, LR: 0.123664\nEpoch [561/800], Batch [40/98], Loss: 5.2737, LR: 0.123664\nEpoch [561/800], Batch [50/98], Loss: 5.2531, LR: 0.123664\nEpoch [561/800], Batch [60/98], Loss: 5.2335, LR: 0.123664\nEpoch [561/800], Batch [70/98], Loss: 5.2596, LR: 0.123664\nEpoch [561/800], Batch [80/98], Loss: 5.2739, LR: 0.123664\nEpoch [561/800], Batch [90/98], Loss: 5.2703, LR: 0.123664\nEpoch [561/800] Average Loss: 5.2537\n\nEpoch [562/800], Batch [0/98], Loss: 5.2423, LR: 0.122713\nEpoch [562/800], Batch [10/98], Loss: 5.2539, LR: 0.122713\nEpoch [562/800], Batch [20/98], Loss: 5.2455, LR: 0.122713\nEpoch [562/800], Batch [30/98], Loss: 5.2923, LR: 0.122713\nEpoch [562/800], Batch [40/98], Loss: 5.2515, LR: 0.122713\nEpoch [562/800], Batch [50/98], Loss: 5.2497, LR: 0.122713\nEpoch [562/800], Batch [60/98], Loss: 5.2474, LR: 0.122713\nEpoch [562/800], Batch [70/98], Loss: 5.2589, LR: 0.122713\nEpoch [562/800], Batch [80/98], Loss: 5.2840, LR: 0.122713\nEpoch [562/800], Batch [90/98], Loss: 5.2574, LR: 0.122713\nEpoch [562/800] Average Loss: 5.2554\n\nEpoch [563/800], Batch [0/98], Loss: 5.2623, LR: 0.121764\nEpoch [563/800], Batch [10/98], Loss: 5.2376, LR: 0.121764\nEpoch [563/800], Batch [20/98], Loss: 5.2578, LR: 0.121764\nEpoch [563/800], Batch [30/98], Loss: 5.2571, LR: 0.121764\nEpoch [563/800], Batch [40/98], Loss: 5.2727, LR: 0.121764\nEpoch [563/800], Batch [50/98], Loss: 5.2335, LR: 0.121764\nEpoch [563/800], Batch [60/98], Loss: 5.2461, LR: 0.121764\nEpoch [563/800], Batch [70/98], Loss: 5.2802, LR: 0.121764\nEpoch [563/800], Batch [80/98], Loss: 5.2539, LR: 0.121764\nEpoch [563/800], Batch [90/98], Loss: 5.2583, LR: 0.121764\nEpoch [563/800] Average Loss: 5.2557\n\nEpoch [564/800], Batch [0/98], Loss: 5.2668, LR: 0.120817\nEpoch [564/800], Batch [10/98], Loss: 5.2423, LR: 0.120817\nEpoch [564/800], Batch [20/98], Loss: 5.2506, LR: 0.120817\nEpoch [564/800], Batch [30/98], Loss: 5.2584, LR: 0.120817\nEpoch [564/800], Batch [40/98], Loss: 5.2519, LR: 0.120817\nEpoch [564/800], Batch [50/98], Loss: 5.2593, LR: 0.120817\nEpoch [564/800], Batch [60/98], Loss: 5.2448, LR: 0.120817\nEpoch [564/800], Batch [70/98], Loss: 5.2775, LR: 0.120817\nEpoch [564/800], Batch [80/98], Loss: 5.2695, LR: 0.120817\nEpoch [564/800], Batch [90/98], Loss: 5.2488, LR: 0.120817\nEpoch [564/800] Average Loss: 5.2533\n\nEpoch [565/800], Batch [0/98], Loss: 5.2453, LR: 0.119874\nEpoch [565/800], Batch [10/98], Loss: 5.2572, LR: 0.119874\nEpoch [565/800], Batch [20/98], Loss: 5.2628, LR: 0.119874\nEpoch [565/800], Batch [30/98], Loss: 5.2636, LR: 0.119874\nEpoch [565/800], Batch [40/98], Loss: 5.2752, LR: 0.119874\nEpoch [565/800], Batch [50/98], Loss: 5.2480, LR: 0.119874\nEpoch [565/800], Batch [60/98], Loss: 5.2595, LR: 0.119874\nEpoch [565/800], Batch [70/98], Loss: 5.2387, LR: 0.119874\nEpoch [565/800], Batch [80/98], Loss: 5.2573, LR: 0.119874\nEpoch [565/800], Batch [90/98], Loss: 5.2535, LR: 0.119874\nEpoch [565/800] Average Loss: 5.2544\n\nEpoch [566/800], Batch [0/98], Loss: 5.2749, LR: 0.118933\nEpoch [566/800], Batch [10/98], Loss: 5.2810, LR: 0.118933\nEpoch [566/800], Batch [20/98], Loss: 5.2569, LR: 0.118933\nEpoch [566/800], Batch [30/98], Loss: 5.2736, LR: 0.118933\nEpoch [566/800], Batch [40/98], Loss: 5.2606, LR: 0.118933\nEpoch [566/800], Batch [50/98], Loss: 5.2713, LR: 0.118933\nEpoch [566/800], Batch [60/98], Loss: 5.2517, LR: 0.118933\nEpoch [566/800], Batch [70/98], Loss: 5.2442, LR: 0.118933\nEpoch [566/800], Batch [80/98], Loss: 5.2638, LR: 0.118933\nEpoch [566/800], Batch [90/98], Loss: 5.2548, LR: 0.118933\nEpoch [566/800] Average Loss: 5.2541\n\nEpoch [567/800], Batch [0/98], Loss: 5.2688, LR: 0.117995\nEpoch [567/800], Batch [10/98], Loss: 5.2544, LR: 0.117995\nEpoch [567/800], Batch [20/98], Loss: 5.2622, LR: 0.117995\nEpoch [567/800], Batch [30/98], Loss: 5.2668, LR: 0.117995\nEpoch [567/800], Batch [40/98], Loss: 5.2562, LR: 0.117995\nEpoch [567/800], Batch [50/98], Loss: 5.2541, LR: 0.117995\nEpoch [567/800], Batch [60/98], Loss: 5.2751, LR: 0.117995\nEpoch [567/800], Batch [70/98], Loss: 5.2549, LR: 0.117995\nEpoch [567/800], Batch [80/98], Loss: 5.2725, LR: 0.117995\nEpoch [567/800], Batch [90/98], Loss: 5.2677, LR: 0.117995\nEpoch [567/800] Average Loss: 5.2528\n\nEpoch [568/800], Batch [0/98], Loss: 5.2712, LR: 0.117060\nEpoch [568/800], Batch [10/98], Loss: 5.2511, LR: 0.117060\nEpoch [568/800], Batch [20/98], Loss: 5.2568, LR: 0.117060\nEpoch [568/800], Batch [30/98], Loss: 5.2657, LR: 0.117060\nEpoch [568/800], Batch [40/98], Loss: 5.2479, LR: 0.117060\nEpoch [568/800], Batch [50/98], Loss: 5.2422, LR: 0.117060\nEpoch [568/800], Batch [60/98], Loss: 5.2570, LR: 0.117060\nEpoch [568/800], Batch [70/98], Loss: 5.2579, LR: 0.117060\nEpoch [568/800], Batch [80/98], Loss: 5.2398, LR: 0.117060\nEpoch [568/800], Batch [90/98], Loss: 5.2651, LR: 0.117060\nEpoch [568/800] Average Loss: 5.2501\n\nEpoch [569/800], Batch [0/98], Loss: 5.2696, LR: 0.116128\nEpoch [569/800], Batch [10/98], Loss: 5.2408, LR: 0.116128\nEpoch [569/800], Batch [20/98], Loss: 5.2424, LR: 0.116128\nEpoch [569/800], Batch [30/98], Loss: 5.2693, LR: 0.116128\nEpoch [569/800], Batch [40/98], Loss: 5.2607, LR: 0.116128\nEpoch [569/800], Batch [50/98], Loss: 5.2563, LR: 0.116128\nEpoch [569/800], Batch [60/98], Loss: 5.2347, LR: 0.116128\nEpoch [569/800], Batch [70/98], Loss: 5.2596, LR: 0.116128\nEpoch [569/800], Batch [80/98], Loss: 5.2228, LR: 0.116128\nEpoch [569/800], Batch [90/98], Loss: 5.2493, LR: 0.116128\nEpoch [569/800] Average Loss: 5.2532\n\nEpoch [570/800], Batch [0/98], Loss: 5.2640, LR: 0.115198\nEpoch [570/800], Batch [10/98], Loss: 5.2344, LR: 0.115198\nEpoch [570/800], Batch [20/98], Loss: 5.2487, LR: 0.115198\nEpoch [570/800], Batch [30/98], Loss: 5.2287, LR: 0.115198\nEpoch [570/800], Batch [40/98], Loss: 5.2856, LR: 0.115198\nEpoch [570/800], Batch [50/98], Loss: 5.2718, LR: 0.115198\nEpoch [570/800], Batch [60/98], Loss: 5.2626, LR: 0.115198\nEpoch [570/800], Batch [70/98], Loss: 5.2737, LR: 0.115198\nEpoch [570/800], Batch [80/98], Loss: 5.2682, LR: 0.115198\nEpoch [570/800], Batch [90/98], Loss: 5.2454, LR: 0.115198\nEpoch [570/800] Average Loss: 5.2533\n\nEpoch [571/800], Batch [0/98], Loss: 5.2449, LR: 0.114272\nEpoch [571/800], Batch [10/98], Loss: 5.2583, LR: 0.114272\nEpoch [571/800], Batch [20/98], Loss: 5.2300, LR: 0.114272\nEpoch [571/800], Batch [30/98], Loss: 5.2483, LR: 0.114272\nEpoch [571/800], Batch [40/98], Loss: 5.2642, LR: 0.114272\nEpoch [571/800], Batch [50/98], Loss: 5.2540, LR: 0.114272\nEpoch [571/800], Batch [60/98], Loss: 5.2503, LR: 0.114272\nEpoch [571/800], Batch [70/98], Loss: 5.2639, LR: 0.114272\nEpoch [571/800], Batch [80/98], Loss: 5.2467, LR: 0.114272\nEpoch [571/800], Batch [90/98], Loss: 5.2462, LR: 0.114272\nEpoch [571/800] Average Loss: 5.2508\n\nEpoch [572/800], Batch [0/98], Loss: 5.2357, LR: 0.113348\nEpoch [572/800], Batch [10/98], Loss: 5.2690, LR: 0.113348\nEpoch [572/800], Batch [20/98], Loss: 5.2678, LR: 0.113348\nEpoch [572/800], Batch [30/98], Loss: 5.2807, LR: 0.113348\nEpoch [572/800], Batch [40/98], Loss: 5.2282, LR: 0.113348\nEpoch [572/800], Batch [50/98], Loss: 5.2375, LR: 0.113348\nEpoch [572/800], Batch [60/98], Loss: 5.2536, LR: 0.113348\nEpoch [572/800], Batch [70/98], Loss: 5.2890, LR: 0.113348\nEpoch [572/800], Batch [80/98], Loss: 5.2712, LR: 0.113348\nEpoch [572/800], Batch [90/98], Loss: 5.2733, LR: 0.113348\nEpoch [572/800] Average Loss: 5.2534\n\nEpoch [573/800], Batch [0/98], Loss: 5.2667, LR: 0.112427\nEpoch [573/800], Batch [10/98], Loss: 5.2335, LR: 0.112427\nEpoch [573/800], Batch [20/98], Loss: 5.2360, LR: 0.112427\nEpoch [573/800], Batch [30/98], Loss: 5.2619, LR: 0.112427\nEpoch [573/800], Batch [40/98], Loss: 5.2689, LR: 0.112427\nEpoch [573/800], Batch [50/98], Loss: 5.2406, LR: 0.112427\nEpoch [573/800], Batch [60/98], Loss: 5.2467, LR: 0.112427\nEpoch [573/800], Batch [70/98], Loss: 5.2418, LR: 0.112427\nEpoch [573/800], Batch [80/98], Loss: 5.2488, LR: 0.112427\nEpoch [573/800], Batch [90/98], Loss: 5.2547, LR: 0.112427\nEpoch [573/800] Average Loss: 5.2516\n\nEpoch [574/800], Batch [0/98], Loss: 5.2505, LR: 0.111509\nEpoch [574/800], Batch [10/98], Loss: 5.2528, LR: 0.111509\nEpoch [574/800], Batch [20/98], Loss: 5.2642, LR: 0.111509\nEpoch [574/800], Batch [30/98], Loss: 5.2575, LR: 0.111509\nEpoch [574/800], Batch [40/98], Loss: 5.2443, LR: 0.111509\nEpoch [574/800], Batch [50/98], Loss: 5.2621, LR: 0.111509\nEpoch [574/800], Batch [60/98], Loss: 5.2501, LR: 0.111509\nEpoch [574/800], Batch [70/98], Loss: 5.2626, LR: 0.111509\nEpoch [574/800], Batch [80/98], Loss: 5.2648, LR: 0.111509\nEpoch [574/800], Batch [90/98], Loss: 5.2676, LR: 0.111509\nEpoch [574/800] Average Loss: 5.2524\n\nEpoch [575/800], Batch [0/98], Loss: 5.2567, LR: 0.110594\nEpoch [575/800], Batch [10/98], Loss: 5.2458, LR: 0.110594\nEpoch [575/800], Batch [20/98], Loss: 5.2617, LR: 0.110594\nEpoch [575/800], Batch [30/98], Loss: 5.2784, LR: 0.110594\nEpoch [575/800], Batch [40/98], Loss: 5.2613, LR: 0.110594\nEpoch [575/800], Batch [50/98], Loss: 5.2688, LR: 0.110594\nEpoch [575/800], Batch [60/98], Loss: 5.2359, LR: 0.110594\nEpoch [575/800], Batch [70/98], Loss: 5.2735, LR: 0.110594\nEpoch [575/800], Batch [80/98], Loss: 5.2316, LR: 0.110594\nEpoch [575/800], Batch [90/98], Loss: 5.2545, LR: 0.110594\nEpoch [575/800] Average Loss: 5.2501\n\nEpoch [576/800], Batch [0/98], Loss: 5.2554, LR: 0.109682\nEpoch [576/800], Batch [10/98], Loss: 5.2489, LR: 0.109682\nEpoch [576/800], Batch [20/98], Loss: 5.2414, LR: 0.109682\nEpoch [576/800], Batch [30/98], Loss: 5.2527, LR: 0.109682\nEpoch [576/800], Batch [40/98], Loss: 5.2705, LR: 0.109682\nEpoch [576/800], Batch [50/98], Loss: 5.2443, LR: 0.109682\nEpoch [576/800], Batch [60/98], Loss: 5.2311, LR: 0.109682\nEpoch [576/800], Batch [70/98], Loss: 5.2718, LR: 0.109682\nEpoch [576/800], Batch [80/98], Loss: 5.2491, LR: 0.109682\nEpoch [576/800], Batch [90/98], Loss: 5.2496, LR: 0.109682\nEpoch [576/800] Average Loss: 5.2520\n\nEpoch [577/800], Batch [0/98], Loss: 5.2507, LR: 0.108773\nEpoch [577/800], Batch [10/98], Loss: 5.2445, LR: 0.108773\nEpoch [577/800], Batch [20/98], Loss: 5.2657, LR: 0.108773\nEpoch [577/800], Batch [30/98], Loss: 5.2304, LR: 0.108773\nEpoch [577/800], Batch [40/98], Loss: 5.2420, LR: 0.108773\nEpoch [577/800], Batch [50/98], Loss: 5.2495, LR: 0.108773\nEpoch [577/800], Batch [60/98], Loss: 5.2617, LR: 0.108773\nEpoch [577/800], Batch [70/98], Loss: 5.2436, LR: 0.108773\nEpoch [577/800], Batch [80/98], Loss: 5.2571, LR: 0.108773\nEpoch [577/800], Batch [90/98], Loss: 5.2634, LR: 0.108773\nEpoch [577/800] Average Loss: 5.2515\n\nEpoch [578/800], Batch [0/98], Loss: 5.2680, LR: 0.107867\nEpoch [578/800], Batch [10/98], Loss: 5.2499, LR: 0.107867\nEpoch [578/800], Batch [20/98], Loss: 5.2518, LR: 0.107867\nEpoch [578/800], Batch [30/98], Loss: 5.2427, LR: 0.107867\nEpoch [578/800], Batch [40/98], Loss: 5.2515, LR: 0.107867\nEpoch [578/800], Batch [50/98], Loss: 5.2646, LR: 0.107867\nEpoch [578/800], Batch [60/98], Loss: 5.2411, LR: 0.107867\nEpoch [578/800], Batch [70/98], Loss: 5.2786, LR: 0.107867\nEpoch [578/800], Batch [80/98], Loss: 5.2564, LR: 0.107867\nEpoch [578/800], Batch [90/98], Loss: 5.2604, LR: 0.107867\nEpoch [578/800] Average Loss: 5.2504\n\nEpoch [579/800], Batch [0/98], Loss: 5.2522, LR: 0.106963\nEpoch [579/800], Batch [10/98], Loss: 5.2529, LR: 0.106963\nEpoch [579/800], Batch [20/98], Loss: 5.2851, LR: 0.106963\nEpoch [579/800], Batch [30/98], Loss: 5.2870, LR: 0.106963\nEpoch [579/800], Batch [40/98], Loss: 5.2717, LR: 0.106963\nEpoch [579/800], Batch [50/98], Loss: 5.2712, LR: 0.106963\nEpoch [579/800], Batch [60/98], Loss: 5.2537, LR: 0.106963\nEpoch [579/800], Batch [70/98], Loss: 5.2561, LR: 0.106963\nEpoch [579/800], Batch [80/98], Loss: 5.2349, LR: 0.106963\nEpoch [579/800], Batch [90/98], Loss: 5.2383, LR: 0.106963\nEpoch [579/800] Average Loss: 5.2531\n\nEpoch [580/800], Batch [0/98], Loss: 5.2483, LR: 0.106063\nEpoch [580/800], Batch [10/98], Loss: 5.2668, LR: 0.106063\nEpoch [580/800], Batch [20/98], Loss: 5.2586, LR: 0.106063\nEpoch [580/800], Batch [30/98], Loss: 5.2435, LR: 0.106063\nEpoch [580/800], Batch [40/98], Loss: 5.2634, LR: 0.106063\nEpoch [580/800], Batch [50/98], Loss: 5.2716, LR: 0.106063\nEpoch [580/800], Batch [60/98], Loss: 5.2468, LR: 0.106063\nEpoch [580/800], Batch [70/98], Loss: 5.2617, LR: 0.106063\nEpoch [580/800], Batch [80/98], Loss: 5.2587, LR: 0.106063\nEpoch [580/800], Batch [90/98], Loss: 5.2368, LR: 0.106063\nEpoch [580/800] Average Loss: 5.2507\n\nEpoch [581/800], Batch [0/98], Loss: 5.2723, LR: 0.105166\nEpoch [581/800], Batch [10/98], Loss: 5.2814, LR: 0.105166\nEpoch [581/800], Batch [20/98], Loss: 5.2912, LR: 0.105166\nEpoch [581/800], Batch [30/98], Loss: 5.2647, LR: 0.105166\nEpoch [581/800], Batch [40/98], Loss: 5.2634, LR: 0.105166\nEpoch [581/800], Batch [50/98], Loss: 5.2542, LR: 0.105166\nEpoch [581/800], Batch [60/98], Loss: 5.2803, LR: 0.105166\nEpoch [581/800], Batch [70/98], Loss: 5.2534, LR: 0.105166\nEpoch [581/800], Batch [80/98], Loss: 5.2631, LR: 0.105166\nEpoch [581/800], Batch [90/98], Loss: 5.2415, LR: 0.105166\nEpoch [581/800] Average Loss: 5.2514\n\nEpoch [582/800], Batch [0/98], Loss: 5.2241, LR: 0.104271\nEpoch [582/800], Batch [10/98], Loss: 5.2639, LR: 0.104271\nEpoch [582/800], Batch [20/98], Loss: 5.2742, LR: 0.104271\nEpoch [582/800], Batch [30/98], Loss: 5.2356, LR: 0.104271\nEpoch [582/800], Batch [40/98], Loss: 5.2611, LR: 0.104271\nEpoch [582/800], Batch [50/98], Loss: 5.2574, LR: 0.104271\nEpoch [582/800], Batch [60/98], Loss: 5.2457, LR: 0.104271\nEpoch [582/800], Batch [70/98], Loss: 5.2700, LR: 0.104271\nEpoch [582/800], Batch [80/98], Loss: 5.2570, LR: 0.104271\nEpoch [582/800], Batch [90/98], Loss: 5.2396, LR: 0.104271\nEpoch [582/800] Average Loss: 5.2533\n\nEpoch [583/800], Batch [0/98], Loss: 5.2767, LR: 0.103380\nEpoch [583/800], Batch [10/98], Loss: 5.2476, LR: 0.103380\nEpoch [583/800], Batch [20/98], Loss: 5.2543, LR: 0.103380\nEpoch [583/800], Batch [30/98], Loss: 5.2553, LR: 0.103380\nEpoch [583/800], Batch [40/98], Loss: 5.2442, LR: 0.103380\nEpoch [583/800], Batch [50/98], Loss: 5.2359, LR: 0.103380\nEpoch [583/800], Batch [60/98], Loss: 5.2574, LR: 0.103380\nEpoch [583/800], Batch [70/98], Loss: 5.2443, LR: 0.103380\nEpoch [583/800], Batch [80/98], Loss: 5.2502, LR: 0.103380\nEpoch [583/800], Batch [90/98], Loss: 5.2724, LR: 0.103380\nEpoch [583/800] Average Loss: 5.2507\n\nEpoch [584/800], Batch [0/98], Loss: 5.2343, LR: 0.102492\nEpoch [584/800], Batch [10/98], Loss: 5.2488, LR: 0.102492\nEpoch [584/800], Batch [20/98], Loss: 5.2593, LR: 0.102492\nEpoch [584/800], Batch [30/98], Loss: 5.2629, LR: 0.102492\nEpoch [584/800], Batch [40/98], Loss: 5.2338, LR: 0.102492\nEpoch [584/800], Batch [50/98], Loss: 5.2475, LR: 0.102492\nEpoch [584/800], Batch [60/98], Loss: 5.2809, LR: 0.102492\nEpoch [584/800], Batch [70/98], Loss: 5.2747, LR: 0.102492\nEpoch [584/800], Batch [80/98], Loss: 5.2621, LR: 0.102492\nEpoch [584/800], Batch [90/98], Loss: 5.2450, LR: 0.102492\nEpoch [584/800] Average Loss: 5.2495\n\nEpoch [585/800], Batch [0/98], Loss: 5.2332, LR: 0.101606\nEpoch [585/800], Batch [10/98], Loss: 5.2362, LR: 0.101606\nEpoch [585/800], Batch [20/98], Loss: 5.2607, LR: 0.101606\nEpoch [585/800], Batch [30/98], Loss: 5.2521, LR: 0.101606\nEpoch [585/800], Batch [40/98], Loss: 5.2717, LR: 0.101606\nEpoch [585/800], Batch [50/98], Loss: 5.2301, LR: 0.101606\nEpoch [585/800], Batch [60/98], Loss: 5.2332, LR: 0.101606\nEpoch [585/800], Batch [70/98], Loss: 5.2600, LR: 0.101606\nEpoch [585/800], Batch [80/98], Loss: 5.2580, LR: 0.101606\nEpoch [585/800], Batch [90/98], Loss: 5.2377, LR: 0.101606\nEpoch [585/800] Average Loss: 5.2501\n\nEpoch [586/800], Batch [0/98], Loss: 5.2437, LR: 0.100724\nEpoch [586/800], Batch [10/98], Loss: 5.2511, LR: 0.100724\nEpoch [586/800], Batch [20/98], Loss: 5.2781, LR: 0.100724\nEpoch [586/800], Batch [30/98], Loss: 5.2431, LR: 0.100724\nEpoch [586/800], Batch [40/98], Loss: 5.2466, LR: 0.100724\nEpoch [586/800], Batch [50/98], Loss: 5.2730, LR: 0.100724\nEpoch [586/800], Batch [60/98], Loss: 5.2347, LR: 0.100724\nEpoch [586/800], Batch [70/98], Loss: 5.2790, LR: 0.100724\nEpoch [586/800], Batch [80/98], Loss: 5.2352, LR: 0.100724\nEpoch [586/800], Batch [90/98], Loss: 5.2536, LR: 0.100724\nEpoch [586/800] Average Loss: 5.2509\n\nEpoch [587/800], Batch [0/98], Loss: 5.2629, LR: 0.099845\nEpoch [587/800], Batch [10/98], Loss: 5.2348, LR: 0.099845\nEpoch [587/800], Batch [20/98], Loss: 5.2741, LR: 0.099845\nEpoch [587/800], Batch [30/98], Loss: 5.2838, LR: 0.099845\nEpoch [587/800], Batch [40/98], Loss: 5.2774, LR: 0.099845\nEpoch [587/800], Batch [50/98], Loss: 5.2417, LR: 0.099845\nEpoch [587/800], Batch [60/98], Loss: 5.2631, LR: 0.099845\nEpoch [587/800], Batch [70/98], Loss: 5.2482, LR: 0.099845\nEpoch [587/800], Batch [80/98], Loss: 5.2356, LR: 0.099845\nEpoch [587/800], Batch [90/98], Loss: 5.2683, LR: 0.099845\nEpoch [587/800] Average Loss: 5.2496\n\nEpoch [588/800], Batch [0/98], Loss: 5.2415, LR: 0.098969\nEpoch [588/800], Batch [10/98], Loss: 5.2371, LR: 0.098969\nEpoch [588/800], Batch [20/98], Loss: 5.2737, LR: 0.098969\nEpoch [588/800], Batch [30/98], Loss: 5.2827, LR: 0.098969\nEpoch [588/800], Batch [40/98], Loss: 5.2715, LR: 0.098969\nEpoch [588/800], Batch [50/98], Loss: 5.2559, LR: 0.098969\nEpoch [588/800], Batch [60/98], Loss: 5.2576, LR: 0.098969\nEpoch [588/800], Batch [70/98], Loss: 5.2444, LR: 0.098969\nEpoch [588/800], Batch [80/98], Loss: 5.2626, LR: 0.098969\nEpoch [588/800], Batch [90/98], Loss: 5.2580, LR: 0.098969\nEpoch [588/800] Average Loss: 5.2527\n\nEpoch [589/800], Batch [0/98], Loss: 5.2394, LR: 0.098096\nEpoch [589/800], Batch [10/98], Loss: 5.2851, LR: 0.098096\nEpoch [589/800], Batch [20/98], Loss: 5.2439, LR: 0.098096\nEpoch [589/800], Batch [30/98], Loss: 5.2473, LR: 0.098096\nEpoch [589/800], Batch [40/98], Loss: 5.2514, LR: 0.098096\nEpoch [589/800], Batch [50/98], Loss: 5.2651, LR: 0.098096\nEpoch [589/800], Batch [60/98], Loss: 5.2539, LR: 0.098096\nEpoch [589/800], Batch [70/98], Loss: 5.2590, LR: 0.098096\nEpoch [589/800], Batch [80/98], Loss: 5.2653, LR: 0.098096\nEpoch [589/800], Batch [90/98], Loss: 5.2336, LR: 0.098096\nEpoch [589/800] Average Loss: 5.2516\n\nEpoch [590/800], Batch [0/98], Loss: 5.2394, LR: 0.097226\nEpoch [590/800], Batch [10/98], Loss: 5.2575, LR: 0.097226\nEpoch [590/800], Batch [20/98], Loss: 5.2473, LR: 0.097226\nEpoch [590/800], Batch [30/98], Loss: 5.2543, LR: 0.097226\nEpoch [590/800], Batch [40/98], Loss: 5.2857, LR: 0.097226\nEpoch [590/800], Batch [50/98], Loss: 5.2671, LR: 0.097226\nEpoch [590/800], Batch [60/98], Loss: 5.2682, LR: 0.097226\nEpoch [590/800], Batch [70/98], Loss: 5.2914, LR: 0.097226\nEpoch [590/800], Batch [80/98], Loss: 5.2561, LR: 0.097226\nEpoch [590/800], Batch [90/98], Loss: 5.2856, LR: 0.097226\nEpoch [590/800] Average Loss: 5.2516\n\nEpoch [591/800], Batch [0/98], Loss: 5.2548, LR: 0.096360\nEpoch [591/800], Batch [10/98], Loss: 5.2579, LR: 0.096360\nEpoch [591/800], Batch [20/98], Loss: 5.2711, LR: 0.096360\nEpoch [591/800], Batch [30/98], Loss: 5.2597, LR: 0.096360\nEpoch [591/800], Batch [40/98], Loss: 5.2645, LR: 0.096360\nEpoch [591/800], Batch [50/98], Loss: 5.2444, LR: 0.096360\nEpoch [591/800], Batch [60/98], Loss: 5.2416, LR: 0.096360\nEpoch [591/800], Batch [70/98], Loss: 5.2671, LR: 0.096360\nEpoch [591/800], Batch [80/98], Loss: 5.2525, LR: 0.096360\nEpoch [591/800], Batch [90/98], Loss: 5.2543, LR: 0.096360\nEpoch [591/800] Average Loss: 5.2499\n\nEpoch [592/800], Batch [0/98], Loss: 5.2370, LR: 0.095496\nEpoch [592/800], Batch [10/98], Loss: 5.2739, LR: 0.095496\nEpoch [592/800], Batch [20/98], Loss: 5.2550, LR: 0.095496\nEpoch [592/800], Batch [30/98], Loss: 5.2461, LR: 0.095496\nEpoch [592/800], Batch [40/98], Loss: 5.2456, LR: 0.095496\nEpoch [592/800], Batch [50/98], Loss: 5.2481, LR: 0.095496\nEpoch [592/800], Batch [60/98], Loss: 5.2683, LR: 0.095496\nEpoch [592/800], Batch [70/98], Loss: 5.2637, LR: 0.095496\nEpoch [592/800], Batch [80/98], Loss: 5.2599, LR: 0.095496\nEpoch [592/800], Batch [90/98], Loss: 5.2636, LR: 0.095496\nEpoch [592/800] Average Loss: 5.2511\n\nEpoch [593/800], Batch [0/98], Loss: 5.2285, LR: 0.094636\nEpoch [593/800], Batch [10/98], Loss: 5.2554, LR: 0.094636\nEpoch [593/800], Batch [20/98], Loss: 5.2343, LR: 0.094636\nEpoch [593/800], Batch [30/98], Loss: 5.2827, LR: 0.094636\nEpoch [593/800], Batch [40/98], Loss: 5.2571, LR: 0.094636\nEpoch [593/800], Batch [50/98], Loss: 5.2531, LR: 0.094636\nEpoch [593/800], Batch [60/98], Loss: 5.2521, LR: 0.094636\nEpoch [593/800], Batch [70/98], Loss: 5.2722, LR: 0.094636\nEpoch [593/800], Batch [80/98], Loss: 5.2410, LR: 0.094636\nEpoch [593/800], Batch [90/98], Loss: 5.2648, LR: 0.094636\nEpoch [593/800] Average Loss: 5.2510\n\nEpoch [594/800], Batch [0/98], Loss: 5.2496, LR: 0.093779\nEpoch [594/800], Batch [10/98], Loss: 5.2472, LR: 0.093779\nEpoch [594/800], Batch [20/98], Loss: 5.2335, LR: 0.093779\nEpoch [594/800], Batch [30/98], Loss: 5.2733, LR: 0.093779\nEpoch [594/800], Batch [40/98], Loss: 5.2671, LR: 0.093779\nEpoch [594/800], Batch [50/98], Loss: 5.2707, LR: 0.093779\nEpoch [594/800], Batch [60/98], Loss: 5.2506, LR: 0.093779\nEpoch [594/800], Batch [70/98], Loss: 5.2474, LR: 0.093779\nEpoch [594/800], Batch [80/98], Loss: 5.2422, LR: 0.093779\nEpoch [594/800], Batch [90/98], Loss: 5.2645, LR: 0.093779\nEpoch [594/800] Average Loss: 5.2494\n\nEpoch [595/800], Batch [0/98], Loss: 5.2586, LR: 0.092925\nEpoch [595/800], Batch [10/98], Loss: 5.2464, LR: 0.092925\nEpoch [595/800], Batch [20/98], Loss: 5.2505, LR: 0.092925\nEpoch [595/800], Batch [30/98], Loss: 5.2694, LR: 0.092925\nEpoch [595/800], Batch [40/98], Loss: 5.2445, LR: 0.092925\nEpoch [595/800], Batch [50/98], Loss: 5.2772, LR: 0.092925\nEpoch [595/800], Batch [60/98], Loss: 5.2412, LR: 0.092925\nEpoch [595/800], Batch [70/98], Loss: 5.2347, LR: 0.092925\nEpoch [595/800], Batch [80/98], Loss: 5.2457, LR: 0.092925\nEpoch [595/800], Batch [90/98], Loss: 5.2176, LR: 0.092925\nEpoch [595/800] Average Loss: 5.2477\n\nEpoch [596/800], Batch [0/98], Loss: 5.2792, LR: 0.092074\nEpoch [596/800], Batch [10/98], Loss: 5.2803, LR: 0.092074\nEpoch [596/800], Batch [20/98], Loss: 5.2475, LR: 0.092074\nEpoch [596/800], Batch [30/98], Loss: 5.2470, LR: 0.092074\nEpoch [596/800], Batch [40/98], Loss: 5.2800, LR: 0.092074\nEpoch [596/800], Batch [50/98], Loss: 5.2349, LR: 0.092074\nEpoch [596/800], Batch [60/98], Loss: 5.2657, LR: 0.092074\nEpoch [596/800], Batch [70/98], Loss: 5.2564, LR: 0.092074\nEpoch [596/800], Batch [80/98], Loss: 5.2483, LR: 0.092074\nEpoch [596/800], Batch [90/98], Loss: 5.2418, LR: 0.092074\nEpoch [596/800] Average Loss: 5.2508\n\nEpoch [597/800], Batch [0/98], Loss: 5.2532, LR: 0.091226\nEpoch [597/800], Batch [10/98], Loss: 5.2370, LR: 0.091226\nEpoch [597/800], Batch [20/98], Loss: 5.2901, LR: 0.091226\nEpoch [597/800], Batch [30/98], Loss: 5.2706, LR: 0.091226\nEpoch [597/800], Batch [40/98], Loss: 5.2529, LR: 0.091226\nEpoch [597/800], Batch [50/98], Loss: 5.2543, LR: 0.091226\nEpoch [597/800], Batch [60/98], Loss: 5.2514, LR: 0.091226\nEpoch [597/800], Batch [70/98], Loss: 5.2541, LR: 0.091226\nEpoch [597/800], Batch [80/98], Loss: 5.2507, LR: 0.091226\nEpoch [597/800], Batch [90/98], Loss: 5.2778, LR: 0.091226\nEpoch [597/800] Average Loss: 5.2501\n\nEpoch [598/800], Batch [0/98], Loss: 5.2545, LR: 0.090382\nEpoch [598/800], Batch [10/98], Loss: 5.2583, LR: 0.090382\nEpoch [598/800], Batch [20/98], Loss: 5.2415, LR: 0.090382\nEpoch [598/800], Batch [30/98], Loss: 5.2403, LR: 0.090382\nEpoch [598/800], Batch [40/98], Loss: 5.2648, LR: 0.090382\nEpoch [598/800], Batch [50/98], Loss: 5.2785, LR: 0.090382\nEpoch [598/800], Batch [60/98], Loss: 5.2531, LR: 0.090382\nEpoch [598/800], Batch [70/98], Loss: 5.2429, LR: 0.090382\nEpoch [598/800], Batch [80/98], Loss: 5.2761, LR: 0.090382\nEpoch [598/800], Batch [90/98], Loss: 5.2418, LR: 0.090382\nEpoch [598/800] Average Loss: 5.2481\n\nEpoch [599/800], Batch [0/98], Loss: 5.2510, LR: 0.089541\nEpoch [599/800], Batch [10/98], Loss: 5.2509, LR: 0.089541\nEpoch [599/800], Batch [20/98], Loss: 5.2314, LR: 0.089541\nEpoch [599/800], Batch [30/98], Loss: 5.2363, LR: 0.089541\nEpoch [599/800], Batch [40/98], Loss: 5.2600, LR: 0.089541\nEpoch [599/800], Batch [50/98], Loss: 5.2596, LR: 0.089541\nEpoch [599/800], Batch [60/98], Loss: 5.2478, LR: 0.089541\nEpoch [599/800], Batch [70/98], Loss: 5.2168, LR: 0.089541\nEpoch [599/800], Batch [80/98], Loss: 5.2343, LR: 0.089541\nEpoch [599/800], Batch [90/98], Loss: 5.2492, LR: 0.089541\nEpoch [599/800] Average Loss: 5.2493\n\nEpoch [600/800], Batch [0/98], Loss: 5.2531, LR: 0.088703\nEpoch [600/800], Batch [10/98], Loss: 5.2594, LR: 0.088703\nEpoch [600/800], Batch [20/98], Loss: 5.2487, LR: 0.088703\nEpoch [600/800], Batch [30/98], Loss: 5.2718, LR: 0.088703\nEpoch [600/800], Batch [40/98], Loss: 5.2671, LR: 0.088703\nEpoch [600/800], Batch [50/98], Loss: 5.2531, LR: 0.088703\nEpoch [600/800], Batch [60/98], Loss: 5.2621, LR: 0.088703\nEpoch [600/800], Batch [70/98], Loss: 5.2676, LR: 0.088703\nEpoch [600/800], Batch [80/98], Loss: 5.2643, LR: 0.088703\nEpoch [600/800], Batch [90/98], Loss: 5.2507, LR: 0.088703\nEpoch [600/800] Average Loss: 5.2498\n\nEpoch [601/800], Batch [0/98], Loss: 5.2610, LR: 0.087868\nEpoch [601/800], Batch [10/98], Loss: 5.2603, LR: 0.087868\nEpoch [601/800], Batch [20/98], Loss: 5.2614, LR: 0.087868\nEpoch [601/800], Batch [30/98], Loss: 5.2617, LR: 0.087868\nEpoch [601/800], Batch [40/98], Loss: 5.2584, LR: 0.087868\nEpoch [601/800], Batch [50/98], Loss: 5.2509, LR: 0.087868\nEpoch [601/800], Batch [60/98], Loss: 5.2673, LR: 0.087868\nEpoch [601/800], Batch [70/98], Loss: 5.2346, LR: 0.087868\nEpoch [601/800], Batch [80/98], Loss: 5.2433, LR: 0.087868\nEpoch [601/800], Batch [90/98], Loss: 5.2290, LR: 0.087868\nEpoch [601/800] Average Loss: 5.2486\n\nEpoch [602/800], Batch [0/98], Loss: 5.2572, LR: 0.087037\nEpoch [602/800], Batch [10/98], Loss: 5.2422, LR: 0.087037\nEpoch [602/800], Batch [20/98], Loss: 5.2523, LR: 0.087037\nEpoch [602/800], Batch [30/98], Loss: 5.2495, LR: 0.087037\nEpoch [602/800], Batch [40/98], Loss: 5.2482, LR: 0.087037\nEpoch [602/800], Batch [50/98], Loss: 5.2631, LR: 0.087037\nEpoch [602/800], Batch [60/98], Loss: 5.2373, LR: 0.087037\nEpoch [602/800], Batch [70/98], Loss: 5.2755, LR: 0.087037\nEpoch [602/800], Batch [80/98], Loss: 5.2473, LR: 0.087037\nEpoch [602/800], Batch [90/98], Loss: 5.2647, LR: 0.087037\nEpoch [602/800] Average Loss: 5.2492\n\nEpoch [603/800], Batch [0/98], Loss: 5.2414, LR: 0.086208\nEpoch [603/800], Batch [10/98], Loss: 5.2665, LR: 0.086208\nEpoch [603/800], Batch [20/98], Loss: 5.2222, LR: 0.086208\nEpoch [603/800], Batch [30/98], Loss: 5.2438, LR: 0.086208\nEpoch [603/800], Batch [40/98], Loss: 5.2639, LR: 0.086208\nEpoch [603/800], Batch [50/98], Loss: 5.2691, LR: 0.086208\nEpoch [603/800], Batch [60/98], Loss: 5.2622, LR: 0.086208\nEpoch [603/800], Batch [70/98], Loss: 5.2669, LR: 0.086208\nEpoch [603/800], Batch [80/98], Loss: 5.2378, LR: 0.086208\nEpoch [603/800], Batch [90/98], Loss: 5.2722, LR: 0.086208\nEpoch [603/800] Average Loss: 5.2487\n\nEpoch [604/800], Batch [0/98], Loss: 5.2498, LR: 0.085384\nEpoch [604/800], Batch [10/98], Loss: 5.2568, LR: 0.085384\nEpoch [604/800], Batch [20/98], Loss: 5.2671, LR: 0.085384\nEpoch [604/800], Batch [30/98], Loss: 5.2523, LR: 0.085384\nEpoch [604/800], Batch [40/98], Loss: 5.2661, LR: 0.085384\nEpoch [604/800], Batch [50/98], Loss: 5.2501, LR: 0.085384\nEpoch [604/800], Batch [60/98], Loss: 5.2477, LR: 0.085384\nEpoch [604/800], Batch [70/98], Loss: 5.2678, LR: 0.085384\nEpoch [604/800], Batch [80/98], Loss: 5.2643, LR: 0.085384\nEpoch [604/800], Batch [90/98], Loss: 5.2496, LR: 0.085384\nEpoch [604/800] Average Loss: 5.2483\n\nEpoch [605/800], Batch [0/98], Loss: 5.2531, LR: 0.084562\nEpoch [605/800], Batch [10/98], Loss: 5.2549, LR: 0.084562\nEpoch [605/800], Batch [20/98], Loss: 5.2428, LR: 0.084562\nEpoch [605/800], Batch [30/98], Loss: 5.2614, LR: 0.084562\nEpoch [605/800], Batch [40/98], Loss: 5.2763, LR: 0.084562\nEpoch [605/800], Batch [50/98], Loss: 5.2552, LR: 0.084562\nEpoch [605/800], Batch [60/98], Loss: 5.2394, LR: 0.084562\nEpoch [605/800], Batch [70/98], Loss: 5.2499, LR: 0.084562\nEpoch [605/800], Batch [80/98], Loss: 5.2610, LR: 0.084562\nEpoch [605/800], Batch [90/98], Loss: 5.2641, LR: 0.084562\nEpoch [605/800] Average Loss: 5.2466\n\nEpoch [606/800], Batch [0/98], Loss: 5.2586, LR: 0.083744\nEpoch [606/800], Batch [10/98], Loss: 5.2575, LR: 0.083744\nEpoch [606/800], Batch [20/98], Loss: 5.2313, LR: 0.083744\nEpoch [606/800], Batch [30/98], Loss: 5.2583, LR: 0.083744\nEpoch [606/800], Batch [40/98], Loss: 5.2124, LR: 0.083744\nEpoch [606/800], Batch [50/98], Loss: 5.2342, LR: 0.083744\nEpoch [606/800], Batch [60/98], Loss: 5.2429, LR: 0.083744\nEpoch [606/800], Batch [70/98], Loss: 5.2569, LR: 0.083744\nEpoch [606/800], Batch [80/98], Loss: 5.2439, LR: 0.083744\nEpoch [606/800], Batch [90/98], Loss: 5.2347, LR: 0.083744\nEpoch [606/800] Average Loss: 5.2448\n\nEpoch [607/800], Batch [0/98], Loss: 5.2367, LR: 0.082929\nEpoch [607/800], Batch [10/98], Loss: 5.2395, LR: 0.082929\nEpoch [607/800], Batch [20/98], Loss: 5.2593, LR: 0.082929\nEpoch [607/800], Batch [30/98], Loss: 5.2248, LR: 0.082929\nEpoch [607/800], Batch [40/98], Loss: 5.2618, LR: 0.082929\nEpoch [607/800], Batch [50/98], Loss: 5.2593, LR: 0.082929\nEpoch [607/800], Batch [60/98], Loss: 5.2581, LR: 0.082929\nEpoch [607/800], Batch [70/98], Loss: 5.2476, LR: 0.082929\nEpoch [607/800], Batch [80/98], Loss: 5.2680, LR: 0.082929\nEpoch [607/800], Batch [90/98], Loss: 5.2538, LR: 0.082929\nEpoch [607/800] Average Loss: 5.2483\n\nEpoch [608/800], Batch [0/98], Loss: 5.2686, LR: 0.082118\nEpoch [608/800], Batch [10/98], Loss: 5.2489, LR: 0.082118\nEpoch [608/800], Batch [20/98], Loss: 5.2525, LR: 0.082118\nEpoch [608/800], Batch [30/98], Loss: 5.2727, LR: 0.082118\nEpoch [608/800], Batch [40/98], Loss: 5.2343, LR: 0.082118\nEpoch [608/800], Batch [50/98], Loss: 5.2332, LR: 0.082118\nEpoch [608/800], Batch [60/98], Loss: 5.2369, LR: 0.082118\nEpoch [608/800], Batch [70/98], Loss: 5.2403, LR: 0.082118\nEpoch [608/800], Batch [80/98], Loss: 5.2607, LR: 0.082118\nEpoch [608/800], Batch [90/98], Loss: 5.2394, LR: 0.082118\nEpoch [608/800] Average Loss: 5.2483\n\nEpoch [609/800], Batch [0/98], Loss: 5.2524, LR: 0.081309\nEpoch [609/800], Batch [10/98], Loss: 5.2785, LR: 0.081309\nEpoch [609/800], Batch [20/98], Loss: 5.2091, LR: 0.081309\nEpoch [609/800], Batch [30/98], Loss: 5.2500, LR: 0.081309\nEpoch [609/800], Batch [40/98], Loss: 5.2293, LR: 0.081309\nEpoch [609/800], Batch [50/98], Loss: 5.2517, LR: 0.081309\nEpoch [609/800], Batch [60/98], Loss: 5.2463, LR: 0.081309\nEpoch [609/800], Batch [70/98], Loss: 5.2578, LR: 0.081309\nEpoch [609/800], Batch [80/98], Loss: 5.2295, LR: 0.081309\nEpoch [609/800], Batch [90/98], Loss: 5.2279, LR: 0.081309\nEpoch [609/800] Average Loss: 5.2460\n\nEpoch [610/800], Batch [0/98], Loss: 5.2275, LR: 0.080505\nEpoch [610/800], Batch [10/98], Loss: 5.2661, LR: 0.080505\nEpoch [610/800], Batch [20/98], Loss: 5.2428, LR: 0.080505\nEpoch [610/800], Batch [30/98], Loss: 5.2582, LR: 0.080505\nEpoch [610/800], Batch [40/98], Loss: 5.2351, LR: 0.080505\nEpoch [610/800], Batch [50/98], Loss: 5.2434, LR: 0.080505\nEpoch [610/800], Batch [60/98], Loss: 5.2328, LR: 0.080505\nEpoch [610/800], Batch [70/98], Loss: 5.2464, LR: 0.080505\nEpoch [610/800], Batch [80/98], Loss: 5.2265, LR: 0.080505\nEpoch [610/800], Batch [90/98], Loss: 5.2431, LR: 0.080505\nEpoch [610/800] Average Loss: 5.2461\n\nEpoch [611/800], Batch [0/98], Loss: 5.2794, LR: 0.079703\nEpoch [611/800], Batch [10/98], Loss: 5.2553, LR: 0.079703\nEpoch [611/800], Batch [20/98], Loss: 5.2513, LR: 0.079703\nEpoch [611/800], Batch [30/98], Loss: 5.2442, LR: 0.079703\nEpoch [611/800], Batch [40/98], Loss: 5.2516, LR: 0.079703\nEpoch [611/800], Batch [50/98], Loss: 5.2776, LR: 0.079703\nEpoch [611/800], Batch [60/98], Loss: 5.2522, LR: 0.079703\nEpoch [611/800], Batch [70/98], Loss: 5.2498, LR: 0.079703\nEpoch [611/800], Batch [80/98], Loss: 5.2438, LR: 0.079703\nEpoch [611/800], Batch [90/98], Loss: 5.2519, LR: 0.079703\nEpoch [611/800] Average Loss: 5.2484\n\nEpoch [612/800], Batch [0/98], Loss: 5.2697, LR: 0.078905\nEpoch [612/800], Batch [10/98], Loss: 5.2459, LR: 0.078905\nEpoch [612/800], Batch [20/98], Loss: 5.2416, LR: 0.078905\nEpoch [612/800], Batch [30/98], Loss: 5.2316, LR: 0.078905\nEpoch [612/800], Batch [40/98], Loss: 5.2515, LR: 0.078905\nEpoch [612/800], Batch [50/98], Loss: 5.2588, LR: 0.078905\nEpoch [612/800], Batch [60/98], Loss: 5.2356, LR: 0.078905\nEpoch [612/800], Batch [70/98], Loss: 5.2417, LR: 0.078905\nEpoch [612/800], Batch [80/98], Loss: 5.2631, LR: 0.078905\nEpoch [612/800], Batch [90/98], Loss: 5.2578, LR: 0.078905\nEpoch [612/800] Average Loss: 5.2469\n\nEpoch [613/800], Batch [0/98], Loss: 5.2435, LR: 0.078111\nEpoch [613/800], Batch [10/98], Loss: 5.2582, LR: 0.078111\nEpoch [613/800], Batch [20/98], Loss: 5.2261, LR: 0.078111\nEpoch [613/800], Batch [30/98], Loss: 5.2583, LR: 0.078111\nEpoch [613/800], Batch [40/98], Loss: 5.2398, LR: 0.078111\nEpoch [613/800], Batch [50/98], Loss: 5.2582, LR: 0.078111\nEpoch [613/800], Batch [60/98], Loss: 5.2368, LR: 0.078111\nEpoch [613/800], Batch [70/98], Loss: 5.2475, LR: 0.078111\nEpoch [613/800], Batch [80/98], Loss: 5.2634, LR: 0.078111\nEpoch [613/800], Batch [90/98], Loss: 5.2542, LR: 0.078111\nEpoch [613/800] Average Loss: 5.2464\n\nEpoch [614/800], Batch [0/98], Loss: 5.2567, LR: 0.077320\nEpoch [614/800], Batch [10/98], Loss: 5.2418, LR: 0.077320\nEpoch [614/800], Batch [20/98], Loss: 5.2543, LR: 0.077320\nEpoch [614/800], Batch [30/98], Loss: 5.2404, LR: 0.077320\nEpoch [614/800], Batch [40/98], Loss: 5.2596, LR: 0.077320\nEpoch [614/800], Batch [50/98], Loss: 5.2345, LR: 0.077320\nEpoch [614/800], Batch [60/98], Loss: 5.2473, LR: 0.077320\nEpoch [614/800], Batch [70/98], Loss: 5.2406, LR: 0.077320\nEpoch [614/800], Batch [80/98], Loss: 5.2498, LR: 0.077320\nEpoch [614/800], Batch [90/98], Loss: 5.2587, LR: 0.077320\nEpoch [614/800] Average Loss: 5.2464\n\nEpoch [615/800], Batch [0/98], Loss: 5.2504, LR: 0.076532\nEpoch [615/800], Batch [10/98], Loss: 5.2267, LR: 0.076532\nEpoch [615/800], Batch [20/98], Loss: 5.2435, LR: 0.076532\nEpoch [615/800], Batch [30/98], Loss: 5.2320, LR: 0.076532\nEpoch [615/800], Batch [40/98], Loss: 5.2440, LR: 0.076532\nEpoch [615/800], Batch [50/98], Loss: 5.2387, LR: 0.076532\nEpoch [615/800], Batch [60/98], Loss: 5.2444, LR: 0.076532\nEpoch [615/800], Batch [70/98], Loss: 5.2328, LR: 0.076532\nEpoch [615/800], Batch [80/98], Loss: 5.2723, LR: 0.076532\nEpoch [615/800], Batch [90/98], Loss: 5.2633, LR: 0.076532\nEpoch [615/800] Average Loss: 5.2458\n\nEpoch [616/800], Batch [0/98], Loss: 5.2524, LR: 0.075748\nEpoch [616/800], Batch [10/98], Loss: 5.2449, LR: 0.075748\nEpoch [616/800], Batch [20/98], Loss: 5.2533, LR: 0.075748\nEpoch [616/800], Batch [30/98], Loss: 5.2454, LR: 0.075748\nEpoch [616/800], Batch [40/98], Loss: 5.2478, LR: 0.075748\nEpoch [616/800], Batch [50/98], Loss: 5.2724, LR: 0.075748\nEpoch [616/800], Batch [60/98], Loss: 5.2712, LR: 0.075748\nEpoch [616/800], Batch [70/98], Loss: 5.2570, LR: 0.075748\nEpoch [616/800], Batch [80/98], Loss: 5.2197, LR: 0.075748\nEpoch [616/800], Batch [90/98], Loss: 5.2473, LR: 0.075748\nEpoch [616/800] Average Loss: 5.2462\n\nEpoch [617/800], Batch [0/98], Loss: 5.2428, LR: 0.074967\nEpoch [617/800], Batch [10/98], Loss: 5.2561, LR: 0.074967\nEpoch [617/800], Batch [20/98], Loss: 5.2732, LR: 0.074967\nEpoch [617/800], Batch [30/98], Loss: 5.2324, LR: 0.074967\nEpoch [617/800], Batch [40/98], Loss: 5.2566, LR: 0.074967\nEpoch [617/800], Batch [50/98], Loss: 5.2471, LR: 0.074967\nEpoch [617/800], Batch [60/98], Loss: 5.2501, LR: 0.074967\nEpoch [617/800], Batch [70/98], Loss: 5.2252, LR: 0.074967\nEpoch [617/800], Batch [80/98], Loss: 5.2216, LR: 0.074967\nEpoch [617/800], Batch [90/98], Loss: 5.2484, LR: 0.074967\nEpoch [617/800] Average Loss: 5.2451\n\nEpoch [618/800], Batch [0/98], Loss: 5.2480, LR: 0.074189\nEpoch [618/800], Batch [10/98], Loss: 5.2346, LR: 0.074189\nEpoch [618/800], Batch [20/98], Loss: 5.2474, LR: 0.074189\nEpoch [618/800], Batch [30/98], Loss: 5.2433, LR: 0.074189\nEpoch [618/800], Batch [40/98], Loss: 5.2512, LR: 0.074189\nEpoch [618/800], Batch [50/98], Loss: 5.2477, LR: 0.074189\nEpoch [618/800], Batch [60/98], Loss: 5.2563, LR: 0.074189\nEpoch [618/800], Batch [70/98], Loss: 5.2526, LR: 0.074189\nEpoch [618/800], Batch [80/98], Loss: 5.2346, LR: 0.074189\nEpoch [618/800], Batch [90/98], Loss: 5.2529, LR: 0.074189\nEpoch [618/800] Average Loss: 5.2466\n\nEpoch [619/800], Batch [0/98], Loss: 5.2754, LR: 0.073415\nEpoch [619/800], Batch [10/98], Loss: 5.2358, LR: 0.073415\nEpoch [619/800], Batch [20/98], Loss: 5.2425, LR: 0.073415\nEpoch [619/800], Batch [30/98], Loss: 5.2491, LR: 0.073415\nEpoch [619/800], Batch [40/98], Loss: 5.2586, LR: 0.073415\nEpoch [619/800], Batch [50/98], Loss: 5.2553, LR: 0.073415\nEpoch [619/800], Batch [60/98], Loss: 5.2585, LR: 0.073415\nEpoch [619/800], Batch [70/98], Loss: 5.2295, LR: 0.073415\nEpoch [619/800], Batch [80/98], Loss: 5.2423, LR: 0.073415\nEpoch [619/800], Batch [90/98], Loss: 5.2456, LR: 0.073415\nEpoch [619/800] Average Loss: 5.2446\n\nEpoch [620/800], Batch [0/98], Loss: 5.2234, LR: 0.072645\nEpoch [620/800], Batch [10/98], Loss: 5.2603, LR: 0.072645\nEpoch [620/800], Batch [20/98], Loss: 5.2538, LR: 0.072645\nEpoch [620/800], Batch [30/98], Loss: 5.2276, LR: 0.072645\nEpoch [620/800], Batch [40/98], Loss: 5.2840, LR: 0.072645\nEpoch [620/800], Batch [50/98], Loss: 5.2644, LR: 0.072645\nEpoch [620/800], Batch [60/98], Loss: 5.2466, LR: 0.072645\nEpoch [620/800], Batch [70/98], Loss: 5.2305, LR: 0.072645\nEpoch [620/800], Batch [80/98], Loss: 5.2450, LR: 0.072645\nEpoch [620/800], Batch [90/98], Loss: 5.2718, LR: 0.072645\nEpoch [620/800] Average Loss: 5.2441\n\nEpoch [621/800], Batch [0/98], Loss: 5.2269, LR: 0.071878\nEpoch [621/800], Batch [10/98], Loss: 5.2348, LR: 0.071878\nEpoch [621/800], Batch [20/98], Loss: 5.2715, LR: 0.071878\nEpoch [621/800], Batch [30/98], Loss: 5.2357, LR: 0.071878\nEpoch [621/800], Batch [40/98], Loss: 5.2683, LR: 0.071878\nEpoch [621/800], Batch [50/98], Loss: 5.2519, LR: 0.071878\nEpoch [621/800], Batch [60/98], Loss: 5.2530, LR: 0.071878\nEpoch [621/800], Batch [70/98], Loss: 5.2329, LR: 0.071878\nEpoch [621/800], Batch [80/98], Loss: 5.2620, LR: 0.071878\nEpoch [621/800], Batch [90/98], Loss: 5.2409, LR: 0.071878\nEpoch [621/800] Average Loss: 5.2431\n\nEpoch [622/800], Batch [0/98], Loss: 5.2461, LR: 0.071115\nEpoch [622/800], Batch [10/98], Loss: 5.2285, LR: 0.071115\nEpoch [622/800], Batch [20/98], Loss: 5.2438, LR: 0.071115\nEpoch [622/800], Batch [30/98], Loss: 5.2235, LR: 0.071115\nEpoch [622/800], Batch [40/98], Loss: 5.2414, LR: 0.071115\nEpoch [622/800], Batch [50/98], Loss: 5.2515, LR: 0.071115\nEpoch [622/800], Batch [60/98], Loss: 5.2433, LR: 0.071115\nEpoch [622/800], Batch [70/98], Loss: 5.2253, LR: 0.071115\nEpoch [622/800], Batch [80/98], Loss: 5.2470, LR: 0.071115\nEpoch [622/800], Batch [90/98], Loss: 5.2665, LR: 0.071115\nEpoch [622/800] Average Loss: 5.2453\n\nEpoch [623/800], Batch [0/98], Loss: 5.2568, LR: 0.070355\nEpoch [623/800], Batch [10/98], Loss: 5.2538, LR: 0.070355\nEpoch [623/800], Batch [20/98], Loss: 5.2426, LR: 0.070355\nEpoch [623/800], Batch [30/98], Loss: 5.2372, LR: 0.070355\nEpoch [623/800], Batch [40/98], Loss: 5.2705, LR: 0.070355\nEpoch [623/800], Batch [50/98], Loss: 5.2724, LR: 0.070355\nEpoch [623/800], Batch [60/98], Loss: 5.2844, LR: 0.070355\nEpoch [623/800], Batch [70/98], Loss: 5.2469, LR: 0.070355\nEpoch [623/800], Batch [80/98], Loss: 5.2595, LR: 0.070355\nEpoch [623/800], Batch [90/98], Loss: 5.2727, LR: 0.070355\nEpoch [623/800] Average Loss: 5.2466\n\nEpoch [624/800], Batch [0/98], Loss: 5.2372, LR: 0.069599\nEpoch [624/800], Batch [10/98], Loss: 5.2431, LR: 0.069599\nEpoch [624/800], Batch [20/98], Loss: 5.2336, LR: 0.069599\nEpoch [624/800], Batch [30/98], Loss: 5.2506, LR: 0.069599\nEpoch [624/800], Batch [40/98], Loss: 5.2688, LR: 0.069599\nEpoch [624/800], Batch [50/98], Loss: 5.2646, LR: 0.069599\nEpoch [624/800], Batch [60/98], Loss: 5.2809, LR: 0.069599\nEpoch [624/800], Batch [70/98], Loss: 5.2372, LR: 0.069599\nEpoch [624/800], Batch [80/98], Loss: 5.2496, LR: 0.069599\nEpoch [624/800], Batch [90/98], Loss: 5.2548, LR: 0.069599\nEpoch [624/800] Average Loss: 5.2454\n\nEpoch [625/800], Batch [0/98], Loss: 5.2358, LR: 0.068846\nEpoch [625/800], Batch [10/98], Loss: 5.2589, LR: 0.068846\nEpoch [625/800], Batch [20/98], Loss: 5.2325, LR: 0.068846\nEpoch [625/800], Batch [30/98], Loss: 5.2478, LR: 0.068846\nEpoch [625/800], Batch [40/98], Loss: 5.2400, LR: 0.068846\nEpoch [625/800], Batch [50/98], Loss: 5.2556, LR: 0.068846\nEpoch [625/800], Batch [60/98], Loss: 5.2436, LR: 0.068846\nEpoch [625/800], Batch [70/98], Loss: 5.2653, LR: 0.068846\nEpoch [625/800], Batch [80/98], Loss: 5.2710, LR: 0.068846\nEpoch [625/800], Batch [90/98], Loss: 5.2602, LR: 0.068846\nEpoch [625/800] Average Loss: 5.2434\n\nEpoch [626/800], Batch [0/98], Loss: 5.2318, LR: 0.068097\nEpoch [626/800], Batch [10/98], Loss: 5.2345, LR: 0.068097\nEpoch [626/800], Batch [20/98], Loss: 5.2542, LR: 0.068097\nEpoch [626/800], Batch [30/98], Loss: 5.2503, LR: 0.068097\nEpoch [626/800], Batch [40/98], Loss: 5.2428, LR: 0.068097\nEpoch [626/800], Batch [50/98], Loss: 5.2409, LR: 0.068097\nEpoch [626/800], Batch [60/98], Loss: 5.2757, LR: 0.068097\nEpoch [626/800], Batch [70/98], Loss: 5.2551, LR: 0.068097\nEpoch [626/800], Batch [80/98], Loss: 5.2276, LR: 0.068097\nEpoch [626/800], Batch [90/98], Loss: 5.2448, LR: 0.068097\nEpoch [626/800] Average Loss: 5.2466\n\nEpoch [627/800], Batch [0/98], Loss: 5.2550, LR: 0.067351\nEpoch [627/800], Batch [10/98], Loss: 5.2642, LR: 0.067351\nEpoch [627/800], Batch [20/98], Loss: 5.2425, LR: 0.067351\nEpoch [627/800], Batch [30/98], Loss: 5.2619, LR: 0.067351\nEpoch [627/800], Batch [40/98], Loss: 5.2485, LR: 0.067351\nEpoch [627/800], Batch [50/98], Loss: 5.2652, LR: 0.067351\nEpoch [627/800], Batch [60/98], Loss: 5.2464, LR: 0.067351\nEpoch [627/800], Batch [70/98], Loss: 5.2499, LR: 0.067351\nEpoch [627/800], Batch [80/98], Loss: 5.2422, LR: 0.067351\nEpoch [627/800], Batch [90/98], Loss: 5.2696, LR: 0.067351\nEpoch [627/800] Average Loss: 5.2454\n\nEpoch [628/800], Batch [0/98], Loss: 5.2638, LR: 0.066609\nEpoch [628/800], Batch [10/98], Loss: 5.2388, LR: 0.066609\nEpoch [628/800], Batch [20/98], Loss: 5.2702, LR: 0.066609\nEpoch [628/800], Batch [30/98], Loss: 5.2496, LR: 0.066609\nEpoch [628/800], Batch [40/98], Loss: 5.2698, LR: 0.066609\nEpoch [628/800], Batch [50/98], Loss: 5.2501, LR: 0.066609\nEpoch [628/800], Batch [60/98], Loss: 5.2429, LR: 0.066609\nEpoch [628/800], Batch [70/98], Loss: 5.2422, LR: 0.066609\nEpoch [628/800], Batch [80/98], Loss: 5.2685, LR: 0.066609\nEpoch [628/800], Batch [90/98], Loss: 5.2374, LR: 0.066609\nEpoch [628/800] Average Loss: 5.2461\n\nEpoch [629/800], Batch [0/98], Loss: 5.2557, LR: 0.065871\nEpoch [629/800], Batch [10/98], Loss: 5.2405, LR: 0.065871\nEpoch [629/800], Batch [20/98], Loss: 5.2479, LR: 0.065871\nEpoch [629/800], Batch [30/98], Loss: 5.2237, LR: 0.065871\nEpoch [629/800], Batch [40/98], Loss: 5.2447, LR: 0.065871\nEpoch [629/800], Batch [50/98], Loss: 5.2198, LR: 0.065871\nEpoch [629/800], Batch [60/98], Loss: 5.2333, LR: 0.065871\nEpoch [629/800], Batch [70/98], Loss: 5.2511, LR: 0.065871\nEpoch [629/800], Batch [80/98], Loss: 5.2633, LR: 0.065871\nEpoch [629/800], Batch [90/98], Loss: 5.2259, LR: 0.065871\nEpoch [629/800] Average Loss: 5.2445\n\nEpoch [630/800], Batch [0/98], Loss: 5.2570, LR: 0.065136\nEpoch [630/800], Batch [10/98], Loss: 5.2691, LR: 0.065136\nEpoch [630/800], Batch [20/98], Loss: 5.2646, LR: 0.065136\nEpoch [630/800], Batch [30/98], Loss: 5.2639, LR: 0.065136\nEpoch [630/800], Batch [40/98], Loss: 5.2579, LR: 0.065136\nEpoch [630/800], Batch [50/98], Loss: 5.2823, LR: 0.065136\nEpoch [630/800], Batch [60/98], Loss: 5.2525, LR: 0.065136\nEpoch [630/800], Batch [70/98], Loss: 5.2486, LR: 0.065136\nEpoch [630/800], Batch [80/98], Loss: 5.2682, LR: 0.065136\nEpoch [630/800], Batch [90/98], Loss: 5.2427, LR: 0.065136\nEpoch [630/800] Average Loss: 5.2461\n\nEpoch [631/800], Batch [0/98], Loss: 5.2318, LR: 0.064405\nEpoch [631/800], Batch [10/98], Loss: 5.2643, LR: 0.064405\nEpoch [631/800], Batch [20/98], Loss: 5.2710, LR: 0.064405\nEpoch [631/800], Batch [30/98], Loss: 5.2361, LR: 0.064405\nEpoch [631/800], Batch [40/98], Loss: 5.2446, LR: 0.064405\nEpoch [631/800], Batch [50/98], Loss: 5.2619, LR: 0.064405\nEpoch [631/800], Batch [60/98], Loss: 5.2301, LR: 0.064405\nEpoch [631/800], Batch [70/98], Loss: 5.2485, LR: 0.064405\nEpoch [631/800], Batch [80/98], Loss: 5.2300, LR: 0.064405\nEpoch [631/800], Batch [90/98], Loss: 5.2459, LR: 0.064405\nEpoch [631/800] Average Loss: 5.2445\n\nEpoch [632/800], Batch [0/98], Loss: 5.2277, LR: 0.063677\nEpoch [632/800], Batch [10/98], Loss: 5.2293, LR: 0.063677\nEpoch [632/800], Batch [20/98], Loss: 5.2597, LR: 0.063677\nEpoch [632/800], Batch [30/98], Loss: 5.2461, LR: 0.063677\nEpoch [632/800], Batch [40/98], Loss: 5.2460, LR: 0.063677\nEpoch [632/800], Batch [50/98], Loss: 5.2758, LR: 0.063677\nEpoch [632/800], Batch [60/98], Loss: 5.2300, LR: 0.063677\nEpoch [632/800], Batch [70/98], Loss: 5.2363, LR: 0.063677\nEpoch [632/800], Batch [80/98], Loss: 5.2669, LR: 0.063677\nEpoch [632/800], Batch [90/98], Loss: 5.2513, LR: 0.063677\nEpoch [632/800] Average Loss: 5.2437\n\nEpoch [633/800], Batch [0/98], Loss: 5.2478, LR: 0.062953\nEpoch [633/800], Batch [10/98], Loss: 5.2431, LR: 0.062953\nEpoch [633/800], Batch [20/98], Loss: 5.2456, LR: 0.062953\nEpoch [633/800], Batch [30/98], Loss: 5.2381, LR: 0.062953\nEpoch [633/800], Batch [40/98], Loss: 5.2701, LR: 0.062953\nEpoch [633/800], Batch [50/98], Loss: 5.2502, LR: 0.062953\nEpoch [633/800], Batch [60/98], Loss: 5.2366, LR: 0.062953\nEpoch [633/800], Batch [70/98], Loss: 5.2667, LR: 0.062953\nEpoch [633/800], Batch [80/98], Loss: 5.2546, LR: 0.062953\nEpoch [633/800], Batch [90/98], Loss: 5.2615, LR: 0.062953\nEpoch [633/800] Average Loss: 5.2439\n\nEpoch [634/800], Batch [0/98], Loss: 5.2359, LR: 0.062233\nEpoch [634/800], Batch [10/98], Loss: 5.2483, LR: 0.062233\nEpoch [634/800], Batch [20/98], Loss: 5.2682, LR: 0.062233\nEpoch [634/800], Batch [30/98], Loss: 5.2621, LR: 0.062233\nEpoch [634/800], Batch [40/98], Loss: 5.2401, LR: 0.062233\nEpoch [634/800], Batch [50/98], Loss: 5.2526, LR: 0.062233\nEpoch [634/800], Batch [60/98], Loss: 5.2403, LR: 0.062233\nEpoch [634/800], Batch [70/98], Loss: 5.2293, LR: 0.062233\nEpoch [634/800], Batch [80/98], Loss: 5.2508, LR: 0.062233\nEpoch [634/800], Batch [90/98], Loss: 5.2690, LR: 0.062233\nEpoch [634/800] Average Loss: 5.2458\n\nEpoch [635/800], Batch [0/98], Loss: 5.2702, LR: 0.061517\nEpoch [635/800], Batch [10/98], Loss: 5.2576, LR: 0.061517\nEpoch [635/800], Batch [20/98], Loss: 5.2305, LR: 0.061517\nEpoch [635/800], Batch [30/98], Loss: 5.2472, LR: 0.061517\nEpoch [635/800], Batch [40/98], Loss: 5.2316, LR: 0.061517\nEpoch [635/800], Batch [50/98], Loss: 5.2221, LR: 0.061517\nEpoch [635/800], Batch [60/98], Loss: 5.2681, LR: 0.061517\nEpoch [635/800], Batch [70/98], Loss: 5.2625, LR: 0.061517\nEpoch [635/800], Batch [80/98], Loss: 5.2723, LR: 0.061517\nEpoch [635/800], Batch [90/98], Loss: 5.2401, LR: 0.061517\nEpoch [635/800] Average Loss: 5.2446\n\nEpoch [636/800], Batch [0/98], Loss: 5.2461, LR: 0.060804\nEpoch [636/800], Batch [10/98], Loss: 5.2588, LR: 0.060804\nEpoch [636/800], Batch [20/98], Loss: 5.2449, LR: 0.060804\nEpoch [636/800], Batch [30/98], Loss: 5.2523, LR: 0.060804\nEpoch [636/800], Batch [40/98], Loss: 5.2385, LR: 0.060804\nEpoch [636/800], Batch [50/98], Loss: 5.2100, LR: 0.060804\nEpoch [636/800], Batch [60/98], Loss: 5.2560, LR: 0.060804\nEpoch [636/800], Batch [70/98], Loss: 5.2227, LR: 0.060804\nEpoch [636/800], Batch [80/98], Loss: 5.2429, LR: 0.060804\nEpoch [636/800], Batch [90/98], Loss: 5.2467, LR: 0.060804\nEpoch [636/800] Average Loss: 5.2427\n\nEpoch [637/800], Batch [0/98], Loss: 5.2461, LR: 0.060095\nEpoch [637/800], Batch [10/98], Loss: 5.2654, LR: 0.060095\nEpoch [637/800], Batch [20/98], Loss: 5.2480, LR: 0.060095\nEpoch [637/800], Batch [30/98], Loss: 5.2819, LR: 0.060095\nEpoch [637/800], Batch [40/98], Loss: 5.2447, LR: 0.060095\nEpoch [637/800], Batch [50/98], Loss: 5.2403, LR: 0.060095\nEpoch [637/800], Batch [60/98], Loss: 5.2176, LR: 0.060095\nEpoch [637/800], Batch [70/98], Loss: 5.2413, LR: 0.060095\nEpoch [637/800], Batch [80/98], Loss: 5.2540, LR: 0.060095\nEpoch [637/800], Batch [90/98], Loss: 5.2660, LR: 0.060095\nEpoch [637/800] Average Loss: 5.2432\n\nEpoch [638/800], Batch [0/98], Loss: 5.2846, LR: 0.059389\nEpoch [638/800], Batch [10/98], Loss: 5.2499, LR: 0.059389\nEpoch [638/800], Batch [20/98], Loss: 5.2525, LR: 0.059389\nEpoch [638/800], Batch [30/98], Loss: 5.2602, LR: 0.059389\nEpoch [638/800], Batch [40/98], Loss: 5.2568, LR: 0.059389\nEpoch [638/800], Batch [50/98], Loss: 5.2640, LR: 0.059389\nEpoch [638/800], Batch [60/98], Loss: 5.2479, LR: 0.059389\nEpoch [638/800], Batch [70/98], Loss: 5.2400, LR: 0.059389\nEpoch [638/800], Batch [80/98], Loss: 5.2653, LR: 0.059389\nEpoch [638/800], Batch [90/98], Loss: 5.2222, LR: 0.059389\nEpoch [638/800] Average Loss: 5.2457\n\nEpoch [639/800], Batch [0/98], Loss: 5.2598, LR: 0.058687\nEpoch [639/800], Batch [10/98], Loss: 5.2552, LR: 0.058687\nEpoch [639/800], Batch [20/98], Loss: 5.2230, LR: 0.058687\nEpoch [639/800], Batch [30/98], Loss: 5.2401, LR: 0.058687\nEpoch [639/800], Batch [40/98], Loss: 5.2260, LR: 0.058687\nEpoch [639/800], Batch [50/98], Loss: 5.2441, LR: 0.058687\nEpoch [639/800], Batch [60/98], Loss: 5.2689, LR: 0.058687\nEpoch [639/800], Batch [70/98], Loss: 5.2670, LR: 0.058687\nEpoch [639/800], Batch [80/98], Loss: 5.2424, LR: 0.058687\nEpoch [639/800], Batch [90/98], Loss: 5.2513, LR: 0.058687\nEpoch [639/800] Average Loss: 5.2422\n\nEpoch [640/800], Batch [0/98], Loss: 5.2369, LR: 0.057989\nEpoch [640/800], Batch [10/98], Loss: 5.2367, LR: 0.057989\nEpoch [640/800], Batch [20/98], Loss: 5.2422, LR: 0.057989\nEpoch [640/800], Batch [30/98], Loss: 5.2674, LR: 0.057989\nEpoch [640/800], Batch [40/98], Loss: 5.2477, LR: 0.057989\nEpoch [640/800], Batch [50/98], Loss: 5.2197, LR: 0.057989\nEpoch [640/800], Batch [60/98], Loss: 5.2489, LR: 0.057989\nEpoch [640/800], Batch [70/98], Loss: 5.2342, LR: 0.057989\nEpoch [640/800], Batch [80/98], Loss: 5.2283, LR: 0.057989\nEpoch [640/800], Batch [90/98], Loss: 5.2178, LR: 0.057989\nEpoch [640/800] Average Loss: 5.2425\n\nEpoch [641/800], Batch [0/98], Loss: 5.2303, LR: 0.057295\nEpoch [641/800], Batch [10/98], Loss: 5.2572, LR: 0.057295\nEpoch [641/800], Batch [20/98], Loss: 5.2419, LR: 0.057295\nEpoch [641/800], Batch [30/98], Loss: 5.2351, LR: 0.057295\nEpoch [641/800], Batch [40/98], Loss: 5.2390, LR: 0.057295\nEpoch [641/800], Batch [50/98], Loss: 5.2520, LR: 0.057295\nEpoch [641/800], Batch [60/98], Loss: 5.2294, LR: 0.057295\nEpoch [641/800], Batch [70/98], Loss: 5.2618, LR: 0.057295\nEpoch [641/800], Batch [80/98], Loss: 5.2565, LR: 0.057295\nEpoch [641/800], Batch [90/98], Loss: 5.2573, LR: 0.057295\nEpoch [641/800] Average Loss: 5.2413\n\nEpoch [642/800], Batch [0/98], Loss: 5.2399, LR: 0.056604\nEpoch [642/800], Batch [10/98], Loss: 5.2542, LR: 0.056604\nEpoch [642/800], Batch [20/98], Loss: 5.2608, LR: 0.056604\nEpoch [642/800], Batch [30/98], Loss: 5.2399, LR: 0.056604\nEpoch [642/800], Batch [40/98], Loss: 5.2468, LR: 0.056604\nEpoch [642/800], Batch [50/98], Loss: 5.2583, LR: 0.056604\nEpoch [642/800], Batch [60/98], Loss: 5.2111, LR: 0.056604\nEpoch [642/800], Batch [70/98], Loss: 5.2521, LR: 0.056604\nEpoch [642/800], Batch [80/98], Loss: 5.2485, LR: 0.056604\nEpoch [642/800], Batch [90/98], Loss: 5.2579, LR: 0.056604\nEpoch [642/800] Average Loss: 5.2439\n\nEpoch [643/800], Batch [0/98], Loss: 5.2610, LR: 0.055917\nEpoch [643/800], Batch [10/98], Loss: 5.2518, LR: 0.055917\nEpoch [643/800], Batch [20/98], Loss: 5.2179, LR: 0.055917\nEpoch [643/800], Batch [30/98], Loss: 5.2491, LR: 0.055917\nEpoch [643/800], Batch [40/98], Loss: 5.2607, LR: 0.055917\nEpoch [643/800], Batch [50/98], Loss: 5.2553, LR: 0.055917\nEpoch [643/800], Batch [60/98], Loss: 5.2414, LR: 0.055917\nEpoch [643/800], Batch [70/98], Loss: 5.2401, LR: 0.055917\nEpoch [643/800], Batch [80/98], Loss: 5.2492, LR: 0.055917\nEpoch [643/800], Batch [90/98], Loss: 5.2540, LR: 0.055917\nEpoch [643/800] Average Loss: 5.2424\n\nEpoch [644/800], Batch [0/98], Loss: 5.2400, LR: 0.055234\nEpoch [644/800], Batch [10/98], Loss: 5.2724, LR: 0.055234\nEpoch [644/800], Batch [20/98], Loss: 5.2499, LR: 0.055234\nEpoch [644/800], Batch [30/98], Loss: 5.2549, LR: 0.055234\nEpoch [644/800], Batch [40/98], Loss: 5.2381, LR: 0.055234\nEpoch [644/800], Batch [50/98], Loss: 5.2492, LR: 0.055234\nEpoch [644/800], Batch [60/98], Loss: 5.2574, LR: 0.055234\nEpoch [644/800], Batch [70/98], Loss: 5.2397, LR: 0.055234\nEpoch [644/800], Batch [80/98], Loss: 5.2644, LR: 0.055234\nEpoch [644/800], Batch [90/98], Loss: 5.2395, LR: 0.055234\nEpoch [644/800] Average Loss: 5.2431\n\nEpoch [645/800], Batch [0/98], Loss: 5.2514, LR: 0.054555\nEpoch [645/800], Batch [10/98], Loss: 5.2396, LR: 0.054555\nEpoch [645/800], Batch [20/98], Loss: 5.2652, LR: 0.054555\nEpoch [645/800], Batch [30/98], Loss: 5.2540, LR: 0.054555\nEpoch [645/800], Batch [40/98], Loss: 5.2474, LR: 0.054555\nEpoch [645/800], Batch [50/98], Loss: 5.2417, LR: 0.054555\nEpoch [645/800], Batch [60/98], Loss: 5.2372, LR: 0.054555\nEpoch [645/800], Batch [70/98], Loss: 5.2546, LR: 0.054555\nEpoch [645/800], Batch [80/98], Loss: 5.2615, LR: 0.054555\nEpoch [645/800], Batch [90/98], Loss: 5.2589, LR: 0.054555\nEpoch [645/800] Average Loss: 5.2443\n\nEpoch [646/800], Batch [0/98], Loss: 5.2419, LR: 0.053880\nEpoch [646/800], Batch [10/98], Loss: 5.2517, LR: 0.053880\nEpoch [646/800], Batch [20/98], Loss: 5.2171, LR: 0.053880\nEpoch [646/800], Batch [30/98], Loss: 5.2540, LR: 0.053880\nEpoch [646/800], Batch [40/98], Loss: 5.2457, LR: 0.053880\nEpoch [646/800], Batch [50/98], Loss: 5.2595, LR: 0.053880\nEpoch [646/800], Batch [60/98], Loss: 5.2501, LR: 0.053880\nEpoch [646/800], Batch [70/98], Loss: 5.2411, LR: 0.053880\nEpoch [646/800], Batch [80/98], Loss: 5.2396, LR: 0.053880\nEpoch [646/800], Batch [90/98], Loss: 5.2372, LR: 0.053880\nEpoch [646/800] Average Loss: 5.2417\n\nEpoch [647/800], Batch [0/98], Loss: 5.2487, LR: 0.053208\nEpoch [647/800], Batch [10/98], Loss: 5.2329, LR: 0.053208\nEpoch [647/800], Batch [20/98], Loss: 5.2320, LR: 0.053208\nEpoch [647/800], Batch [30/98], Loss: 5.2485, LR: 0.053208\nEpoch [647/800], Batch [40/98], Loss: 5.2528, LR: 0.053208\nEpoch [647/800], Batch [50/98], Loss: 5.2380, LR: 0.053208\nEpoch [647/800], Batch [60/98], Loss: 5.2454, LR: 0.053208\nEpoch [647/800], Batch [70/98], Loss: 5.2328, LR: 0.053208\nEpoch [647/800], Batch [80/98], Loss: 5.2454, LR: 0.053208\nEpoch [647/800], Batch [90/98], Loss: 5.2348, LR: 0.053208\nEpoch [647/800] Average Loss: 5.2429\n\nEpoch [648/800], Batch [0/98], Loss: 5.2316, LR: 0.052540\nEpoch [648/800], Batch [10/98], Loss: 5.2253, LR: 0.052540\nEpoch [648/800], Batch [20/98], Loss: 5.2351, LR: 0.052540\nEpoch [648/800], Batch [30/98], Loss: 5.2646, LR: 0.052540\nEpoch [648/800], Batch [40/98], Loss: 5.2465, LR: 0.052540\nEpoch [648/800], Batch [50/98], Loss: 5.2292, LR: 0.052540\nEpoch [648/800], Batch [60/98], Loss: 5.2740, LR: 0.052540\nEpoch [648/800], Batch [70/98], Loss: 5.2610, LR: 0.052540\nEpoch [648/800], Batch [80/98], Loss: 5.2347, LR: 0.052540\nEpoch [648/800], Batch [90/98], Loss: 5.2380, LR: 0.052540\nEpoch [648/800] Average Loss: 5.2428\n\nEpoch [649/800], Batch [0/98], Loss: 5.2256, LR: 0.051876\nEpoch [649/800], Batch [10/98], Loss: 5.2813, LR: 0.051876\nEpoch [649/800], Batch [20/98], Loss: 5.2486, LR: 0.051876\nEpoch [649/800], Batch [30/98], Loss: 5.2288, LR: 0.051876\nEpoch [649/800], Batch [40/98], Loss: 5.2329, LR: 0.051876\nEpoch [649/800], Batch [50/98], Loss: 5.2630, LR: 0.051876\nEpoch [649/800], Batch [60/98], Loss: 5.2344, LR: 0.051876\nEpoch [649/800], Batch [70/98], Loss: 5.2362, LR: 0.051876\nEpoch [649/800], Batch [80/98], Loss: 5.2153, LR: 0.051876\nEpoch [649/800], Batch [90/98], Loss: 5.2372, LR: 0.051876\nEpoch [649/800] Average Loss: 5.2415\n\nEpoch [650/800], Batch [0/98], Loss: 5.2575, LR: 0.051216\nEpoch [650/800], Batch [10/98], Loss: 5.2535, LR: 0.051216\nEpoch [650/800], Batch [20/98], Loss: 5.2503, LR: 0.051216\nEpoch [650/800], Batch [30/98], Loss: 5.2472, LR: 0.051216\nEpoch [650/800], Batch [40/98], Loss: 5.2465, LR: 0.051216\nEpoch [650/800], Batch [50/98], Loss: 5.2566, LR: 0.051216\nEpoch [650/800], Batch [60/98], Loss: 5.2551, LR: 0.051216\nEpoch [650/800], Batch [70/98], Loss: 5.2402, LR: 0.051216\nEpoch [650/800], Batch [80/98], Loss: 5.2508, LR: 0.051216\nEpoch [650/800], Batch [90/98], Loss: 5.2589, LR: 0.051216\nEpoch [650/800] Average Loss: 5.2400\n\nEpoch [651/800], Batch [0/98], Loss: 5.2503, LR: 0.050559\nEpoch [651/800], Batch [10/98], Loss: 5.2394, LR: 0.050559\nEpoch [651/800], Batch [20/98], Loss: 5.2544, LR: 0.050559\nEpoch [651/800], Batch [30/98], Loss: 5.2430, LR: 0.050559\nEpoch [651/800], Batch [40/98], Loss: 5.2298, LR: 0.050559\nEpoch [651/800], Batch [50/98], Loss: 5.2631, LR: 0.050559\nEpoch [651/800], Batch [60/98], Loss: 5.2564, LR: 0.050559\nEpoch [651/800], Batch [70/98], Loss: 5.2623, LR: 0.050559\nEpoch [651/800], Batch [80/98], Loss: 5.2331, LR: 0.050559\nEpoch [651/800], Batch [90/98], Loss: 5.2662, LR: 0.050559\nEpoch [651/800] Average Loss: 5.2419\n\nEpoch [652/800], Batch [0/98], Loss: 5.2613, LR: 0.049907\nEpoch [652/800], Batch [10/98], Loss: 5.2418, LR: 0.049907\nEpoch [652/800], Batch [20/98], Loss: 5.2230, LR: 0.049907\nEpoch [652/800], Batch [30/98], Loss: 5.2422, LR: 0.049907\nEpoch [652/800], Batch [40/98], Loss: 5.2359, LR: 0.049907\nEpoch [652/800], Batch [50/98], Loss: 5.2535, LR: 0.049907\nEpoch [652/800], Batch [60/98], Loss: 5.2388, LR: 0.049907\nEpoch [652/800], Batch [70/98], Loss: 5.2481, LR: 0.049907\nEpoch [652/800], Batch [80/98], Loss: 5.2397, LR: 0.049907\nEpoch [652/800], Batch [90/98], Loss: 5.2602, LR: 0.049907\nEpoch [652/800] Average Loss: 5.2392\n\nEpoch [653/800], Batch [0/98], Loss: 5.2479, LR: 0.049258\nEpoch [653/800], Batch [10/98], Loss: 5.2378, LR: 0.049258\nEpoch [653/800], Batch [20/98], Loss: 5.2124, LR: 0.049258\nEpoch [653/800], Batch [30/98], Loss: 5.2447, LR: 0.049258\nEpoch [653/800], Batch [40/98], Loss: 5.2515, LR: 0.049258\nEpoch [653/800], Batch [50/98], Loss: 5.2414, LR: 0.049258\nEpoch [653/800], Batch [60/98], Loss: 5.2604, LR: 0.049258\nEpoch [653/800], Batch [70/98], Loss: 5.2425, LR: 0.049258\nEpoch [653/800], Batch [80/98], Loss: 5.2596, LR: 0.049258\nEpoch [653/800], Batch [90/98], Loss: 5.2439, LR: 0.049258\nEpoch [653/800] Average Loss: 5.2413\n\nEpoch [654/800], Batch [0/98], Loss: 5.2526, LR: 0.048613\nEpoch [654/800], Batch [10/98], Loss: 5.2453, LR: 0.048613\nEpoch [654/800], Batch [20/98], Loss: 5.2629, LR: 0.048613\nEpoch [654/800], Batch [30/98], Loss: 5.2696, LR: 0.048613\nEpoch [654/800], Batch [40/98], Loss: 5.2303, LR: 0.048613\nEpoch [654/800], Batch [50/98], Loss: 5.2625, LR: 0.048613\nEpoch [654/800], Batch [60/98], Loss: 5.2668, LR: 0.048613\nEpoch [654/800], Batch [70/98], Loss: 5.2461, LR: 0.048613\nEpoch [654/800], Batch [80/98], Loss: 5.2485, LR: 0.048613\nEpoch [654/800], Batch [90/98], Loss: 5.2323, LR: 0.048613\nEpoch [654/800] Average Loss: 5.2417\n\nEpoch [655/800], Batch [0/98], Loss: 5.2459, LR: 0.047972\nEpoch [655/800], Batch [10/98], Loss: 5.2345, LR: 0.047972\nEpoch [655/800], Batch [20/98], Loss: 5.2432, LR: 0.047972\nEpoch [655/800], Batch [30/98], Loss: 5.2720, LR: 0.047972\nEpoch [655/800], Batch [40/98], Loss: 5.2660, LR: 0.047972\nEpoch [655/800], Batch [50/98], Loss: 5.2489, LR: 0.047972\nEpoch [655/800], Batch [60/98], Loss: 5.2513, LR: 0.047972\nEpoch [655/800], Batch [70/98], Loss: 5.2506, LR: 0.047972\nEpoch [655/800], Batch [80/98], Loss: 5.2589, LR: 0.047972\nEpoch [655/800], Batch [90/98], Loss: 5.3057, LR: 0.047972\nEpoch [655/800] Average Loss: 5.2433\n\nEpoch [656/800], Batch [0/98], Loss: 5.2342, LR: 0.047335\nEpoch [656/800], Batch [10/98], Loss: 5.2704, LR: 0.047335\nEpoch [656/800], Batch [20/98], Loss: 5.2271, LR: 0.047335\nEpoch [656/800], Batch [30/98], Loss: 5.2562, LR: 0.047335\nEpoch [656/800], Batch [40/98], Loss: 5.2504, LR: 0.047335\nEpoch [656/800], Batch [50/98], Loss: 5.2294, LR: 0.047335\nEpoch [656/800], Batch [60/98], Loss: 5.2320, LR: 0.047335\nEpoch [656/800], Batch [70/98], Loss: 5.2450, LR: 0.047335\nEpoch [656/800], Batch [80/98], Loss: 5.2263, LR: 0.047335\nEpoch [656/800], Batch [90/98], Loss: 5.2300, LR: 0.047335\nEpoch [656/800] Average Loss: 5.2413\n\nEpoch [657/800], Batch [0/98], Loss: 5.2580, LR: 0.046702\nEpoch [657/800], Batch [10/98], Loss: 5.2707, LR: 0.046702\nEpoch [657/800], Batch [20/98], Loss: 5.2269, LR: 0.046702\nEpoch [657/800], Batch [30/98], Loss: 5.2290, LR: 0.046702\nEpoch [657/800], Batch [40/98], Loss: 5.2398, LR: 0.046702\nEpoch [657/800], Batch [50/98], Loss: 5.2267, LR: 0.046702\nEpoch [657/800], Batch [60/98], Loss: 5.2342, LR: 0.046702\nEpoch [657/800], Batch [70/98], Loss: 5.2452, LR: 0.046702\nEpoch [657/800], Batch [80/98], Loss: 5.2489, LR: 0.046702\nEpoch [657/800], Batch [90/98], Loss: 5.2326, LR: 0.046702\nEpoch [657/800] Average Loss: 5.2403\n\nEpoch [658/800], Batch [0/98], Loss: 5.2310, LR: 0.046072\nEpoch [658/800], Batch [10/98], Loss: 5.2420, LR: 0.046072\nEpoch [658/800], Batch [20/98], Loss: 5.2419, LR: 0.046072\nEpoch [658/800], Batch [30/98], Loss: 5.2300, LR: 0.046072\nEpoch [658/800], Batch [40/98], Loss: 5.2297, LR: 0.046072\nEpoch [658/800], Batch [50/98], Loss: 5.2520, LR: 0.046072\nEpoch [658/800], Batch [60/98], Loss: 5.2420, LR: 0.046072\nEpoch [658/800], Batch [70/98], Loss: 5.2552, LR: 0.046072\nEpoch [658/800], Batch [80/98], Loss: 5.2199, LR: 0.046072\nEpoch [658/800], Batch [90/98], Loss: 5.2191, LR: 0.046072\nEpoch [658/800] Average Loss: 5.2394\n\nEpoch [659/800], Batch [0/98], Loss: 5.2470, LR: 0.045447\nEpoch [659/800], Batch [10/98], Loss: 5.2374, LR: 0.045447\nEpoch [659/800], Batch [20/98], Loss: 5.2538, LR: 0.045447\nEpoch [659/800], Batch [30/98], Loss: 5.2629, LR: 0.045447\nEpoch [659/800], Batch [40/98], Loss: 5.2295, LR: 0.045447\nEpoch [659/800], Batch [50/98], Loss: 5.2403, LR: 0.045447\nEpoch [659/800], Batch [60/98], Loss: 5.2455, LR: 0.045447\nEpoch [659/800], Batch [70/98], Loss: 5.2666, LR: 0.045447\nEpoch [659/800], Batch [80/98], Loss: 5.2425, LR: 0.045447\nEpoch [659/800], Batch [90/98], Loss: 5.2130, LR: 0.045447\nEpoch [659/800] Average Loss: 5.2389\n\nEpoch [660/800], Batch [0/98], Loss: 5.2302, LR: 0.044825\nEpoch [660/800], Batch [10/98], Loss: 5.2337, LR: 0.044825\nEpoch [660/800], Batch [20/98], Loss: 5.2488, LR: 0.044825\nEpoch [660/800], Batch [30/98], Loss: 5.2281, LR: 0.044825\nEpoch [660/800], Batch [40/98], Loss: 5.2483, LR: 0.044825\nEpoch [660/800], Batch [50/98], Loss: 5.2276, LR: 0.044825\nEpoch [660/800], Batch [60/98], Loss: 5.2651, LR: 0.044825\nEpoch [660/800], Batch [70/98], Loss: 5.2800, LR: 0.044825\nEpoch [660/800], Batch [80/98], Loss: 5.2373, LR: 0.044825\nEpoch [660/800], Batch [90/98], Loss: 5.2319, LR: 0.044825\nEpoch [660/800] Average Loss: 5.2404\n\nEpoch [661/800], Batch [0/98], Loss: 5.2374, LR: 0.044208\nEpoch [661/800], Batch [10/98], Loss: 5.2534, LR: 0.044208\nEpoch [661/800], Batch [20/98], Loss: 5.2598, LR: 0.044208\nEpoch [661/800], Batch [30/98], Loss: 5.2445, LR: 0.044208\nEpoch [661/800], Batch [40/98], Loss: 5.2478, LR: 0.044208\nEpoch [661/800], Batch [50/98], Loss: 5.2533, LR: 0.044208\nEpoch [661/800], Batch [60/98], Loss: 5.2327, LR: 0.044208\nEpoch [661/800], Batch [70/98], Loss: 5.2525, LR: 0.044208\nEpoch [661/800], Batch [80/98], Loss: 5.2524, LR: 0.044208\nEpoch [661/800], Batch [90/98], Loss: 5.2467, LR: 0.044208\nEpoch [661/800] Average Loss: 5.2401\n\nEpoch [662/800], Batch [0/98], Loss: 5.2388, LR: 0.043594\nEpoch [662/800], Batch [10/98], Loss: 5.2229, LR: 0.043594\nEpoch [662/800], Batch [20/98], Loss: 5.2140, LR: 0.043594\nEpoch [662/800], Batch [30/98], Loss: 5.2849, LR: 0.043594\nEpoch [662/800], Batch [40/98], Loss: 5.2340, LR: 0.043594\nEpoch [662/800], Batch [50/98], Loss: 5.2555, LR: 0.043594\nEpoch [662/800], Batch [60/98], Loss: 5.2484, LR: 0.043594\nEpoch [662/800], Batch [70/98], Loss: 5.2399, LR: 0.043594\nEpoch [662/800], Batch [80/98], Loss: 5.2501, LR: 0.043594\nEpoch [662/800], Batch [90/98], Loss: 5.2342, LR: 0.043594\nEpoch [662/800] Average Loss: 5.2403\n\nEpoch [663/800], Batch [0/98], Loss: 5.2548, LR: 0.042985\nEpoch [663/800], Batch [10/98], Loss: 5.2413, LR: 0.042985\nEpoch [663/800], Batch [20/98], Loss: 5.2452, LR: 0.042985\nEpoch [663/800], Batch [30/98], Loss: 5.2686, LR: 0.042985\nEpoch [663/800], Batch [40/98], Loss: 5.2459, LR: 0.042985\nEpoch [663/800], Batch [50/98], Loss: 5.2744, LR: 0.042985\nEpoch [663/800], Batch [60/98], Loss: 5.2406, LR: 0.042985\nEpoch [663/800], Batch [70/98], Loss: 5.2340, LR: 0.042985\nEpoch [663/800], Batch [80/98], Loss: 5.2272, LR: 0.042985\nEpoch [663/800], Batch [90/98], Loss: 5.2578, LR: 0.042985\nEpoch [663/800] Average Loss: 5.2401\n\nEpoch [664/800], Batch [0/98], Loss: 5.2633, LR: 0.042379\nEpoch [664/800], Batch [10/98], Loss: 5.2514, LR: 0.042379\nEpoch [664/800], Batch [20/98], Loss: 5.2436, LR: 0.042379\nEpoch [664/800], Batch [30/98], Loss: 5.2268, LR: 0.042379\nEpoch [664/800], Batch [40/98], Loss: 5.2393, LR: 0.042379\nEpoch [664/800], Batch [50/98], Loss: 5.2782, LR: 0.042379\nEpoch [664/800], Batch [60/98], Loss: 5.2311, LR: 0.042379\nEpoch [664/800], Batch [70/98], Loss: 5.2521, LR: 0.042379\nEpoch [664/800], Batch [80/98], Loss: 5.2179, LR: 0.042379\nEpoch [664/800], Batch [90/98], Loss: 5.2421, LR: 0.042379\nEpoch [664/800] Average Loss: 5.2401\n\nEpoch [665/800], Batch [0/98], Loss: 5.2615, LR: 0.041777\nEpoch [665/800], Batch [10/98], Loss: 5.2551, LR: 0.041777\nEpoch [665/800], Batch [20/98], Loss: 5.2527, LR: 0.041777\nEpoch [665/800], Batch [30/98], Loss: 5.2384, LR: 0.041777\nEpoch [665/800], Batch [40/98], Loss: 5.2553, LR: 0.041777\nEpoch [665/800], Batch [50/98], Loss: 5.2236, LR: 0.041777\nEpoch [665/800], Batch [60/98], Loss: 5.2488, LR: 0.041777\nEpoch [665/800], Batch [70/98], Loss: 5.2301, LR: 0.041777\nEpoch [665/800], Batch [80/98], Loss: 5.2391, LR: 0.041777\nEpoch [665/800], Batch [90/98], Loss: 5.2493, LR: 0.041777\nEpoch [665/800] Average Loss: 5.2373\n\nEpoch [666/800], Batch [0/98], Loss: 5.2904, LR: 0.041180\nEpoch [666/800], Batch [10/98], Loss: 5.2337, LR: 0.041180\nEpoch [666/800], Batch [20/98], Loss: 5.2356, LR: 0.041180\nEpoch [666/800], Batch [30/98], Loss: 5.2444, LR: 0.041180\nEpoch [666/800], Batch [40/98], Loss: 5.2207, LR: 0.041180\nEpoch [666/800], Batch [50/98], Loss: 5.2361, LR: 0.041180\nEpoch [666/800], Batch [60/98], Loss: 5.2419, LR: 0.041180\nEpoch [666/800], Batch [70/98], Loss: 5.2761, LR: 0.041180\nEpoch [666/800], Batch [80/98], Loss: 5.2505, LR: 0.041180\nEpoch [666/800], Batch [90/98], Loss: 5.2527, LR: 0.041180\nEpoch [666/800] Average Loss: 5.2412\n\nEpoch [667/800], Batch [0/98], Loss: 5.2502, LR: 0.040586\nEpoch [667/800], Batch [10/98], Loss: 5.2515, LR: 0.040586\nEpoch [667/800], Batch [20/98], Loss: 5.2341, LR: 0.040586\nEpoch [667/800], Batch [30/98], Loss: 5.2514, LR: 0.040586\nEpoch [667/800], Batch [40/98], Loss: 5.2376, LR: 0.040586\nEpoch [667/800], Batch [50/98], Loss: 5.2631, LR: 0.040586\nEpoch [667/800], Batch [60/98], Loss: 5.2717, LR: 0.040586\nEpoch [667/800], Batch [70/98], Loss: 5.2469, LR: 0.040586\nEpoch [667/800], Batch [80/98], Loss: 5.2445, LR: 0.040586\nEpoch [667/800], Batch [90/98], Loss: 5.2308, LR: 0.040586\nEpoch [667/800] Average Loss: 5.2375\n\nEpoch [668/800], Batch [0/98], Loss: 5.2098, LR: 0.039996\nEpoch [668/800], Batch [10/98], Loss: 5.2453, LR: 0.039996\nEpoch [668/800], Batch [20/98], Loss: 5.2435, LR: 0.039996\nEpoch [668/800], Batch [30/98], Loss: 5.2520, LR: 0.039996\nEpoch [668/800], Batch [40/98], Loss: 5.2367, LR: 0.039996\nEpoch [668/800], Batch [50/98], Loss: 5.2456, LR: 0.039996\nEpoch [668/800], Batch [60/98], Loss: 5.2614, LR: 0.039996\nEpoch [668/800], Batch [70/98], Loss: 5.2274, LR: 0.039996\nEpoch [668/800], Batch [80/98], Loss: 5.2532, LR: 0.039996\nEpoch [668/800], Batch [90/98], Loss: 5.2402, LR: 0.039996\nEpoch [668/800] Average Loss: 5.2386\n\nEpoch [669/800], Batch [0/98], Loss: 5.2406, LR: 0.039411\nEpoch [669/800], Batch [10/98], Loss: 5.2396, LR: 0.039411\nEpoch [669/800], Batch [20/98], Loss: 5.2411, LR: 0.039411\nEpoch [669/800], Batch [30/98], Loss: 5.2379, LR: 0.039411\nEpoch [669/800], Batch [40/98], Loss: 5.2243, LR: 0.039411\nEpoch [669/800], Batch [50/98], Loss: 5.2586, LR: 0.039411\nEpoch [669/800], Batch [60/98], Loss: 5.2533, LR: 0.039411\nEpoch [669/800], Batch [70/98], Loss: 5.2326, LR: 0.039411\nEpoch [669/800], Batch [80/98], Loss: 5.2182, LR: 0.039411\nEpoch [669/800], Batch [90/98], Loss: 5.2431, LR: 0.039411\nEpoch [669/800] Average Loss: 5.2404\n\nEpoch [670/800], Batch [0/98], Loss: 5.2254, LR: 0.038829\nEpoch [670/800], Batch [10/98], Loss: 5.2267, LR: 0.038829\nEpoch [670/800], Batch [20/98], Loss: 5.2364, LR: 0.038829\nEpoch [670/800], Batch [30/98], Loss: 5.2599, LR: 0.038829\nEpoch [670/800], Batch [40/98], Loss: 5.2430, LR: 0.038829\nEpoch [670/800], Batch [50/98], Loss: 5.2343, LR: 0.038829\nEpoch [670/800], Batch [60/98], Loss: 5.2323, LR: 0.038829\nEpoch [670/800], Batch [70/98], Loss: 5.2409, LR: 0.038829\nEpoch [670/800], Batch [80/98], Loss: 5.2289, LR: 0.038829\nEpoch [670/800], Batch [90/98], Loss: 5.2468, LR: 0.038829\nEpoch [670/800] Average Loss: 5.2373\n\nEpoch [671/800], Batch [0/98], Loss: 5.2568, LR: 0.038251\nEpoch [671/800], Batch [10/98], Loss: 5.2586, LR: 0.038251\nEpoch [671/800], Batch [20/98], Loss: 5.2218, LR: 0.038251\nEpoch [671/800], Batch [30/98], Loss: 5.2164, LR: 0.038251\nEpoch [671/800], Batch [40/98], Loss: 5.2292, LR: 0.038251\nEpoch [671/800], Batch [50/98], Loss: 5.2279, LR: 0.038251\nEpoch [671/800], Batch [60/98], Loss: 5.2680, LR: 0.038251\nEpoch [671/800], Batch [70/98], Loss: 5.2577, LR: 0.038251\nEpoch [671/800], Batch [80/98], Loss: 5.2288, LR: 0.038251\nEpoch [671/800], Batch [90/98], Loss: 5.2520, LR: 0.038251\nEpoch [671/800] Average Loss: 5.2391\n\nEpoch [672/800], Batch [0/98], Loss: 5.2420, LR: 0.037678\nEpoch [672/800], Batch [10/98], Loss: 5.2437, LR: 0.037678\nEpoch [672/800], Batch [20/98], Loss: 5.2530, LR: 0.037678\nEpoch [672/800], Batch [30/98], Loss: 5.2393, LR: 0.037678\nEpoch [672/800], Batch [40/98], Loss: 5.2221, LR: 0.037678\nEpoch [672/800], Batch [50/98], Loss: 5.2218, LR: 0.037678\nEpoch [672/800], Batch [60/98], Loss: 5.2638, LR: 0.037678\nEpoch [672/800], Batch [70/98], Loss: 5.2510, LR: 0.037678\nEpoch [672/800], Batch [80/98], Loss: 5.2537, LR: 0.037678\nEpoch [672/800], Batch [90/98], Loss: 5.2453, LR: 0.037678\nEpoch [672/800] Average Loss: 5.2379\n\nEpoch [673/800], Batch [0/98], Loss: 5.2401, LR: 0.037108\nEpoch [673/800], Batch [10/98], Loss: 5.2573, LR: 0.037108\nEpoch [673/800], Batch [20/98], Loss: 5.2605, LR: 0.037108\nEpoch [673/800], Batch [30/98], Loss: 5.2559, LR: 0.037108\nEpoch [673/800], Batch [40/98], Loss: 5.2306, LR: 0.037108\nEpoch [673/800], Batch [50/98], Loss: 5.2172, LR: 0.037108\nEpoch [673/800], Batch [60/98], Loss: 5.2263, LR: 0.037108\nEpoch [673/800], Batch [70/98], Loss: 5.2091, LR: 0.037108\nEpoch [673/800], Batch [80/98], Loss: 5.2251, LR: 0.037108\nEpoch [673/800], Batch [90/98], Loss: 5.2168, LR: 0.037108\nEpoch [673/800] Average Loss: 5.2373\n\nEpoch [674/800], Batch [0/98], Loss: 5.2395, LR: 0.036542\nEpoch [674/800], Batch [10/98], Loss: 5.2575, LR: 0.036542\nEpoch [674/800], Batch [20/98], Loss: 5.2551, LR: 0.036542\nEpoch [674/800], Batch [30/98], Loss: 5.2604, LR: 0.036542\nEpoch [674/800], Batch [40/98], Loss: 5.2452, LR: 0.036542\nEpoch [674/800], Batch [50/98], Loss: 5.2350, LR: 0.036542\nEpoch [674/800], Batch [60/98], Loss: 5.2332, LR: 0.036542\nEpoch [674/800], Batch [70/98], Loss: 5.2430, LR: 0.036542\nEpoch [674/800], Batch [80/98], Loss: 5.2351, LR: 0.036542\nEpoch [674/800], Batch [90/98], Loss: 5.2535, LR: 0.036542\nEpoch [674/800] Average Loss: 5.2363\n\nEpoch [675/800], Batch [0/98], Loss: 5.2496, LR: 0.035981\nEpoch [675/800], Batch [10/98], Loss: 5.2371, LR: 0.035981\nEpoch [675/800], Batch [20/98], Loss: 5.2125, LR: 0.035981\nEpoch [675/800], Batch [30/98], Loss: 5.2513, LR: 0.035981\nEpoch [675/800], Batch [40/98], Loss: 5.2479, LR: 0.035981\nEpoch [675/800], Batch [50/98], Loss: 5.2413, LR: 0.035981\nEpoch [675/800], Batch [60/98], Loss: 5.2412, LR: 0.035981\nEpoch [675/800], Batch [70/98], Loss: 5.2433, LR: 0.035981\nEpoch [675/800], Batch [80/98], Loss: 5.2358, LR: 0.035981\nEpoch [675/800], Batch [90/98], Loss: 5.2393, LR: 0.035981\nEpoch [675/800] Average Loss: 5.2366\n\nEpoch [676/800], Batch [0/98], Loss: 5.2215, LR: 0.035424\nEpoch [676/800], Batch [10/98], Loss: 5.2256, LR: 0.035424\nEpoch [676/800], Batch [20/98], Loss: 5.2349, LR: 0.035424\nEpoch [676/800], Batch [30/98], Loss: 5.2197, LR: 0.035424\nEpoch [676/800], Batch [40/98], Loss: 5.2561, LR: 0.035424\nEpoch [676/800], Batch [50/98], Loss: 5.2612, LR: 0.035424\nEpoch [676/800], Batch [60/98], Loss: 5.2470, LR: 0.035424\nEpoch [676/800], Batch [70/98], Loss: 5.2290, LR: 0.035424\nEpoch [676/800], Batch [80/98], Loss: 5.2508, LR: 0.035424\nEpoch [676/800], Batch [90/98], Loss: 5.2453, LR: 0.035424\nEpoch [676/800] Average Loss: 5.2370\n\nEpoch [677/800], Batch [0/98], Loss: 5.2427, LR: 0.034870\nEpoch [677/800], Batch [10/98], Loss: 5.2206, LR: 0.034870\nEpoch [677/800], Batch [20/98], Loss: 5.2510, LR: 0.034870\nEpoch [677/800], Batch [30/98], Loss: 5.2289, LR: 0.034870\nEpoch [677/800], Batch [40/98], Loss: 5.2673, LR: 0.034870\nEpoch [677/800], Batch [50/98], Loss: 5.2343, LR: 0.034870\nEpoch [677/800], Batch [60/98], Loss: 5.2197, LR: 0.034870\nEpoch [677/800], Batch [70/98], Loss: 5.2542, LR: 0.034870\nEpoch [677/800], Batch [80/98], Loss: 5.2299, LR: 0.034870\nEpoch [677/800], Batch [90/98], Loss: 5.2498, LR: 0.034870\nEpoch [677/800] Average Loss: 5.2376\n\nEpoch [678/800], Batch [0/98], Loss: 5.2413, LR: 0.034321\nEpoch [678/800], Batch [10/98], Loss: 5.2277, LR: 0.034321\nEpoch [678/800], Batch [20/98], Loss: 5.2407, LR: 0.034321\nEpoch [678/800], Batch [30/98], Loss: 5.2587, LR: 0.034321\nEpoch [678/800], Batch [40/98], Loss: 5.2574, LR: 0.034321\nEpoch [678/800], Batch [50/98], Loss: 5.2348, LR: 0.034321\nEpoch [678/800], Batch [60/98], Loss: 5.2335, LR: 0.034321\nEpoch [678/800], Batch [70/98], Loss: 5.2331, LR: 0.034321\nEpoch [678/800], Batch [80/98], Loss: 5.2453, LR: 0.034321\nEpoch [678/800], Batch [90/98], Loss: 5.2380, LR: 0.034321\nEpoch [678/800] Average Loss: 5.2365\n\nEpoch [679/800], Batch [0/98], Loss: 5.2432, LR: 0.033776\nEpoch [679/800], Batch [10/98], Loss: 5.2133, LR: 0.033776\nEpoch [679/800], Batch [20/98], Loss: 5.2559, LR: 0.033776\nEpoch [679/800], Batch [30/98], Loss: 5.2378, LR: 0.033776\nEpoch [679/800], Batch [40/98], Loss: 5.2377, LR: 0.033776\nEpoch [679/800], Batch [50/98], Loss: 5.2632, LR: 0.033776\nEpoch [679/800], Batch [60/98], Loss: 5.2472, LR: 0.033776\nEpoch [679/800], Batch [70/98], Loss: 5.2367, LR: 0.033776\nEpoch [679/800], Batch [80/98], Loss: 5.2344, LR: 0.033776\nEpoch [679/800], Batch [90/98], Loss: 5.2256, LR: 0.033776\nEpoch [679/800] Average Loss: 5.2382\n\nEpoch [680/800], Batch [0/98], Loss: 5.2311, LR: 0.033235\nEpoch [680/800], Batch [10/98], Loss: 5.2383, LR: 0.033235\nEpoch [680/800], Batch [20/98], Loss: 5.2241, LR: 0.033235\nEpoch [680/800], Batch [30/98], Loss: 5.2526, LR: 0.033235\nEpoch [680/800], Batch [40/98], Loss: 5.2480, LR: 0.033235\nEpoch [680/800], Batch [50/98], Loss: 5.2193, LR: 0.033235\nEpoch [680/800], Batch [60/98], Loss: 5.2534, LR: 0.033235\nEpoch [680/800], Batch [70/98], Loss: 5.2330, LR: 0.033235\nEpoch [680/800], Batch [80/98], Loss: 5.2422, LR: 0.033235\nEpoch [680/800], Batch [90/98], Loss: 5.2226, LR: 0.033235\nEpoch [680/800] Average Loss: 5.2357\n\nEpoch [681/800], Batch [0/98], Loss: 5.2459, LR: 0.032698\nEpoch [681/800], Batch [10/98], Loss: 5.2462, LR: 0.032698\nEpoch [681/800], Batch [20/98], Loss: 5.2446, LR: 0.032698\nEpoch [681/800], Batch [30/98], Loss: 5.2806, LR: 0.032698\nEpoch [681/800], Batch [40/98], Loss: 5.2396, LR: 0.032698\nEpoch [681/800], Batch [50/98], Loss: 5.2207, LR: 0.032698\nEpoch [681/800], Batch [60/98], Loss: 5.2476, LR: 0.032698\nEpoch [681/800], Batch [70/98], Loss: 5.2384, LR: 0.032698\nEpoch [681/800], Batch [80/98], Loss: 5.2172, LR: 0.032698\nEpoch [681/800], Batch [90/98], Loss: 5.2346, LR: 0.032698\nEpoch [681/800] Average Loss: 5.2357\n\nEpoch [682/800], Batch [0/98], Loss: 5.2372, LR: 0.032165\nEpoch [682/800], Batch [10/98], Loss: 5.2388, LR: 0.032165\nEpoch [682/800], Batch [20/98], Loss: 5.2182, LR: 0.032165\nEpoch [682/800], Batch [30/98], Loss: 5.2352, LR: 0.032165\nEpoch [682/800], Batch [40/98], Loss: 5.2244, LR: 0.032165\nEpoch [682/800], Batch [50/98], Loss: 5.2459, LR: 0.032165\nEpoch [682/800], Batch [60/98], Loss: 5.2523, LR: 0.032165\nEpoch [682/800], Batch [70/98], Loss: 5.2218, LR: 0.032165\nEpoch [682/800], Batch [80/98], Loss: 5.2656, LR: 0.032165\nEpoch [682/800], Batch [90/98], Loss: 5.2254, LR: 0.032165\nEpoch [682/800] Average Loss: 5.2363\n\nEpoch [683/800], Batch [0/98], Loss: 5.2384, LR: 0.031637\nEpoch [683/800], Batch [10/98], Loss: 5.2346, LR: 0.031637\nEpoch [683/800], Batch [20/98], Loss: 5.2492, LR: 0.031637\nEpoch [683/800], Batch [30/98], Loss: 5.2345, LR: 0.031637\nEpoch [683/800], Batch [40/98], Loss: 5.2161, LR: 0.031637\nEpoch [683/800], Batch [50/98], Loss: 5.2439, LR: 0.031637\nEpoch [683/800], Batch [60/98], Loss: 5.2406, LR: 0.031637\nEpoch [683/800], Batch [70/98], Loss: 5.2522, LR: 0.031637\nEpoch [683/800], Batch [80/98], Loss: 5.2510, LR: 0.031637\nEpoch [683/800], Batch [90/98], Loss: 5.2484, LR: 0.031637\nEpoch [683/800] Average Loss: 5.2365\n\nEpoch [684/800], Batch [0/98], Loss: 5.2492, LR: 0.031112\nEpoch [684/800], Batch [10/98], Loss: 5.2490, LR: 0.031112\nEpoch [684/800], Batch [20/98], Loss: 5.2549, LR: 0.031112\nEpoch [684/800], Batch [30/98], Loss: 5.2417, LR: 0.031112\nEpoch [684/800], Batch [40/98], Loss: 5.2418, LR: 0.031112\nEpoch [684/800], Batch [50/98], Loss: 5.2442, LR: 0.031112\nEpoch [684/800], Batch [60/98], Loss: 5.2711, LR: 0.031112\nEpoch [684/800], Batch [70/98], Loss: 5.2472, LR: 0.031112\nEpoch [684/800], Batch [80/98], Loss: 5.2244, LR: 0.031112\nEpoch [684/800], Batch [90/98], Loss: 5.2654, LR: 0.031112\nEpoch [684/800] Average Loss: 5.2386\n\nEpoch [685/800], Batch [0/98], Loss: 5.2331, LR: 0.030592\nEpoch [685/800], Batch [10/98], Loss: 5.2283, LR: 0.030592\nEpoch [685/800], Batch [20/98], Loss: 5.2588, LR: 0.030592\nEpoch [685/800], Batch [30/98], Loss: 5.2337, LR: 0.030592\nEpoch [685/800], Batch [40/98], Loss: 5.2626, LR: 0.030592\nEpoch [685/800], Batch [50/98], Loss: 5.2504, LR: 0.030592\nEpoch [685/800], Batch [60/98], Loss: 5.2504, LR: 0.030592\nEpoch [685/800], Batch [70/98], Loss: 5.2453, LR: 0.030592\nEpoch [685/800], Batch [80/98], Loss: 5.2275, LR: 0.030592\nEpoch [685/800], Batch [90/98], Loss: 5.2808, LR: 0.030592\nEpoch [685/800] Average Loss: 5.2365\n\nEpoch [686/800], Batch [0/98], Loss: 5.2172, LR: 0.030076\nEpoch [686/800], Batch [10/98], Loss: 5.2580, LR: 0.030076\nEpoch [686/800], Batch [20/98], Loss: 5.2445, LR: 0.030076\nEpoch [686/800], Batch [30/98], Loss: 5.2338, LR: 0.030076\nEpoch [686/800], Batch [40/98], Loss: 5.2324, LR: 0.030076\nEpoch [686/800], Batch [50/98], Loss: 5.2458, LR: 0.030076\nEpoch [686/800], Batch [60/98], Loss: 5.2250, LR: 0.030076\nEpoch [686/800], Batch [70/98], Loss: 5.2579, LR: 0.030076\nEpoch [686/800], Batch [80/98], Loss: 5.2493, LR: 0.030076\nEpoch [686/800], Batch [90/98], Loss: 5.2253, LR: 0.030076\nEpoch [686/800] Average Loss: 5.2360\n\nEpoch [687/800], Batch [0/98], Loss: 5.2473, LR: 0.029563\nEpoch [687/800], Batch [10/98], Loss: 5.2325, LR: 0.029563\nEpoch [687/800], Batch [20/98], Loss: 5.2764, LR: 0.029563\nEpoch [687/800], Batch [30/98], Loss: 5.2552, LR: 0.029563\nEpoch [687/800], Batch [40/98], Loss: 5.2456, LR: 0.029563\nEpoch [687/800], Batch [50/98], Loss: 5.2275, LR: 0.029563\nEpoch [687/800], Batch [60/98], Loss: 5.2503, LR: 0.029563\nEpoch [687/800], Batch [70/98], Loss: 5.2366, LR: 0.029563\nEpoch [687/800], Batch [80/98], Loss: 5.2283, LR: 0.029563\nEpoch [687/800], Batch [90/98], Loss: 5.2469, LR: 0.029563\nEpoch [687/800] Average Loss: 5.2372\n\nEpoch [688/800], Batch [0/98], Loss: 5.2337, LR: 0.029056\nEpoch [688/800], Batch [10/98], Loss: 5.2490, LR: 0.029056\nEpoch [688/800], Batch [20/98], Loss: 5.2551, LR: 0.029056\nEpoch [688/800], Batch [30/98], Loss: 5.2452, LR: 0.029056\nEpoch [688/800], Batch [40/98], Loss: 5.2545, LR: 0.029056\nEpoch [688/800], Batch [50/98], Loss: 5.2491, LR: 0.029056\nEpoch [688/800], Batch [60/98], Loss: 5.2308, LR: 0.029056\nEpoch [688/800], Batch [70/98], Loss: 5.2408, LR: 0.029056\nEpoch [688/800], Batch [80/98], Loss: 5.2501, LR: 0.029056\nEpoch [688/800], Batch [90/98], Loss: 5.2312, LR: 0.029056\nEpoch [688/800] Average Loss: 5.2374\n\nEpoch [689/800], Batch [0/98], Loss: 5.2612, LR: 0.028552\nEpoch [689/800], Batch [10/98], Loss: 5.2299, LR: 0.028552\nEpoch [689/800], Batch [20/98], Loss: 5.2505, LR: 0.028552\nEpoch [689/800], Batch [30/98], Loss: 5.2436, LR: 0.028552\nEpoch [689/800], Batch [40/98], Loss: 5.2245, LR: 0.028552\nEpoch [689/800], Batch [50/98], Loss: 5.2185, LR: 0.028552\nEpoch [689/800], Batch [60/98], Loss: 5.2059, LR: 0.028552\nEpoch [689/800], Batch [70/98], Loss: 5.2257, LR: 0.028552\nEpoch [689/800], Batch [80/98], Loss: 5.2299, LR: 0.028552\nEpoch [689/800], Batch [90/98], Loss: 5.2675, LR: 0.028552\nEpoch [689/800] Average Loss: 5.2346\n\nEpoch [690/800], Batch [0/98], Loss: 5.2544, LR: 0.028052\nEpoch [690/800], Batch [10/98], Loss: 5.2486, LR: 0.028052\nEpoch [690/800], Batch [20/98], Loss: 5.2321, LR: 0.028052\nEpoch [690/800], Batch [30/98], Loss: 5.2413, LR: 0.028052\nEpoch [690/800], Batch [40/98], Loss: 5.2403, LR: 0.028052\nEpoch [690/800], Batch [50/98], Loss: 5.2110, LR: 0.028052\nEpoch [690/800], Batch [60/98], Loss: 5.2231, LR: 0.028052\nEpoch [690/800], Batch [70/98], Loss: 5.2564, LR: 0.028052\nEpoch [690/800], Batch [80/98], Loss: 5.2380, LR: 0.028052\nEpoch [690/800], Batch [90/98], Loss: 5.2767, LR: 0.028052\nEpoch [690/800] Average Loss: 5.2354\n\nEpoch [691/800], Batch [0/98], Loss: 5.2464, LR: 0.027557\nEpoch [691/800], Batch [10/98], Loss: 5.2553, LR: 0.027557\nEpoch [691/800], Batch [20/98], Loss: 5.2302, LR: 0.027557\nEpoch [691/800], Batch [30/98], Loss: 5.2406, LR: 0.027557\nEpoch [691/800], Batch [40/98], Loss: 5.2387, LR: 0.027557\nEpoch [691/800], Batch [50/98], Loss: 5.2303, LR: 0.027557\nEpoch [691/800], Batch [60/98], Loss: 5.2383, LR: 0.027557\nEpoch [691/800], Batch [70/98], Loss: 5.2500, LR: 0.027557\nEpoch [691/800], Batch [80/98], Loss: 5.2356, LR: 0.027557\nEpoch [691/800], Batch [90/98], Loss: 5.2370, LR: 0.027557\nEpoch [691/800] Average Loss: 5.2344\n\nEpoch [692/800], Batch [0/98], Loss: 5.2296, LR: 0.027066\nEpoch [692/800], Batch [10/98], Loss: 5.2215, LR: 0.027066\nEpoch [692/800], Batch [20/98], Loss: 5.2347, LR: 0.027066\nEpoch [692/800], Batch [30/98], Loss: 5.2484, LR: 0.027066\nEpoch [692/800], Batch [40/98], Loss: 5.2512, LR: 0.027066\nEpoch [692/800], Batch [50/98], Loss: 5.2097, LR: 0.027066\nEpoch [692/800], Batch [60/98], Loss: 5.2318, LR: 0.027066\nEpoch [692/800], Batch [70/98], Loss: 5.2275, LR: 0.027066\nEpoch [692/800], Batch [80/98], Loss: 5.2436, LR: 0.027066\nEpoch [692/800], Batch [90/98], Loss: 5.2199, LR: 0.027066\nEpoch [692/800] Average Loss: 5.2358\n\nEpoch [693/800], Batch [0/98], Loss: 5.2299, LR: 0.026579\nEpoch [693/800], Batch [10/98], Loss: 5.2569, LR: 0.026579\nEpoch [693/800], Batch [20/98], Loss: 5.2592, LR: 0.026579\nEpoch [693/800], Batch [30/98], Loss: 5.2640, LR: 0.026579\nEpoch [693/800], Batch [40/98], Loss: 5.2267, LR: 0.026579\nEpoch [693/800], Batch [50/98], Loss: 5.2380, LR: 0.026579\nEpoch [693/800], Batch [60/98], Loss: 5.2287, LR: 0.026579\nEpoch [693/800], Batch [70/98], Loss: 5.2242, LR: 0.026579\nEpoch [693/800], Batch [80/98], Loss: 5.2485, LR: 0.026579\nEpoch [693/800], Batch [90/98], Loss: 5.2494, LR: 0.026579\nEpoch [693/800] Average Loss: 5.2347\n\nEpoch [694/800], Batch [0/98], Loss: 5.2363, LR: 0.026096\nEpoch [694/800], Batch [10/98], Loss: 5.2386, LR: 0.026096\nEpoch [694/800], Batch [20/98], Loss: 5.2276, LR: 0.026096\nEpoch [694/800], Batch [30/98], Loss: 5.2488, LR: 0.026096\nEpoch [694/800], Batch [40/98], Loss: 5.2448, LR: 0.026096\nEpoch [694/800], Batch [50/98], Loss: 5.2515, LR: 0.026096\nEpoch [694/800], Batch [60/98], Loss: 5.2425, LR: 0.026096\nEpoch [694/800], Batch [70/98], Loss: 5.2376, LR: 0.026096\nEpoch [694/800], Batch [80/98], Loss: 5.2531, LR: 0.026096\nEpoch [694/800], Batch [90/98], Loss: 5.2365, LR: 0.026096\nEpoch [694/800] Average Loss: 5.2364\n\nEpoch [695/800], Batch [0/98], Loss: 5.2281, LR: 0.025618\nEpoch [695/800], Batch [10/98], Loss: 5.2459, LR: 0.025618\nEpoch [695/800], Batch [20/98], Loss: 5.2463, LR: 0.025618\nEpoch [695/800], Batch [30/98], Loss: 5.2628, LR: 0.025618\nEpoch [695/800], Batch [40/98], Loss: 5.2336, LR: 0.025618\nEpoch [695/800], Batch [50/98], Loss: 5.2326, LR: 0.025618\nEpoch [695/800], Batch [60/98], Loss: 5.2364, LR: 0.025618\nEpoch [695/800], Batch [70/98], Loss: 5.2376, LR: 0.025618\nEpoch [695/800], Batch [80/98], Loss: 5.2144, LR: 0.025618\nEpoch [695/800], Batch [90/98], Loss: 5.2618, LR: 0.025618\nEpoch [695/800] Average Loss: 5.2335\n\nEpoch [696/800], Batch [0/98], Loss: 5.2391, LR: 0.025144\nEpoch [696/800], Batch [10/98], Loss: 5.2586, LR: 0.025144\nEpoch [696/800], Batch [20/98], Loss: 5.2503, LR: 0.025144\nEpoch [696/800], Batch [30/98], Loss: 5.2247, LR: 0.025144\nEpoch [696/800], Batch [40/98], Loss: 5.2334, LR: 0.025144\nEpoch [696/800], Batch [50/98], Loss: 5.2173, LR: 0.025144\nEpoch [696/800], Batch [60/98], Loss: 5.2281, LR: 0.025144\nEpoch [696/800], Batch [70/98], Loss: 5.2340, LR: 0.025144\nEpoch [696/800], Batch [80/98], Loss: 5.2369, LR: 0.025144\nEpoch [696/800], Batch [90/98], Loss: 5.2278, LR: 0.025144\nEpoch [696/800] Average Loss: 5.2343\n\nEpoch [697/800], Batch [0/98], Loss: 5.2476, LR: 0.024674\nEpoch [697/800], Batch [10/98], Loss: 5.2415, LR: 0.024674\nEpoch [697/800], Batch [20/98], Loss: 5.2337, LR: 0.024674\nEpoch [697/800], Batch [30/98], Loss: 5.2228, LR: 0.024674\nEpoch [697/800], Batch [40/98], Loss: 5.2325, LR: 0.024674\nEpoch [697/800], Batch [50/98], Loss: 5.2652, LR: 0.024674\nEpoch [697/800], Batch [60/98], Loss: 5.2379, LR: 0.024674\nEpoch [697/800], Batch [70/98], Loss: 5.2494, LR: 0.024674\nEpoch [697/800], Batch [80/98], Loss: 5.2525, LR: 0.024674\nEpoch [697/800], Batch [90/98], Loss: 5.2289, LR: 0.024674\nEpoch [697/800] Average Loss: 5.2366\n\nEpoch [698/800], Batch [0/98], Loss: 5.2131, LR: 0.024208\nEpoch [698/800], Batch [10/98], Loss: 5.2264, LR: 0.024208\nEpoch [698/800], Batch [20/98], Loss: 5.2319, LR: 0.024208\nEpoch [698/800], Batch [30/98], Loss: 5.2294, LR: 0.024208\nEpoch [698/800], Batch [40/98], Loss: 5.2360, LR: 0.024208\nEpoch [698/800], Batch [50/98], Loss: 5.2339, LR: 0.024208\nEpoch [698/800], Batch [60/98], Loss: 5.2672, LR: 0.024208\nEpoch [698/800], Batch [70/98], Loss: 5.2341, LR: 0.024208\nEpoch [698/800], Batch [80/98], Loss: 5.2435, LR: 0.024208\nEpoch [698/800], Batch [90/98], Loss: 5.2468, LR: 0.024208\nEpoch [698/800] Average Loss: 5.2338\n\nEpoch [699/800], Batch [0/98], Loss: 5.2332, LR: 0.023746\nEpoch [699/800], Batch [10/98], Loss: 5.2138, LR: 0.023746\nEpoch [699/800], Batch [20/98], Loss: 5.2377, LR: 0.023746\nEpoch [699/800], Batch [30/98], Loss: 5.2397, LR: 0.023746\nEpoch [699/800], Batch [40/98], Loss: 5.2350, LR: 0.023746\nEpoch [699/800], Batch [50/98], Loss: 5.2432, LR: 0.023746\nEpoch [699/800], Batch [60/98], Loss: 5.2243, LR: 0.023746\nEpoch [699/800], Batch [70/98], Loss: 5.2600, LR: 0.023746\nEpoch [699/800], Batch [80/98], Loss: 5.2220, LR: 0.023746\nEpoch [699/800], Batch [90/98], Loss: 5.2322, LR: 0.023746\nEpoch [699/800] Average Loss: 5.2340\n\nEpoch [700/800], Batch [0/98], Loss: 5.2357, LR: 0.023289\nEpoch [700/800], Batch [10/98], Loss: 5.2492, LR: 0.023289\nEpoch [700/800], Batch [20/98], Loss: 5.2381, LR: 0.023289\nEpoch [700/800], Batch [30/98], Loss: 5.2421, LR: 0.023289\nEpoch [700/800], Batch [40/98], Loss: 5.2230, LR: 0.023289\nEpoch [700/800], Batch [50/98], Loss: 5.2600, LR: 0.023289\nEpoch [700/800], Batch [60/98], Loss: 5.2371, LR: 0.023289\nEpoch [700/800], Batch [70/98], Loss: 5.2678, LR: 0.023289\nEpoch [700/800], Batch [80/98], Loss: 5.2212, LR: 0.023289\nEpoch [700/800], Batch [90/98], Loss: 5.2442, LR: 0.023289\nEpoch [700/800] Average Loss: 5.2339\n\nEpoch [701/800], Batch [0/98], Loss: 5.2626, LR: 0.022836\nEpoch [701/800], Batch [10/98], Loss: 5.2478, LR: 0.022836\nEpoch [701/800], Batch [20/98], Loss: 5.2362, LR: 0.022836\nEpoch [701/800], Batch [30/98], Loss: 5.2412, LR: 0.022836\nEpoch [701/800], Batch [40/98], Loss: 5.2200, LR: 0.022836\nEpoch [701/800], Batch [50/98], Loss: 5.2172, LR: 0.022836\nEpoch [701/800], Batch [60/98], Loss: 5.2345, LR: 0.022836\nEpoch [701/800], Batch [70/98], Loss: 5.2310, LR: 0.022836\nEpoch [701/800], Batch [80/98], Loss: 5.2323, LR: 0.022836\nEpoch [701/800], Batch [90/98], Loss: 5.2357, LR: 0.022836\nEpoch [701/800] Average Loss: 5.2342\n\nEpoch [702/800], Batch [0/98], Loss: 5.2588, LR: 0.022387\nEpoch [702/800], Batch [10/98], Loss: 5.2271, LR: 0.022387\nEpoch [702/800], Batch [20/98], Loss: 5.2353, LR: 0.022387\nEpoch [702/800], Batch [30/98], Loss: 5.2342, LR: 0.022387\nEpoch [702/800], Batch [40/98], Loss: 5.2305, LR: 0.022387\nEpoch [702/800], Batch [50/98], Loss: 5.2323, LR: 0.022387\nEpoch [702/800], Batch [60/98], Loss: 5.2453, LR: 0.022387\nEpoch [702/800], Batch [70/98], Loss: 5.2312, LR: 0.022387\nEpoch [702/800], Batch [80/98], Loss: 5.2237, LR: 0.022387\nEpoch [702/800], Batch [90/98], Loss: 5.2580, LR: 0.022387\nEpoch [702/800] Average Loss: 5.2350\n\nEpoch [703/800], Batch [0/98], Loss: 5.2202, LR: 0.021943\nEpoch [703/800], Batch [10/98], Loss: 5.2259, LR: 0.021943\nEpoch [703/800], Batch [20/98], Loss: 5.2369, LR: 0.021943\nEpoch [703/800], Batch [30/98], Loss: 5.2418, LR: 0.021943\nEpoch [703/800], Batch [40/98], Loss: 5.2377, LR: 0.021943\nEpoch [703/800], Batch [50/98], Loss: 5.2518, LR: 0.021943\nEpoch [703/800], Batch [60/98], Loss: 5.2383, LR: 0.021943\nEpoch [703/800], Batch [70/98], Loss: 5.2465, LR: 0.021943\nEpoch [703/800], Batch [80/98], Loss: 5.2420, LR: 0.021943\nEpoch [703/800], Batch [90/98], Loss: 5.2590, LR: 0.021943\nEpoch [703/800] Average Loss: 5.2338\n\nEpoch [704/800], Batch [0/98], Loss: 5.2274, LR: 0.021503\nEpoch [704/800], Batch [10/98], Loss: 5.2332, LR: 0.021503\nEpoch [704/800], Batch [20/98], Loss: 5.2210, LR: 0.021503\nEpoch [704/800], Batch [30/98], Loss: 5.2252, LR: 0.021503\nEpoch [704/800], Batch [40/98], Loss: 5.2460, LR: 0.021503\nEpoch [704/800], Batch [50/98], Loss: 5.2193, LR: 0.021503\nEpoch [704/800], Batch [60/98], Loss: 5.2501, LR: 0.021503\nEpoch [704/800], Batch [70/98], Loss: 5.2487, LR: 0.021503\nEpoch [704/800], Batch [80/98], Loss: 5.2355, LR: 0.021503\nEpoch [704/800], Batch [90/98], Loss: 5.2476, LR: 0.021503\nEpoch [704/800] Average Loss: 5.2318\n\nEpoch [705/800], Batch [0/98], Loss: 5.2512, LR: 0.021067\nEpoch [705/800], Batch [10/98], Loss: 5.2518, LR: 0.021067\nEpoch [705/800], Batch [20/98], Loss: 5.2376, LR: 0.021067\nEpoch [705/800], Batch [30/98], Loss: 5.2181, LR: 0.021067\nEpoch [705/800], Batch [40/98], Loss: 5.2249, LR: 0.021067\nEpoch [705/800], Batch [50/98], Loss: 5.2468, LR: 0.021067\nEpoch [705/800], Batch [60/98], Loss: 5.2379, LR: 0.021067\nEpoch [705/800], Batch [70/98], Loss: 5.2276, LR: 0.021067\nEpoch [705/800], Batch [80/98], Loss: 5.2482, LR: 0.021067\nEpoch [705/800], Batch [90/98], Loss: 5.2164, LR: 0.021067\nEpoch [705/800] Average Loss: 5.2337\n\nEpoch [706/800], Batch [0/98], Loss: 5.2403, LR: 0.020636\nEpoch [706/800], Batch [10/98], Loss: 5.2452, LR: 0.020636\nEpoch [706/800], Batch [20/98], Loss: 5.2399, LR: 0.020636\nEpoch [706/800], Batch [30/98], Loss: 5.2371, LR: 0.020636\nEpoch [706/800], Batch [40/98], Loss: 5.2691, LR: 0.020636\nEpoch [706/800], Batch [50/98], Loss: 5.2380, LR: 0.020636\nEpoch [706/800], Batch [60/98], Loss: 5.2375, LR: 0.020636\nEpoch [706/800], Batch [70/98], Loss: 5.2478, LR: 0.020636\nEpoch [706/800], Batch [80/98], Loss: 5.2451, LR: 0.020636\nEpoch [706/800], Batch [90/98], Loss: 5.2018, LR: 0.020636\nEpoch [706/800] Average Loss: 5.2331\n\nEpoch [707/800], Batch [0/98], Loss: 5.2323, LR: 0.020208\nEpoch [707/800], Batch [10/98], Loss: 5.2409, LR: 0.020208\nEpoch [707/800], Batch [20/98], Loss: 5.2284, LR: 0.020208\nEpoch [707/800], Batch [30/98], Loss: 5.2398, LR: 0.020208\nEpoch [707/800], Batch [40/98], Loss: 5.2121, LR: 0.020208\nEpoch [707/800], Batch [50/98], Loss: 5.2548, LR: 0.020208\nEpoch [707/800], Batch [60/98], Loss: 5.2499, LR: 0.020208\nEpoch [707/800], Batch [70/98], Loss: 5.2293, LR: 0.020208\nEpoch [707/800], Batch [80/98], Loss: 5.2573, LR: 0.020208\nEpoch [707/800], Batch [90/98], Loss: 5.2264, LR: 0.020208\nEpoch [707/800] Average Loss: 5.2327\n\nEpoch [708/800], Batch [0/98], Loss: 5.2244, LR: 0.019785\nEpoch [708/800], Batch [10/98], Loss: 5.2456, LR: 0.019785\nEpoch [708/800], Batch [20/98], Loss: 5.1977, LR: 0.019785\nEpoch [708/800], Batch [30/98], Loss: 5.2608, LR: 0.019785\nEpoch [708/800], Batch [40/98], Loss: 5.2529, LR: 0.019785\nEpoch [708/800], Batch [50/98], Loss: 5.2336, LR: 0.019785\nEpoch [708/800], Batch [60/98], Loss: 5.2213, LR: 0.019785\nEpoch [708/800], Batch [70/98], Loss: 5.2212, LR: 0.019785\nEpoch [708/800], Batch [80/98], Loss: 5.2278, LR: 0.019785\nEpoch [708/800], Batch [90/98], Loss: 5.2345, LR: 0.019785\nEpoch [708/800] Average Loss: 5.2317\n\nEpoch [709/800], Batch [0/98], Loss: 5.2440, LR: 0.019367\nEpoch [709/800], Batch [10/98], Loss: 5.1982, LR: 0.019367\nEpoch [709/800], Batch [20/98], Loss: 5.2260, LR: 0.019367\nEpoch [709/800], Batch [30/98], Loss: 5.2567, LR: 0.019367\nEpoch [709/800], Batch [40/98], Loss: 5.2131, LR: 0.019367\nEpoch [709/800], Batch [50/98], Loss: 5.2286, LR: 0.019367\nEpoch [709/800], Batch [60/98], Loss: 5.2368, LR: 0.019367\nEpoch [709/800], Batch [70/98], Loss: 5.2490, LR: 0.019367\nEpoch [709/800], Batch [80/98], Loss: 5.2624, LR: 0.019367\nEpoch [709/800], Batch [90/98], Loss: 5.2410, LR: 0.019367\nEpoch [709/800] Average Loss: 5.2317\n\nEpoch [710/800], Batch [0/98], Loss: 5.2283, LR: 0.018953\nEpoch [710/800], Batch [10/98], Loss: 5.2315, LR: 0.018953\nEpoch [710/800], Batch [20/98], Loss: 5.2288, LR: 0.018953\nEpoch [710/800], Batch [30/98], Loss: 5.2430, LR: 0.018953\nEpoch [710/800], Batch [40/98], Loss: 5.2440, LR: 0.018953\nEpoch [710/800], Batch [50/98], Loss: 5.2338, LR: 0.018953\nEpoch [710/800], Batch [60/98], Loss: 5.2381, LR: 0.018953\nEpoch [710/800], Batch [70/98], Loss: 5.2190, LR: 0.018953\nEpoch [710/800], Batch [80/98], Loss: 5.2520, LR: 0.018953\nEpoch [710/800], Batch [90/98], Loss: 5.2241, LR: 0.018953\nEpoch [710/800] Average Loss: 5.2326\n\nEpoch [711/800], Batch [0/98], Loss: 5.2440, LR: 0.018543\nEpoch [711/800], Batch [10/98], Loss: 5.2215, LR: 0.018543\nEpoch [711/800], Batch [20/98], Loss: 5.2420, LR: 0.018543\nEpoch [711/800], Batch [30/98], Loss: 5.2398, LR: 0.018543\nEpoch [711/800], Batch [40/98], Loss: 5.2362, LR: 0.018543\nEpoch [711/800], Batch [50/98], Loss: 5.2235, LR: 0.018543\nEpoch [711/800], Batch [60/98], Loss: 5.2333, LR: 0.018543\nEpoch [711/800], Batch [70/98], Loss: 5.2237, LR: 0.018543\nEpoch [711/800], Batch [80/98], Loss: 5.2392, LR: 0.018543\nEpoch [711/800], Batch [90/98], Loss: 5.2268, LR: 0.018543\nEpoch [711/800] Average Loss: 5.2328\n\nEpoch [712/800], Batch [0/98], Loss: 5.2309, LR: 0.018137\nEpoch [712/800], Batch [10/98], Loss: 5.2378, LR: 0.018137\nEpoch [712/800], Batch [20/98], Loss: 5.2384, LR: 0.018137\nEpoch [712/800], Batch [30/98], Loss: 5.2287, LR: 0.018137\nEpoch [712/800], Batch [40/98], Loss: 5.2545, LR: 0.018137\nEpoch [712/800], Batch [50/98], Loss: 5.2271, LR: 0.018137\nEpoch [712/800], Batch [60/98], Loss: 5.2310, LR: 0.018137\nEpoch [712/800], Batch [70/98], Loss: 5.2215, LR: 0.018137\nEpoch [712/800], Batch [80/98], Loss: 5.2260, LR: 0.018137\nEpoch [712/800], Batch [90/98], Loss: 5.2244, LR: 0.018137\nEpoch [712/800] Average Loss: 5.2299\n\nEpoch [713/800], Batch [0/98], Loss: 5.2512, LR: 0.017736\nEpoch [713/800], Batch [10/98], Loss: 5.2410, LR: 0.017736\nEpoch [713/800], Batch [20/98], Loss: 5.2298, LR: 0.017736\nEpoch [713/800], Batch [30/98], Loss: 5.2348, LR: 0.017736\nEpoch [713/800], Batch [40/98], Loss: 5.2437, LR: 0.017736\nEpoch [713/800], Batch [50/98], Loss: 5.2452, LR: 0.017736\nEpoch [713/800], Batch [60/98], Loss: 5.2089, LR: 0.017736\nEpoch [713/800], Batch [70/98], Loss: 5.2356, LR: 0.017736\nEpoch [713/800], Batch [80/98], Loss: 5.2189, LR: 0.017736\nEpoch [713/800], Batch [90/98], Loss: 5.2291, LR: 0.017736\nEpoch [713/800] Average Loss: 5.2308\n\nEpoch [714/800], Batch [0/98], Loss: 5.2209, LR: 0.017339\nEpoch [714/800], Batch [10/98], Loss: 5.2328, LR: 0.017339\nEpoch [714/800], Batch [20/98], Loss: 5.2334, LR: 0.017339\nEpoch [714/800], Batch [30/98], Loss: 5.2324, LR: 0.017339\nEpoch [714/800], Batch [40/98], Loss: 5.2540, LR: 0.017339\nEpoch [714/800], Batch [50/98], Loss: 5.2588, LR: 0.017339\nEpoch [714/800], Batch [60/98], Loss: 5.2281, LR: 0.017339\nEpoch [714/800], Batch [70/98], Loss: 5.2438, LR: 0.017339\nEpoch [714/800], Batch [80/98], Loss: 5.2503, LR: 0.017339\nEpoch [714/800], Batch [90/98], Loss: 5.2301, LR: 0.017339\nEpoch [714/800] Average Loss: 5.2326\n\nEpoch [715/800], Batch [0/98], Loss: 5.2624, LR: 0.016946\nEpoch [715/800], Batch [10/98], Loss: 5.2257, LR: 0.016946\nEpoch [715/800], Batch [20/98], Loss: 5.2540, LR: 0.016946\nEpoch [715/800], Batch [30/98], Loss: 5.2533, LR: 0.016946\nEpoch [715/800], Batch [40/98], Loss: 5.2148, LR: 0.016946\nEpoch [715/800], Batch [50/98], Loss: 5.2750, LR: 0.016946\nEpoch [715/800], Batch [60/98], Loss: 5.2340, LR: 0.016946\nEpoch [715/800], Batch [70/98], Loss: 5.2364, LR: 0.016946\nEpoch [715/800], Batch [80/98], Loss: 5.2259, LR: 0.016946\nEpoch [715/800], Batch [90/98], Loss: 5.2344, LR: 0.016946\nEpoch [715/800] Average Loss: 5.2338\n\nEpoch [716/800], Batch [0/98], Loss: 5.2251, LR: 0.016558\nEpoch [716/800], Batch [10/98], Loss: 5.2256, LR: 0.016558\nEpoch [716/800], Batch [20/98], Loss: 5.2322, LR: 0.016558\nEpoch [716/800], Batch [30/98], Loss: 5.2591, LR: 0.016558\nEpoch [716/800], Batch [40/98], Loss: 5.2543, LR: 0.016558\nEpoch [716/800], Batch [50/98], Loss: 5.2279, LR: 0.016558\nEpoch [716/800], Batch [60/98], Loss: 5.2146, LR: 0.016558\nEpoch [716/800], Batch [70/98], Loss: 5.2332, LR: 0.016558\nEpoch [716/800], Batch [80/98], Loss: 5.2299, LR: 0.016558\nEpoch [716/800], Batch [90/98], Loss: 5.2485, LR: 0.016558\nEpoch [716/800] Average Loss: 5.2292\n\nEpoch [717/800], Batch [0/98], Loss: 5.2294, LR: 0.016174\nEpoch [717/800], Batch [10/98], Loss: 5.2389, LR: 0.016174\nEpoch [717/800], Batch [20/98], Loss: 5.2406, LR: 0.016174\nEpoch [717/800], Batch [30/98], Loss: 5.2422, LR: 0.016174\nEpoch [717/800], Batch [40/98], Loss: 5.2511, LR: 0.016174\nEpoch [717/800], Batch [50/98], Loss: 5.2248, LR: 0.016174\nEpoch [717/800], Batch [60/98], Loss: 5.2453, LR: 0.016174\nEpoch [717/800], Batch [70/98], Loss: 5.2565, LR: 0.016174\nEpoch [717/800], Batch [80/98], Loss: 5.2412, LR: 0.016174\nEpoch [717/800], Batch [90/98], Loss: 5.2187, LR: 0.016174\nEpoch [717/800] Average Loss: 5.2332\n\nEpoch [718/800], Batch [0/98], Loss: 5.2424, LR: 0.015795\nEpoch [718/800], Batch [10/98], Loss: 5.2424, LR: 0.015795\nEpoch [718/800], Batch [20/98], Loss: 5.2333, LR: 0.015795\nEpoch [718/800], Batch [30/98], Loss: 5.2427, LR: 0.015795\nEpoch [718/800], Batch [40/98], Loss: 5.2555, LR: 0.015795\nEpoch [718/800], Batch [50/98], Loss: 5.2370, LR: 0.015795\nEpoch [718/800], Batch [60/98], Loss: 5.2459, LR: 0.015795\nEpoch [718/800], Batch [70/98], Loss: 5.2279, LR: 0.015795\nEpoch [718/800], Batch [80/98], Loss: 5.2322, LR: 0.015795\nEpoch [718/800], Batch [90/98], Loss: 5.2289, LR: 0.015795\nEpoch [718/800] Average Loss: 5.2337\n\nEpoch [719/800], Batch [0/98], Loss: 5.2302, LR: 0.015420\nEpoch [719/800], Batch [10/98], Loss: 5.2570, LR: 0.015420\nEpoch [719/800], Batch [20/98], Loss: 5.2272, LR: 0.015420\nEpoch [719/800], Batch [30/98], Loss: 5.2293, LR: 0.015420\nEpoch [719/800], Batch [40/98], Loss: 5.2189, LR: 0.015420\nEpoch [719/800], Batch [50/98], Loss: 5.2263, LR: 0.015420\nEpoch [719/800], Batch [60/98], Loss: 5.2357, LR: 0.015420\nEpoch [719/800], Batch [70/98], Loss: 5.2362, LR: 0.015420\nEpoch [719/800], Batch [80/98], Loss: 5.2357, LR: 0.015420\nEpoch [719/800], Batch [90/98], Loss: 5.2092, LR: 0.015420\nEpoch [719/800] Average Loss: 5.2320\n\nEpoch [720/800], Batch [0/98], Loss: 5.2434, LR: 0.015049\nEpoch [720/800], Batch [10/98], Loss: 5.2068, LR: 0.015049\nEpoch [720/800], Batch [20/98], Loss: 5.2345, LR: 0.015049\nEpoch [720/800], Batch [30/98], Loss: 5.2396, LR: 0.015049\nEpoch [720/800], Batch [40/98], Loss: 5.2326, LR: 0.015049\nEpoch [720/800], Batch [50/98], Loss: 5.2265, LR: 0.015049\nEpoch [720/800], Batch [60/98], Loss: 5.2247, LR: 0.015049\nEpoch [720/800], Batch [70/98], Loss: 5.2434, LR: 0.015049\nEpoch [720/800], Batch [80/98], Loss: 5.2367, LR: 0.015049\nEpoch [720/800], Batch [90/98], Loss: 5.2380, LR: 0.015049\nEpoch [720/800] Average Loss: 5.2299\n\nEpoch [721/800], Batch [0/98], Loss: 5.2191, LR: 0.014683\nEpoch [721/800], Batch [10/98], Loss: 5.2231, LR: 0.014683\nEpoch [721/800], Batch [20/98], Loss: 5.2223, LR: 0.014683\nEpoch [721/800], Batch [30/98], Loss: 5.2635, LR: 0.014683\nEpoch [721/800], Batch [40/98], Loss: 5.2245, LR: 0.014683\nEpoch [721/800], Batch [50/98], Loss: 5.2233, LR: 0.014683\nEpoch [721/800], Batch [60/98], Loss: 5.2681, LR: 0.014683\nEpoch [721/800], Batch [70/98], Loss: 5.2193, LR: 0.014683\nEpoch [721/800], Batch [80/98], Loss: 5.2137, LR: 0.014683\nEpoch [721/800], Batch [90/98], Loss: 5.2038, LR: 0.014683\nEpoch [721/800] Average Loss: 5.2314\n\nEpoch [722/800], Batch [0/98], Loss: 5.2262, LR: 0.014321\nEpoch [722/800], Batch [10/98], Loss: 5.2425, LR: 0.014321\nEpoch [722/800], Batch [20/98], Loss: 5.2362, LR: 0.014321\nEpoch [722/800], Batch [30/98], Loss: 5.2223, LR: 0.014321\nEpoch [722/800], Batch [40/98], Loss: 5.2441, LR: 0.014321\nEpoch [722/800], Batch [50/98], Loss: 5.2331, LR: 0.014321\nEpoch [722/800], Batch [60/98], Loss: 5.2133, LR: 0.014321\nEpoch [722/800], Batch [70/98], Loss: 5.2436, LR: 0.014321\nEpoch [722/800], Batch [80/98], Loss: 5.2328, LR: 0.014321\nEpoch [722/800], Batch [90/98], Loss: 5.2485, LR: 0.014321\nEpoch [722/800] Average Loss: 5.2298\n\nEpoch [723/800], Batch [0/98], Loss: 5.2246, LR: 0.013964\nEpoch [723/800], Batch [10/98], Loss: 5.2449, LR: 0.013964\nEpoch [723/800], Batch [20/98], Loss: 5.2413, LR: 0.013964\nEpoch [723/800], Batch [30/98], Loss: 5.2487, LR: 0.013964\nEpoch [723/800], Batch [40/98], Loss: 5.2511, LR: 0.013964\nEpoch [723/800], Batch [50/98], Loss: 5.2357, LR: 0.013964\nEpoch [723/800], Batch [60/98], Loss: 5.2318, LR: 0.013964\nEpoch [723/800], Batch [70/98], Loss: 5.2632, LR: 0.013964\nEpoch [723/800], Batch [80/98], Loss: 5.2258, LR: 0.013964\nEpoch [723/800], Batch [90/98], Loss: 5.2685, LR: 0.013964\nEpoch [723/800] Average Loss: 5.2296\n\nEpoch [724/800], Batch [0/98], Loss: 5.2454, LR: 0.013611\nEpoch [724/800], Batch [10/98], Loss: 5.2401, LR: 0.013611\nEpoch [724/800], Batch [20/98], Loss: 5.2464, LR: 0.013611\nEpoch [724/800], Batch [30/98], Loss: 5.2506, LR: 0.013611\nEpoch [724/800], Batch [40/98], Loss: 5.2261, LR: 0.013611\nEpoch [724/800], Batch [50/98], Loss: 5.2275, LR: 0.013611\nEpoch [724/800], Batch [60/98], Loss: 5.2160, LR: 0.013611\nEpoch [724/800], Batch [70/98], Loss: 5.2210, LR: 0.013611\nEpoch [724/800], Batch [80/98], Loss: 5.2363, LR: 0.013611\nEpoch [724/800], Batch [90/98], Loss: 5.2201, LR: 0.013611\nEpoch [724/800] Average Loss: 5.2300\n\nEpoch [725/800], Batch [0/98], Loss: 5.2577, LR: 0.013262\nEpoch [725/800], Batch [10/98], Loss: 5.2278, LR: 0.013262\nEpoch [725/800], Batch [20/98], Loss: 5.2380, LR: 0.013262\nEpoch [725/800], Batch [30/98], Loss: 5.2200, LR: 0.013262\nEpoch [725/800], Batch [40/98], Loss: 5.2456, LR: 0.013262\nEpoch [725/800], Batch [50/98], Loss: 5.2084, LR: 0.013262\nEpoch [725/800], Batch [60/98], Loss: 5.2538, LR: 0.013262\nEpoch [725/800], Batch [70/98], Loss: 5.2323, LR: 0.013262\nEpoch [725/800], Batch [80/98], Loss: 5.2251, LR: 0.013262\nEpoch [725/800], Batch [90/98], Loss: 5.2294, LR: 0.013262\nEpoch [725/800] Average Loss: 5.2325\n\nEpoch [726/800], Batch [0/98], Loss: 5.2265, LR: 0.012918\nEpoch [726/800], Batch [10/98], Loss: 5.2417, LR: 0.012918\nEpoch [726/800], Batch [20/98], Loss: 5.2534, LR: 0.012918\nEpoch [726/800], Batch [30/98], Loss: 5.2320, LR: 0.012918\nEpoch [726/800], Batch [40/98], Loss: 5.2220, LR: 0.012918\nEpoch [726/800], Batch [50/98], Loss: 5.2432, LR: 0.012918\nEpoch [726/800], Batch [60/98], Loss: 5.2346, LR: 0.012918\nEpoch [726/800], Batch [70/98], Loss: 5.2454, LR: 0.012918\nEpoch [726/800], Batch [80/98], Loss: 5.2259, LR: 0.012918\nEpoch [726/800], Batch [90/98], Loss: 5.2608, LR: 0.012918\nEpoch [726/800] Average Loss: 5.2308\n\nEpoch [727/800], Batch [0/98], Loss: 5.2338, LR: 0.012578\nEpoch [727/800], Batch [10/98], Loss: 5.2454, LR: 0.012578\nEpoch [727/800], Batch [20/98], Loss: 5.2227, LR: 0.012578\nEpoch [727/800], Batch [30/98], Loss: 5.2516, LR: 0.012578\nEpoch [727/800], Batch [40/98], Loss: 5.2267, LR: 0.012578\nEpoch [727/800], Batch [50/98], Loss: 5.2167, LR: 0.012578\nEpoch [727/800], Batch [60/98], Loss: 5.2377, LR: 0.012578\nEpoch [727/800], Batch [70/98], Loss: 5.2368, LR: 0.012578\nEpoch [727/800], Batch [80/98], Loss: 5.2353, LR: 0.012578\nEpoch [727/800], Batch [90/98], Loss: 5.2171, LR: 0.012578\nEpoch [727/800] Average Loss: 5.2291\n\nEpoch [728/800], Batch [0/98], Loss: 5.2268, LR: 0.012243\nEpoch [728/800], Batch [10/98], Loss: 5.2522, LR: 0.012243\nEpoch [728/800], Batch [20/98], Loss: 5.2229, LR: 0.012243\nEpoch [728/800], Batch [30/98], Loss: 5.2227, LR: 0.012243\nEpoch [728/800], Batch [40/98], Loss: 5.2224, LR: 0.012243\nEpoch [728/800], Batch [50/98], Loss: 5.2195, LR: 0.012243\nEpoch [728/800], Batch [60/98], Loss: 5.2368, LR: 0.012243\nEpoch [728/800], Batch [70/98], Loss: 5.2157, LR: 0.012243\nEpoch [728/800], Batch [80/98], Loss: 5.2108, LR: 0.012243\nEpoch [728/800], Batch [90/98], Loss: 5.2108, LR: 0.012243\nEpoch [728/800] Average Loss: 5.2306\n\nEpoch [729/800], Batch [0/98], Loss: 5.2506, LR: 0.011912\nEpoch [729/800], Batch [10/98], Loss: 5.2389, LR: 0.011912\nEpoch [729/800], Batch [20/98], Loss: 5.2270, LR: 0.011912\nEpoch [729/800], Batch [30/98], Loss: 5.2297, LR: 0.011912\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ProjectionHead(nn.Module):\n    \"\"\"MLP projection head with batch normalization for contrastive learning\"\"\"\n    def __init__(self, in_dim=512, hidden_dim=2048, out_dim=128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim, bias=False),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim, bias=False),\n            nn.BatchNorm1d(out_dim)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\nclass SimCLR(nn.Module):\n    \"\"\"SimCLR self-supervised learning model\"\"\"\n    def __init__(self, encoder_dim=512, projection_dim=128, hidden_dim=2048):\n        super().__init__()\n        # ResNet18 encoder\n        resnet = torchvision.models.resnet18(weights=None)\n        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection_head = ProjectionHead(encoder_dim, hidden_dim, projection_dim)\n    \n    def forward(self, x):\n        h = self.encoder(x)\n        h = h.view(h.size(0), -1)\n        z = self.projection_head(h)\n        return h, z\n\nclass LinearClassifier(nn.Module):\n    \"\"\"Linear classifier on top of frozen encoder\"\"\"\n    def __init__(self, encoder, num_classes=10):\n        super().__init__()\n        self.encoder = encoder\n        self.classifier = nn.Linear(512, num_classes)\n        \n        # Freeze encoder\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n    \n    def forward(self, x):\n        with torch.no_grad():\n            features = self.encoder(x)\n            features = features.view(features.size(0), -1)\n        return self.classifier(features)\n\ndef extract_features(model, dataloader, device):\n    \"\"\"Extract features from the encoder\"\"\"\n    model.eval()\n    features_list = []\n    labels_list = []\n    \n    print(\"Extracting features...\")\n    with torch.no_grad():\n        for images, labels in tqdm(dataloader):\n            images = images.to(device)\n            \n            # Get features from encoder\n            h, _ = model(images)\n            features_list.append(h.cpu().numpy())\n            labels_list.append(labels.numpy())\n    \n    features = np.vstack(features_list)\n    labels = np.concatenate(labels_list)\n    \n    return features, labels\n\ndef linear_evaluation(train_features, train_labels, test_features, test_labels):\n    \"\"\"Evaluate with linear classifier (sklearn)\"\"\"\n    print(\"\\nTraining linear classifier on frozen features...\")\n    \n    # Normalize features\n    scaler = StandardScaler()\n    train_features_scaled = scaler.fit_transform(train_features)\n    test_features_scaled = scaler.transform(test_features)\n    \n    # Train logistic regression\n    clf = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n    clf.fit(train_features_scaled, train_labels)\n    \n    # Predictions\n    train_preds = clf.predict(train_features_scaled)\n    test_preds = clf.predict(test_features_scaled)\n    \n    return train_preds, test_preds, clf\n\ndef finetune_evaluation(encoder, train_loader, test_loader, device, epochs=10):\n    \"\"\"Fine-tune evaluation with a linear classifier on top of frozen encoder\"\"\"\n    print(\"\\nFine-tuning linear classifier on frozen encoder...\")\n    \n    # Create classifier\n    classifier = LinearClassifier(encoder, num_classes=10).to(device)\n    \n    # Optimizer and loss\n    optimizer = torch.optim.Adam(classifier.classifier.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training\n    for epoch in range(epochs):\n        classifier.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            images, labels = images.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = classifier(images)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100. * correct / total\n        print(f'Epoch {epoch+1}: Loss: {total_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%')\n    \n    # Evaluation\n    classifier.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = classifier(images)\n            _, predicted = outputs.max(1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    \n    return np.array(all_preds), np.array(all_labels)\n\ndef calculate_metrics(y_true, y_pred, class_names):\n    \"\"\"Calculate comprehensive metrics\"\"\"\n    \n    # Accuracy\n    accuracy = accuracy_score(y_true, y_pred)\n    \n    # F1 Scores\n    f1_micro = f1_score(y_true, y_pred, average='micro')\n    f1_macro = f1_score(y_true, y_pred, average='macro')\n    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n    f1_per_class = f1_score(y_true, y_pred, average=None)\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Per-class accuracy (IoU for classification)\n    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n    \n    # Mean IoU (Intersection over Union)\n    # For classification: IoU = TP / (TP + FP + FN)\n    iou_per_class = []\n    for i in range(len(class_names)):\n        tp = cm[i, i]\n        fp = cm[:, i].sum() - tp\n        fn = cm[i, :].sum() - tp\n        iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n        iou_per_class.append(iou)\n    \n    mean_iou = np.mean(iou_per_class)\n    \n    return {\n        'accuracy': accuracy,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'f1_weighted': f1_weighted,\n        'f1_per_class': f1_per_class,\n        'confusion_matrix': cm,\n        'per_class_accuracy': per_class_acc,\n        'iou_per_class': iou_per_class,\n        'mean_iou': mean_iou\n    }\n\ndef plot_confusion_matrix(cm, class_names, save_path='confusion_matrix.png'):\n    \"\"\"Plot confusion matrix\"\"\"\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n    plt.ylabel('True Label', fontsize=12)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"✓ Confusion matrix saved to: {save_path}\")\n\ndef plot_per_class_metrics(f1_scores, accuracies, ious, class_names, save_path='per_class_metrics.png'):\n    \"\"\"Plot per-class metrics\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    # F1 Scores\n    axes[0].bar(range(len(class_names)), f1_scores, color='steelblue', alpha=0.8)\n    axes[0].set_xticks(range(len(class_names)))\n    axes[0].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[0].set_ylabel('F1 Score', fontsize=12)\n    axes[0].set_title('F1 Score per Class', fontsize=14, fontweight='bold')\n    axes[0].grid(axis='y', alpha=0.3)\n    axes[0].set_ylim([0, 1])\n    \n    # Accuracy\n    axes[1].bar(range(len(class_names)), accuracies, color='green', alpha=0.8)\n    axes[1].set_xticks(range(len(class_names)))\n    axes[1].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[1].set_ylabel('Accuracy', fontsize=12)\n    axes[1].set_title('Accuracy per Class', fontsize=14, fontweight='bold')\n    axes[1].grid(axis='y', alpha=0.3)\n    axes[1].set_ylim([0, 1])\n    \n    # IoU\n    axes[2].bar(range(len(class_names)), ious, color='orange', alpha=0.8)\n    axes[2].set_xticks(range(len(class_names)))\n    axes[2].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[2].set_ylabel('IoU', fontsize=12)\n    axes[2].set_title('IoU per Class', fontsize=14, fontweight='bold')\n    axes[2].grid(axis='y', alpha=0.3)\n    axes[2].set_ylim([0, 1])\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"✓ Per-class metrics saved to: {save_path}\")\n\ndef print_evaluation_report(metrics, class_names, method_name=\"Model\"):\n    \"\"\"Print comprehensive evaluation report\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"{method_name} EVALUATION RESULTS\")\n    print(\"=\"*80)\n    \n    print(f\"\\n📊 Overall Metrics:\")\n    print(f\"   Accuracy:        {metrics['accuracy']*100:.2f}%\")\n    print(f\"   F1 Score (Micro): {metrics['f1_micro']:.4f}\")\n    print(f\"   F1 Score (Macro): {metrics['f1_macro']:.4f}\")\n    print(f\"   F1 Score (Weighted): {metrics['f1_weighted']:.4f}\")\n    print(f\"   Mean IoU:        {metrics['mean_iou']:.4f}\")\n    \n    print(f\"\\n📈 Per-Class Metrics:\")\n    print(f\"{'Class':<15} {'Accuracy':<12} {'F1 Score':<12} {'IoU':<12}\")\n    print(\"-\" * 80)\n    for i, class_name in enumerate(class_names):\n        print(f\"{class_name:<15} {metrics['per_class_accuracy'][i]:>10.4f}  \"\n              f\"{metrics['f1_per_class'][i]:>10.4f}  {metrics['iou_per_class'][i]:>10.4f}\")\n    \n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n\ndef evaluate_simclr(model_path, device='cuda', method='linear'):\n    \"\"\"\n    Complete evaluation pipeline\n    \n    Args:\n        model_path: Path to saved SimCLR model\n        device: Device to use\n        method: 'linear' for sklearn LogReg, 'finetune' for PyTorch linear classifier\n    \"\"\"\n    \n    # Set device\n    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # CIFAR-10 class names\n    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n                   'dog', 'frog', 'horse', 'ship', 'truck']\n    \n    # Load model\n    print(f\"\\nLoading model from: {model_path}\")\n    model = SimCLR(encoder_dim=512, projection_dim=128, hidden_dim=2048)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = model.to(device)\n    model.eval()\n    print(\"✓ Model loaded successfully\")\n    \n    # Prepare datasets\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    train_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=False, transform=transform\n    )\n    test_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=False, transform=transform\n    )\n    \n    if method == 'linear':\n        # Extract features for sklearn evaluation\n        train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False, \n                                 num_workers=4, pin_memory=True)\n        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False,\n                                num_workers=4, pin_memory=True)\n        \n        # Extract features\n        train_features, train_labels = extract_features(model, train_loader, device)\n        test_features, test_labels = extract_features(model, test_loader, device)\n        \n        # Linear evaluation\n        _, test_preds, _ = linear_evaluation(\n            train_features, train_labels, test_features, test_labels\n        )\n        \n        # Calculate metrics\n        metrics = calculate_metrics(test_labels, test_preds, class_names)\n        \n    elif method == 'finetune':\n        # Fine-tune evaluation with PyTorch\n        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n                                 num_workers=4, pin_memory=True)\n        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,\n                                num_workers=4, pin_memory=True)\n        \n        # Get predictions\n        test_preds, test_labels = finetune_evaluation(\n            model.encoder, train_loader, test_loader, device, epochs=10\n        )\n        \n        # Calculate metrics\n        metrics = calculate_metrics(test_labels, test_preds, class_names)\n    \n    else:\n        raise ValueError(f\"Unknown method: {method}. Use 'linear' or 'finetune'\")\n    \n    # Print results\n    print_evaluation_report(metrics, class_names, \n                           method_name=f\"SimCLR ({method.upper()} EVALUATION)\")\n    \n    # Visualizations\n    plot_confusion_matrix(metrics['confusion_matrix'], class_names,\n                         save_path=f'confusion_matrix_{method}.png')\n    \n    plot_per_class_metrics(\n        metrics['f1_per_class'],\n        metrics['per_class_accuracy'],\n        metrics['iou_per_class'],\n        class_names,\n        save_path=f'per_class_metrics_{method}.png'\n    )\n    \n    # Detailed classification report\n    print(\"\\n📋 Detailed Classification Report:\")\n    print(\"-\" * 80)\n    if method == 'linear':\n        from sklearn.metrics import classification_report\n        print(classification_report(test_labels, test_preds, target_names=class_names))\n    else:\n        from sklearn.metrics import classification_report\n        print(classification_report(test_labels, test_preds, target_names=class_names))\n    \n    return metrics\n\ndef compare_with_supervised_baseline(device='cuda'):\n    \"\"\"Compare SimCLR with supervised baseline\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPARING WITH SUPERVISED BASELINE\")\n    print(\"=\"*80)\n    \n    # Train supervised ResNet18\n    print(\"\\nTraining supervised ResNet18 baseline...\")\n    \n    transform = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    train_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=False, transform=transform\n    )\n    test_dataset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=False, transform=test_transform\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n    \n    # Create supervised model\n    model = torchvision.models.resnet18(weights=None, num_classes=10).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Train for 10 epochs\n    for epoch in range(10):\n        model.train()\n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\"):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n    \n    # Evaluate\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, predicted = outputs.max(1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    \n    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n                   'dog', 'frog', 'horse', 'ship', 'truck']\n    \n    metrics = calculate_metrics(np.array(all_labels), np.array(all_preds), class_names)\n    print_evaluation_report(metrics, class_names, method_name=\"SUPERVISED BASELINE\")\n    \n    return metrics\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*80)\n    print(\"SimCLR MODEL EVALUATION\")\n    print(\"=\"*80)\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Check for model file\n    model_path = '/kaggle/working/simclr_model_final.pth'\n    try:\n        print(\"\\n🔍 Evaluation Method:\")\n        print(\"1. Linear Evaluation (sklearn Logistic Regression)\")\n        print(\"2. Fine-tune Evaluation (PyTorch Linear Classifier)\")\n        print(\"3. Both methods\")\n        print(\"4. Compare with supervised baseline\")\n        \n        choice = input(\"\\nSelect method (1/2/3/4): \").strip()\n        \n        if choice == '1':\n            metrics = evaluate_simclr(model_path, device=device, method='linear')\n        \n        elif choice == '2':\n            metrics = evaluate_simclr(model_path, device=device, method='finetune')\n        \n        elif choice == '3':\n            print(\"\\n\" + \"=\"*80)\n            print(\"METHOD 1: LINEAR EVALUATION\")\n            print(\"=\"*80)\n            metrics_linear = evaluate_simclr(model_path, device=device, method='linear')\n            \n            print(\"\\n\" + \"=\"*80)\n            print(\"METHOD 2: FINE-TUNE EVALUATION\")\n            print(\"=\"*80)\n            metrics_finetune = evaluate_simclr(model_path, device=device, method='finetune')\n            \n            # Comparison\n            print(\"\\n\" + \"=\"*80)\n            print(\"COMPARISON SUMMARY\")\n            print(\"=\"*80)\n            print(f\"\\n{'Metric':<25} {'Linear':<15} {'Fine-tune':<15}\")\n            print(\"-\" * 80)\n            print(f\"{'Accuracy':<25} {metrics_linear['accuracy']*100:>13.2f}%  {metrics_finetune['accuracy']*100:>13.2f}%\")\n            print(f\"{'F1 Score (Macro)':<25} {metrics_linear['f1_macro']:>13.4f}  {metrics_finetune['f1_macro']:>13.4f}\")\n            print(f\"{'Mean IoU':<25} {metrics_linear['mean_iou']:>13.4f}  {metrics_finetune['mean_iou']:>13.4f}\")\n        \n        elif choice == '4':\n            # Evaluate SimCLR\n            print(\"\\n\" + \"=\"*80)\n            print(\"EVALUATING SimCLR\")\n            print(\"=\"*80)\n            metrics_simclr = evaluate_simclr(model_path, device=device, method='finetune')\n            \n            # Compare with supervised\n            metrics_supervised = compare_with_supervised_baseline(device=device)\n            \n            # Final comparison\n            print(\"\\n\" + \"=\"*80)\n            print(\"FINAL COMPARISON: SimCLR vs SUPERVISED\")\n            print(\"=\"*80)\n            print(f\"\\n{'Metric':<25} {'SimCLR':<15} {'Supervised':<15}\")\n            print(\"-\" * 80)\n            print(f\"{'Accuracy':<25} {metrics_simclr['accuracy']*100:>13.2f}%  {metrics_supervised['accuracy']*100:>13.2f}%\")\n            print(f\"{'F1 Score (Macro)':<25} {metrics_simclr['f1_macro']:>13.4f}  {metrics_supervised['f1_macro']:>13.4f}\")\n            print(f\"{'Mean IoU':<25} {metrics_simclr['mean_iou']:>13.4f}  {metrics_supervised['mean_iou']:>13.4f}\")\n        \n        else:\n            print(\"Invalid choice!\")\n    \n    except FileNotFoundError:\n        print(f\"\\n❌ Error: Model file '{model_path}' not found!\")\n        print(\"Please train the model first or provide the correct path.\")\n    \n    except Exception as e:\n        print(f\"\\n❌ Error during evaluation: {e}\")\n        raise e\n    \n    print(\"\\n✅ Evaluation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T21:15:58.456494Z","iopub.execute_input":"2026-01-22T21:15:58.457313Z","iopub.status.idle":"2026-01-22T21:17:53.861983Z","shell.execute_reply.started":"2026-01-22T21:15:58.457280Z","shell.execute_reply":"2026-01-22T21:17:53.860952Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSimCLR MODEL EVALUATION\n================================================================================\n\n🔍 Evaluation Method:\n1. Linear Evaluation (sklearn Logistic Regression)\n2. Fine-tune Evaluation (PyTorch Linear Classifier)\n3. Both methods\n4. Compare with supervised baseline\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nSelect method (1/2/3/4):  3\n"},{"name":"stdout","text":"\n================================================================================\nMETHOD 1: LINEAR EVALUATION\n================================================================================\nUsing device: cuda\n\nLoading model from: /kaggle/working/simclr_model_final.pth\n✓ Model loaded successfully\nExtracting features...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [00:06<00:00, 31.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting features...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:01<00:00, 30.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTraining linear classifier on frozen features...\n\n================================================================================\nSimCLR (LINEAR EVALUATION) EVALUATION RESULTS\n================================================================================\n\n📊 Overall Metrics:\n   Accuracy:        79.65%\n   F1 Score (Micro): 0.7965\n   F1 Score (Macro): 0.7960\n   F1 Score (Weighted): 0.7960\n   Mean IoU:        0.6726\n\n📈 Per-Class Metrics:\nClass           Accuracy     F1 Score     IoU         \n--------------------------------------------------------------------------------\nairplane            0.8310      0.8343      0.7158\nautomobile          0.9180      0.9189      0.8500\nbird                0.6650      0.6845      0.5203\ncat                 0.6240      0.6094      0.4382\ndeer                0.7980      0.7658      0.6205\ndog                 0.6450      0.6815      0.5168\nfrog                0.8570      0.8398      0.7238\nhorse               0.8150      0.8232      0.6996\nship                0.9060      0.9074      0.8304\ntruck               0.9060      0.8953      0.8104\n\n================================================================================\n\n✓ Confusion matrix saved to: confusion_matrix_linear.png\n✓ Per-class metrics saved to: per_class_metrics_linear.png\n\n📋 Detailed Classification Report:\n--------------------------------------------------------------------------------\n              precision    recall  f1-score   support\n\n    airplane       0.84      0.83      0.83      1000\n  automobile       0.92      0.92      0.92      1000\n        bird       0.71      0.67      0.68      1000\n         cat       0.60      0.62      0.61      1000\n        deer       0.74      0.80      0.77      1000\n         dog       0.72      0.65      0.68      1000\n        frog       0.82      0.86      0.84      1000\n       horse       0.83      0.81      0.82      1000\n        ship       0.91      0.91      0.91      1000\n       truck       0.88      0.91      0.90      1000\n\n    accuracy                           0.80     10000\n   macro avg       0.80      0.80      0.80     10000\nweighted avg       0.80      0.80      0.80     10000\n\n\n================================================================================\nMETHOD 2: FINE-TUNE EVALUATION\n================================================================================\nUsing device: cuda\n\nLoading model from: /kaggle/working/simclr_model_final.pth\n✓ Model loaded successfully\n\nFine-tuning linear classifier on frozen encoder...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 391/391 [00:06<00:00, 63.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss: 0.6596, Train Acc: 77.75%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 391/391 [00:06<00:00, 63.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss: 0.5707, Train Acc: 80.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 391/391 [00:06<00:00, 62.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss: 0.5575, Train Acc: 80.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 391/391 [00:06<00:00, 63.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss: 0.5496, Train Acc: 80.80%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 391/391 [00:06<00:00, 63.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss: 0.5420, Train Acc: 81.03%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 391/391 [00:06<00:00, 61.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss: 0.5379, Train Acc: 80.96%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 391/391 [00:06<00:00, 63.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Loss: 0.5298, Train Acc: 81.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 391/391 [00:06<00:00, 62.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Loss: 0.5306, Train Acc: 81.45%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 391/391 [00:06<00:00, 63.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Loss: 0.5277, Train Acc: 81.51%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 391/391 [00:06<00:00, 62.27it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss: 0.5261, Train Acc: 81.50%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nSimCLR (FINETUNE EVALUATION) EVALUATION RESULTS\n================================================================================\n\n📊 Overall Metrics:\n   Accuracy:        79.53%\n   F1 Score (Micro): 0.7953\n   F1 Score (Macro): 0.7950\n   F1 Score (Weighted): 0.7950\n   Mean IoU:        0.6705\n\n📈 Per-Class Metrics:\nClass           Accuracy     F1 Score     IoU         \n--------------------------------------------------------------------------------\nairplane            0.8460      0.8218      0.6974\nautomobile          0.9000      0.9174      0.8475\nbird                0.6910      0.6865      0.5227\ncat                 0.6370      0.6149      0.4439\ndeer                0.7740      0.7701      0.6262\ndog                 0.6210      0.6813      0.5166\nfrog                0.8480      0.8289      0.7078\nhorse               0.8430      0.8434      0.7292\nship                0.8870      0.8969      0.8130\ntruck               0.9060      0.8891      0.8004\n\n================================================================================\n\n✓ Confusion matrix saved to: confusion_matrix_finetune.png\n✓ Per-class metrics saved to: per_class_metrics_finetune.png\n\n📋 Detailed Classification Report:\n--------------------------------------------------------------------------------\n              precision    recall  f1-score   support\n\n    airplane       0.80      0.85      0.82      1000\n  automobile       0.94      0.90      0.92      1000\n        bird       0.68      0.69      0.69      1000\n         cat       0.59      0.64      0.61      1000\n        deer       0.77      0.77      0.77      1000\n         dog       0.75      0.62      0.68      1000\n        frog       0.81      0.85      0.83      1000\n       horse       0.84      0.84      0.84      1000\n        ship       0.91      0.89      0.90      1000\n       truck       0.87      0.91      0.89      1000\n\n    accuracy                           0.80     10000\n   macro avg       0.80      0.80      0.80     10000\nweighted avg       0.80      0.80      0.80     10000\n\n\n================================================================================\nCOMPARISON SUMMARY\n================================================================================\n\nMetric                    Linear          Fine-tune      \n--------------------------------------------------------------------------------\nAccuracy                          79.65%          79.53%\nF1 Score (Macro)                 0.7960         0.7950\nMean IoU                         0.6726         0.6705\n\n✅ Evaluation complete!\n","output_type":"stream"}],"execution_count":9}]}